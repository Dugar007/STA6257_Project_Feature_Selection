---
title: "Feature Selection: An Exploration of Algorithm Performance"
author: "Jason Case, Abhishek Dugar, Daniel Nkuah, Khoa Tran"
date: '2024-07-30'
format: 
  revealjs:
    theme: serif
    css: slides.css
course: STA 6257 - Advanced Statistical Modeling
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
  # eval: false  # Prevent code execution
editor: 
  markdown: 
    wrap: 72
---

```{r}
# #| include: false
#environment setup (hidden from output)
library(dplyr)
library(quanteda)
library(caret)
library(wordcloud)
library(ggplot2)
library(glmnet)
library(jsonlite)
library(knitr)
library(kableExtra)

# Global parameter to force overwriting of existing cache files
FORCE_OVERWRITE <- FALSE

baseline_lambda = 0
baseline_maxit = 1000000
baseline_alpha = 0
validation_folds = 10

# Function to cache parameters
cache_parameter <- function(name, value = NULL, path = "cache/", prefix = "param_") {
  # Ensure the cache directory exists
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE)
  }
  
  # Construct the full file name
  file_name <- paste0(path, prefix, name, ".json")
  
  # Check for file existence
  if (file.exists(file_name) && !FORCE_OVERWRITE) {
    # Load and return the value from the file
    cached_value <- fromJSON(file_name)
    # Cast to the appropriate type
    if (cached_value$type == "numeric") {
      return(as.numeric(cached_value$value))
    } else if (cached_value$type == "integer") {
      return(as.integer(cached_value$value))
    } else if (cached_value$type == "list") {
      return(as.list(cached_value$value))
    } else if (cached_value$type == "vector_numeric") {
      return(as.numeric(cached_value$value))
    } else if (cached_value$type == "vector_integer") {
      return(as.integer(cached_value$value))
    } else if (cached_value$type == "named_list_numeric") {
      return(as.list(setNames(as.numeric(cached_value$value), cached_value$names)))
    } else {
      stop("Unsupported cached value type.")
    }
  } else {
    cached_value <- NULL
  }
  
  if (is.null(value)) {
    return(cached_value)
  } else {
    # Determine the type of the value and write it to the file
    if (is.numeric(value) && length(value) == 1) {
      value_type <- "numeric"
    } else if (is.integer(value) && length(value) == 1) {
      value_type <- "integer"
    } else if (is.list(value)) {
      if (!is.null(names(value)) && all(sapply(value, is.numeric))) {
        value_type <- "named_list_numeric"
      } else {
        value_type <- "list"
      }
    } else if (is.numeric(value) && length(value) > 1) {
      value_type <- "vector_numeric"
    } else if (is.integer(value) && length(value) > 1) {
      value_type <- "vector_integer"
    } else {
      stop("Unsupported value type. Only numeric, integer, vectors, and list are supported.")
    }
    
    # Prepare the data to be written as JSON
    if (value_type == "named_list_numeric") {
      json_data <- list(type = value_type, value = unname(value), names = names(value))
    } else {
      json_data <- list(type = value_type, value = value)
    }
    
    # Write the value to the file as JSON
    write_json(json_data, file_name)
    
    # Return the value
    return(value)
  }
}

geometric_sizes <- function(start_size, end_size, num_steps) {
  ratio <- (end_size / start_size)^(1 / (num_steps - 1))
  sizes <- start_size * (ratio ^ (0:(num_steps - 1)))
  return(round(sizes))
}

# Calculate performance score function
calculate_binary_performance <- function(actual, predicted) {
  confusion <- confusionMatrix(predicted, actual)
  performance <- confusion$overall['Accuracy']
  return(performance)
}


# Function to return the list of features with non-zero coefficients
nonzero_feature_indicies <- function(model, lambda) {
  coefs <- coef(model, s = lambda)
  nonzero_indices <- which(coefs != 0)
  # Exclude the intercept (first coefficient)
  nonzero_indices <- nonzero_indices[-1]
  return(nonzero_indices)
}


# Function to calculate performance at different thresholds
CFS_binary_logistic <- function(X, y, num_folds, num_bins, min_vars, geometric_spacing) {
  
  y_levels <- levels(y)  # Get the levels from y
  
  # Step 1: Calculate correlations
  correlations <- apply(X, 2, function(x) cor(x, as.numeric(y)))
  abs_correlations <- abs(correlations)
  
  # Step 2: Select features based on a correlation threshold
  select_features <- function(threshold) {
    selected_features <- which(abs_correlations >= threshold)
    return(selected_features)
  }
  
  # Step 3: Sweep through various thresholds and perform 10-fold cross-validation
  if (geometric_spacing){
    sizes <- geometric_sizes(ncol(X), min_vars, num_bins)
  }
  else {
    sizes <- round(seq(from = ncol(X), to = min_vars, length.out = num_bins))
  }
  
  percentiles <- 1 - sizes / ncol(X)
  thresholds <- quantile(abs_correlations, percentiles)
  results <- list()
  
  for (threshold in thresholds) {
    selected_features <- select_features(threshold)
    
    if (length(selected_features) == 0) {
      next
    }
    
    X_selected <- X[, selected_features, drop = FALSE]
    
    if (ncol(X_selected) < 2) {
      next
    }
    
    # Perform k-fold cross-validation
    # train_control <- trainControl(method = "cv", number = num_folds, verboseIter = TRUE)
    
    scores <- c()
    
    for (i in 1:num_folds) {
      folds <- createFolds(y, k = num_folds, list = TRUE, returnTrain = TRUE)
      fold_scores <- c()
      
      for (j in 1:num_folds) {
        train_index <- folds[[j]]
        test_index <- setdiff(seq_len(nrow(X_selected)), train_index)
        
        x_train_cv <- X_selected[train_index, ]
        y_train_cv <- y[train_index]
        x_test_cv <- X_selected[test_index, ]
        y_test_cv <- y[test_index]
        model_cv <- glmnet(x_train_cv, y_train_cv, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, alpha = baseline_alpha, family = "binomial", parallel = TRUE)
        
        pred_cv <- predict(model_cv, x_test_cv, s = min(model_cv$lambda), type = "response")
        pred_cv <- factor(ifelse(pred_cv > 0.5, y_levels[2], y_levels[1]), levels = y_levels)
        
        fold_scores <- c(fold_scores, calculate_binary_performance(y_test_cv, pred_cv))
      }
      
      scores <- c(scores, mean(fold_scores))
    }
    
    feature_indices <- toJSON(selected_features)
    results[[feature_indices]] <- mean(scores)
  }
  
  return(results)
}


RFE_binary_logistic <- function(X, y, num_folds, num_bins, min_vars, geometric_spacing) {
  results <- list()
  
  y_levels <- levels(y)  # Get the levels from y
  
  if (geometric_spacing){
    sizes <- geometric_sizes(ncol(X), min_vars, num_bins)
  }
  else {
    sizes <- round(seq(from = ncol(X), to = min_vars, length.out = num_bins))
  }
  
  current_features <- seq_len(ncol(X))  # Start with all features
  
  for (size in sizes) {
    cat("Evaluating size:", size, "\n")
    
    # Fit glmnet model to get coefficients
    model <- glmnet(X[, current_features, drop = FALSE], y, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)
    coefs <- as.matrix(coef(model, s = model$lambda.min))
    
    # Get indices of the top 'size' features by their absolute coefficient values
    if (length(current_features) > size) {
      selected_features <- order(abs(coefs[-1, 1]), decreasing = TRUE)[1:size]
      current_features <- current_features[selected_features]
    }
    
    x_selected <- X[, current_features, drop = FALSE]
    
    folds <- createFolds(y, k = num_folds, list = TRUE, returnTrain = TRUE)
    scores <- c()
    
    for (fold_idx in seq_along(folds)) {
      train_idx <- folds[[fold_idx]]
      test_idx <- setdiff(seq_len(nrow(x_selected)), train_idx)
      
      x_train_cv <- x_selected[train_idx, ]
      y_train_cv <- y[train_idx]
      x_test_cv <- x_selected[test_idx, ]
      y_test_cv <- y[test_idx]
      
      model_cv <- glmnet(x_train_cv, y_train_cv, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)
      
      if (length(model_cv$lambda) == 0) {
        next
      }
      
      pred_cv_prob <- predict(model_cv, x_test_cv, s = min(model_cv$lambda), type = "response")
      pred_cv <- factor(ifelse(pred_cv_prob > 0.5, y_levels[2], y_levels[1]), levels = y_levels)
      
      scores <- c(scores, calculate_binary_performance(y_test_cv, pred_cv))
    }
    
    mean_score <- mean(scores, na.rm = TRUE)
    feature_indices <- toJSON(current_features)
    results[[feature_indices]] <- mean_score
  }
  
  return(results)
}

subset_performance <- function(named_list) {
  # Extract lengths of the lists and corresponding performance scores
  lengths <- sapply(names(named_list), function(x) {
    list_obj <- fromJSON(x)
    length(list_obj)
  })
  
  performance <- as.numeric(named_list)
  
  # Create a data frame for plotting
  data <- data.frame(lengths = lengths, performance = performance)
  
  return(data)
}

download_and_extract_zip <- function(url, dest_dir) {
  # Ensure the destination directory exists
  if (!dir.exists(dest_dir)) {
    dir.create(dest_dir, recursive = TRUE)
  }
  
  # Create a temporary file to hold the downloaded zip file
  temp_zip <- tempfile(fileext = ".zip")
  
  # Download the zip file
  download.file(url, temp_zip, mode = "wb")
  
  # Extract the contents of the zip file
  unzip(temp_zip, exdir = dest_dir)
  
  # Remove the temporary zip file
  unlink(temp_zip)
}

download_kaggle_dataset <- function(dataset, path) {
  # Check if the kaggle command is available
  if (system("which kaggle", intern = TRUE) == "") {
    stop("Kaggle API is not installed or not in PATH. Please install it first.")
  }
  
  # Ensure the destination directory exists
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE)
  }
  
  # Construct the download command
  command <- sprintf("kaggle datasets download -d %s -p %s", dataset, path)
  
  # Execute the command
  system(command)
  
  # Unzip the downloaded file
  zipfile <- list.files(path, pattern = "*.zip", full.names = TRUE)
  if (length(zipfile) > 0) {
    unzip(zipfile, exdir = path)
    file.remove(zipfile)
  }
}

# download datasets, if necessary
# download_and_extract_zip("https://archive.ics.uci.edu/static/public/94/spambase.zip", "./data/spambase")
# download_kaggle_dataset("datatattle/covid-19-nlp-text-classification", "./data/covid")

```

```{r}
#Importing Spam Data
last_57_lines <- tail(readLines("./data/spambase/spambase.names"), 57)
# Function to extract and sanitize column names
extract_column_names <- function(line) {
  # Extract the name before the colon
  name <- strsplit(line, ":")[[1]][1]
  # Sanitize the name by replacing special characters with underscores
  sanitized_name <- gsub("[^a-zA-Z0-9_]", "_", name)
  return(sanitized_name)
}

# Apply the function to each line to get the column names
raw_column_names <- c(sapply(last_57_lines, extract_column_names), 'flag_spam')


# Ensure unique column names
unique_column_names <- make.unique(raw_column_names)
spam_data <- read.csv("./data/spambase/spambase.data", header = FALSE, col.names = unique_column_names) %>%
  mutate(flag_spam = factor(flag_spam, levels = c(0, 1), labels = c("Not Spam", "Spam")))
```

```{r}
# Importing Train Data
corona_train <- read.csv("./data/covid/Corona_NLP_train.csv")[, c("OriginalTweet", "Sentiment")]

# Importing Test Data
corona_test <- read.csv("./data/covid/Corona_NLP_test.csv")[, c("OriginalTweet", "Sentiment")]

# Recode Sentiment to factors
corona_train <- corona_train %>%
  filter(Sentiment != "Neutral") %>%
  mutate(Sentiment = recode(Sentiment,
                            "Extremely Negative" = "Negative",
                            "Extremely Positive" = "Positive"),
         Sentiment = factor(Sentiment, levels = c("Negative", "Positive")))

corona_test <- corona_test %>%
  filter(Sentiment != "Neutral") %>%
  mutate(Sentiment = recode(Sentiment,
                            "Extremely Negative" = "Negative",
                            "Extremely Positive" = "Positive"),
         Sentiment = factor(Sentiment, levels = c("Negative", "Positive")))

# Preprocessing and tokenization using quanteda
preprocess_text <- function(text_column) {
  tokens <- tokens(text_column, 
                   what = "word", 
                   remove_punct = TRUE, 
                   remove_numbers = TRUE,
                   remove_symbols = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stopwords("english")) %>%
    tokens_wordstem()
  
  # Create a Document-Feature Matrix (DFM)
  dfm <- dfm(tokens)
  
  return(dfm)
}

corona_train_dfm <- dfm_trim(preprocess_text(corona_train$OriginalTweet), min_termfreq = 10)
corona_test_dfm <- dfm_match(preprocess_text(corona_test$OriginalTweet), features = featnames(corona_train_dfm))

# Convert DFM to sparse matrix
corona_train_sparse <- as(corona_train_dfm, "dgCMatrix")
corona_test_sparse <- as(corona_test_dfm, "dgCMatrix")
```

```{r}
set.seed(42)

# Create a train-test split
trainIndex <- sample(1:nrow(spam_data), size = 0.8 * nrow(spam_data))

# Split the data
spam_train <- spam_data[trainIndex, ]
spam_test <- spam_data[-trainIndex, ]

X_train_raw <- spam_train %>% select(-flag_spam)
X_test_raw  <- spam_test %>% select(-flag_spam)

y_train  <- spam_train$flag_spam
y_test <- spam_test$flag_spam

levels = levels(y_train)

# scale the data, calculate all two-way interactions, and drop the intercept
preprocess_params <- preProcess(X_train_raw, method = c("center", "scale"))
X_train <- model.matrix( ~ .^2, predict(preprocess_params, X_train_raw))[, -1]
X_test <- model.matrix( ~ .^2, predict(preprocess_params, X_test_raw))[, -1]

spam_num_features <- ncol(X_train) 
spam_num_records<- nrow(X_train) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1] != 0)
total_elements <- prod(dim(X_train))
spam_sparsity <- ((total_elements - num_nonzero) / total_elements) * 100


baseline_model <- glmnet(
  X_train, 
  y_train, 
  family = "binomial", 
  alpha = baseline_alpha,
  lambda.min.ratio = baseline_lambda,
  maxit = baseline_maxit,
  parallel = TRUE
)

# Predict on training and test datasets
train_predictions_prob <- predict(baseline_model, X_train, s = min(baseline_model$lambda), type = "response")
test_predictions_prob <- predict(baseline_model, X_test, s = min(baseline_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train)
non_zero_features <- length(nonzero_feature_indicies(baseline_model, min(baseline_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

#save the results
spam_baseline_non_zero_features <- non_zero_features
spam_baseline_test_accuracy_score <- test_accuracy_score
spam_baseline_accuracy_difference <- accuracy_difference



#CORRELATION FEATURE SELECTION
cfs_feature_subsets <- cache_parameter('spam_cfs_feature_subsets')
if (is.null(cfs_feature_subsets)) {
  cfs_feature_subsets <- cache_parameter('spam_cfs_feature_subsets', CFS_binary_logistic(X_train, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(cfs_feature_subsets)[which.max(cfs_feature_subsets)])

# Step 4: Use the optimal threshold to train the final model
X_train_selected <- X_train[, optimal_subset, drop=FALSE]
X_test_selected <- X_test[, optimal_subset, drop=FALSE]

spam_num_features1 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_sparsity1 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, parallel = TRUE)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
spam_cfs_non_zero_features <- non_zero_features
spam_cfs_test_accuracy_score <- test_accuracy_score
spam_cfs_accuracy_difference <- accuracy_difference
spam_cfs_plot_data <- subset_performance(cfs_feature_subsets)


# RECURSIVE FEATURE ELIMINATION

feature_subsets <- cache_parameter('spam_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  feature_subsets <- cache_parameter('spam_rfe_feature_subsets', RFE_binary_logistic(X_train, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
X_train_selected = X_train[, optimal_subset, drop=FALSE]
X_test_selected = X_test[, optimal_subset, drop=FALSE]

spam_num_features2 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_sparsity2 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
spam_rfe_non_zero_features <- non_zero_features
spam_rfe_test_accuracy_score <- test_accuracy_score
spam_rfe_accuracy_difference <- accuracy_difference
spam_rfe_plot_data <- subset_performance(feature_subsets)


#LASSO REGRESSION
lambdas <- cache_parameter('spam_lasso_lambdas')
feature_subsets_sizes <- cache_parameter('spam_lasso_feature_subsets_sizes')
if (is.null(lambdas)) {
  cv_lasso <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 1, maxit = baseline_maxit, nfolds = validation_folds)
  lambda_values <- cv_lasso$lambda
  non_zero_features <- sapply(lambda_values, nonzero_feature_indicies, model = cv_lasso)
  non_zero_feature_sizes <- sapply(non_zero_features, length)
  performance <- cv_lasso$cvm
  lambdas <- cache_parameter('spam_lasso_lambdas', setNames(as.list(performance), as.character(lambda_values)))
  feature_subsets_sizes <- cache_parameter('spam_lasso_feature_subsets_sizes', setNames(as.list(performance), as.character(non_zero_feature_sizes)))
}
optimal_lambda <- as.double(names(lambdas)[which.min(lambdas)])


# Fit the final Lasso model using the best lambda
spam_lasso_final_model <- glmnet(X_train, y_train, family = "binomial", maxit = baseline_maxit, alpha = 1, lambda.min.ratio = optimal_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(spam_lasso_final_model, newx = X_train, s = optimal_lambda, type = "response")
test_predictions_prob <- predict(spam_lasso_final_model, newx = X_test, s = optimal_lambda, type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train) 
non_zero_features <- length(nonzero_feature_indicies(spam_lasso_final_model, min(spam_lasso_final_model$lambda)))

spam_num_features3 <- non_zero_features

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save the results
spam_lasso_non_zero_features <- non_zero_features
spam_lasso_test_accuracy_score <- test_accuracy_score
spam_lasso_accuracy_difference <- accuracy_difference
spam_lasso_plot_data <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), performance = as.numeric(feature_subsets_sizes))
spam_lasso_plot_data_lambda <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), lambdas = as.double(names(lambdas)))


#CFS + RFE REGRESSION

starting_features = fromJSON(names(cfs_feature_subsets)[3])
feature_subsets <- cache_parameter('spam_cfs_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  X_train_cfs_selected <- X_train[, starting_features, drop = FALSE]
  feature_subsets <- cache_parameter('spam_cfs_rfe_feature_subsets', RFE_binary_logistic(X_train_cfs_selected, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = FALSE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
X_train_selected = X_train[, optimal_subset, drop=FALSE]
X_test_selected = X_test[, optimal_subset, drop=FALSE]

spam_num_features4 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_sparsity4 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
spam_cfs_rfe_non_zero_features <- non_zero_features
spam_cfs_rfe_test_accuracy_score <- test_accuracy_score
spam_cfs_rfe_accuracy_difference <- accuracy_difference
spam_cfs_rfe_plot_data <- subset_performance(feature_subsets)

```

```{r}
set.seed(42)

# Convert DFM to sparse matrix
train_sparse <- corona_train_sparse
test_sparse <- corona_test_sparse

# Convert Sentiment to numeric for glmnet
y_train <- corona_train$Sentiment
y_test <- corona_test$Sentiment

levels = levels(y_train)

covid_num_features <- ncol(train_sparse) 
covid_num_records<- nrow(train_sparse) 
num_nonzero <- sum(train_sparse != 0)
total_elements <- prod(dim(train_sparse))
covid_sparsity <- ((total_elements - num_nonzero) / total_elements) * 100

# Fit the logistic regression model with glmnet

baseline_model <- glmnet(
  train_sparse, 
  y_train, 
  family = "binomial", 
  alpha = baseline_alpha,
  lambda.min.ratio = baseline_lambda,
  maxit = baseline_maxit,
  parallel = TRUE
)

# Predict on training and test datasets
train_predictions_prob <- predict(baseline_model, train_sparse, s = min(baseline_model$lambda), type = "response")
test_predictions_prob <- predict(baseline_model, test_sparse, s = min(baseline_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse)
non_zero_features <- length(nonzero_feature_indicies(baseline_model, min(baseline_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
covid_baseline_non_zero_features <- non_zero_features
covid_baseline_test_accuracy_score <- test_accuracy_score
covid_baseline_accuracy_difference <- accuracy_difference

#CORRELATION FEATURE SELECTION
cfs_feature_subsets <- cache_parameter('covid_cfs_feature_subsets')
if (is.null(cfs_feature_subsets)) {
  cfs_feature_subsets <- cache_parameter('covid_cfs_feature_subsets', CFS_binary_logistic(train_sparse, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(cfs_feature_subsets)[which.max(cfs_feature_subsets)])

# Step 4: Use the optimal threshold to train the final model
train_sparse_selected <- train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected <- test_sparse[, optimal_subset, drop=FALSE]

covid_num_features1 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_sparsity1 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, parallel = TRUE)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
covid_cfs_non_zero_features <- non_zero_features
covid_cfs_test_accuracy_score <- test_accuracy_score
covid_cfs_accuracy_difference <- accuracy_difference
covid_cfs_plot_data <- subset_performance(cfs_feature_subsets)


# RECURSIVE FEATURE ELIMINATION

feature_subsets <- cache_parameter('covid_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  feature_subsets <- cache_parameter('covid_rfe_feature_subsets', RFE_binary_logistic(train_sparse, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
train_sparse_selected = train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected = test_sparse[, optimal_subset, drop=FALSE]

covid_num_features2 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_sparsity2 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
covid_rfe_non_zero_features <- non_zero_features
covid_rfe_test_accuracy_score <- test_accuracy_score
covid_rfe_accuracy_difference <- accuracy_difference
covid_rfe_plot_data <- subset_performance(feature_subsets)


#LASSO REGRESSION
lambdas <- cache_parameter('covid_lasso_lambdas')
feature_subsets_sizes <- cache_parameter('covid_lasso_feature_subsets_sizes')
if (is.null(lambdas)) {
  cv_lasso <- cv.glmnet(train_sparse, y_train, family = "binomial", alpha = 1, maxit = baseline_maxit, nfolds = validation_folds)
  lambda_values <- cv_lasso$lambda
  non_zero_features <- sapply(lambda_values, nonzero_feature_indicies, model = cv_lasso)
  non_zero_feature_sizes <- sapply(non_zero_features, length)
  performance <- cv_lasso$cvm
  lambdas <- cache_parameter('covid_lasso_lambdas', setNames(as.list(performance), as.character(lambda_values)))
  feature_subsets_sizes <- cache_parameter('covid_lasso_feature_subsets_sizes', setNames(as.list(performance), as.character(non_zero_feature_sizes)))
}
optimal_lambda <- as.double(names(lambdas)[which.min(lambdas)])


# Fit the final Lasso model using the best lambda
final_model <- glmnet(train_sparse, y_train, family = "binomial", maxit = baseline_maxit, alpha = 1, lambda.min.ratio = optimal_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse, s = optimal_lambda, type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse, s = optimal_lambda, type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

covid_num_features3 <- non_zero_features

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save the results
covid_lasso_non_zero_features <- non_zero_features
covid_lasso_test_accuracy_score <- test_accuracy_score
covid_lasso_accuracy_difference <- accuracy_difference
covid_lasso_plot_data <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), performance = as.numeric(feature_subsets_sizes))
covid_lasso_plot_data_lambda <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), lambdas = as.double(names(lambdas)))

#CFS + RFE REGRESSION

starting_features = fromJSON(names(cfs_feature_subsets)[3])
feature_subsets <- cache_parameter('covid_cfs_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  train_sparse_cfs_selected <- train_sparse[, starting_features, drop = FALSE]
  feature_subsets <- cache_parameter('covid_cfs_rfe_feature_subsets', RFE_binary_logistic(train_sparse_cfs_selected, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = FALSE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
train_sparse_selected = train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected = test_sparse[, optimal_subset, drop=FALSE]

covid_num_features4 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_sparsity4 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save results
covid_cfs_rfe_non_zero_features <- non_zero_features
covid_cfs_rfe_test_accuracy_score <- test_accuracy_score
covid_cfs_rfe_accuracy_difference <- accuracy_difference
covid_cfs_rfe_plot_data <- subset_performance(feature_subsets)

```

```{r}
set.seed(42)

spam_alt_data <- sample_frac(spam_data, .1)


# Create a train-test split
trainIndex <- sample(1:nrow(spam_alt_data), size = 0.8 * nrow(spam_alt_data))

# Split the data
spam_alt_train <- spam_alt_data[trainIndex, ]
spam_alt_test <- spam_alt_data[-trainIndex, ]

X_train_raw <- spam_alt_train %>% select(-flag_spam)
X_test_raw  <- spam_alt_test %>% select(-flag_spam)

y_train  <- spam_alt_train$flag_spam
y_test <- spam_alt_test$flag_spam

levels = levels(y_train)

# scale the data, calculate all two-way interactions, and drop the intercept
preprocess_params <- preProcess(X_train_raw, method = c("center", "scale"))
X_train <- model.matrix( ~ .^2, predict(preprocess_params, X_train_raw))[, -1]
X_test <- model.matrix( ~ .^2, predict(preprocess_params, X_test_raw))[, -1]

spam_alt_num_features <- ncol(X_train) 
spam_alt_num_records<- nrow(X_train) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1] != 0)
total_elements <- prod(dim(X_train))
spam_alt_sparsity <- ((total_elements - num_nonzero) / total_elements) * 100


baseline_model <- glmnet(
  X_train, 
  y_train, 
  family = "binomial", 
  alpha = baseline_alpha,
  lambda.min.ratio = baseline_lambda,
  maxit = baseline_maxit,
  parallel = TRUE
)

# Predict on training and test datasets
train_predictions_prob <- predict(baseline_model, X_train, s = min(baseline_model$lambda), type = "response")
test_predictions_prob <- predict(baseline_model, X_test, s = min(baseline_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_alt_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_alt_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train)
non_zero_features <- length(nonzero_feature_indicies(baseline_model, min(baseline_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

#save the results
spam_alt_baseline_non_zero_features <- non_zero_features
spam_alt_baseline_test_accuracy_score <- test_accuracy_score
spam_alt_baseline_accuracy_difference <- accuracy_difference



#CORRELATION FEATURE SELECTION
cfs_feature_subsets <- cache_parameter('spam_alt_cfs_feature_subsets')
if (is.null(cfs_feature_subsets)) {
  cfs_feature_subsets <- cache_parameter('spam_alt_cfs_feature_subsets', CFS_binary_logistic(X_train, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(cfs_feature_subsets)[which.max(cfs_feature_subsets)])

# Step 4: Use the optimal threshold to train the final model
X_train_selected <- X_train[, optimal_subset, drop=FALSE]
X_test_selected <- X_test[, optimal_subset, drop=FALSE]

spam_alt_num_features1 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_alt_sparsity1 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, parallel = TRUE)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_alt_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_alt_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
spam_alt_cfs_non_zero_features <- non_zero_features
spam_alt_cfs_test_accuracy_score <- test_accuracy_score
spam_alt_cfs_accuracy_difference <- accuracy_difference
spam_alt_cfs_plot_data <- subset_performance(cfs_feature_subsets)


# RECURSIVE FEATURE ELIMINATION

feature_subsets <- cache_parameter('spam_alt_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  feature_subsets <- cache_parameter('spam_alt_rfe_feature_subsets', RFE_binary_logistic(X_train, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
X_train_selected = X_train[, optimal_subset, drop=FALSE]
X_test_selected = X_test[, optimal_subset, drop=FALSE]

spam_alt_num_features2 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_alt_sparsity2 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_alt_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_alt_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
spam_alt_rfe_non_zero_features <- non_zero_features
spam_alt_rfe_test_accuracy_score <- test_accuracy_score
spam_alt_rfe_accuracy_difference <- accuracy_difference
spam_alt_rfe_plot_data <- subset_performance(feature_subsets)


#LASSO REGRESSION
lambdas <- cache_parameter('spam_alt_lasso_lambdas')
feature_subsets_sizes <- cache_parameter('spam_alt_lasso_feature_subsets_sizes')
if (is.null(lambdas)) {
  cv_lasso <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 1, maxit = baseline_maxit, nfolds = validation_folds)
  lambda_values <- cv_lasso$lambda
  non_zero_features <- sapply(lambda_values, nonzero_feature_indicies, model = cv_lasso)
  non_zero_feature_sizes <- sapply(non_zero_features, length)
  performance <- cv_lasso$cvm
  lambdas <- cache_parameter('spam_alt_lasso_lambdas', setNames(as.list(performance), as.character(lambda_values)))
  feature_subsets_sizes <- cache_parameter('spam_alt_lasso_feature_subsets_sizes', setNames(as.list(performance), as.character(non_zero_feature_sizes)))
}
optimal_lambda <- as.double(names(lambdas)[which.min(lambdas)])


# Fit the final Lasso model using the best lambda
final_model <- glmnet(X_train, y_train, family = "binomial", maxit = baseline_maxit, alpha = 1, lambda.min.ratio = optimal_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train, s = optimal_lambda, type = "response")
test_predictions_prob <- predict(final_model, newx = X_test, s = optimal_lambda, type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_alt_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_alt_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

spam_alt_num_features3 <- non_zero_features

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save the results
spam_alt_lasso_non_zero_features <- non_zero_features
spam_alt_lasso_test_accuracy_score <- test_accuracy_score
spam_alt_lasso_accuracy_difference <- accuracy_difference
spam_alt_lasso_plot_data <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), performance = as.numeric(feature_subsets_sizes))
spam_alt_lasso_plot_data_lambda <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), lambdas = as.double(names(lambdas)))


#CFS + RFE REGRESSION

starting_features = fromJSON(names(cfs_feature_subsets)[3])
feature_subsets <- cache_parameter('spam_alt_cfs_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  X_train_cfs_selected <- X_train[, starting_features, drop = FALSE]
  feature_subsets <- cache_parameter('spam_alt_cfs_rfe_feature_subsets', RFE_binary_logistic(X_train_cfs_selected, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = FALSE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
X_train_selected = X_train[, optimal_subset, drop=FALSE]
X_test_selected = X_test[, optimal_subset, drop=FALSE]

spam_alt_num_features4 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_alt_sparsity4 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_alt_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_alt_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
spam_alt_cfs_rfe_non_zero_features <- non_zero_features
spam_alt_cfs_rfe_test_accuracy_score <- test_accuracy_score
spam_alt_cfs_rfe_accuracy_difference <- accuracy_difference
spam_alt_cfs_rfe_plot_data <- subset_performance(feature_subsets)


```

```{r}
set.seed(42)

train_dfm <- dfm_trim(preprocess_text(corona_train$OriginalTweet), min_termfreq = 2)
test_dfm <- dfm_match(preprocess_text(corona_test$OriginalTweet), features = featnames(train_dfm))

# Convert DFM to sparse matrix
train_sparse <- as(train_dfm, "dgCMatrix")
test_sparse <- as(test_dfm, "dgCMatrix")

# Convert Sentiment to numeric for glmnet
y_train <- corona_train$Sentiment
y_test <- corona_test$Sentiment

levels = levels(y_train)


covid_alt_num_features <- ncol(train_sparse) 
covid_alt_num_records <- nrow(train_sparse) 
num_nonzero <- sum(train_sparse != 0)
total_elements <- prod(dim(train_sparse))
covid_alt_sparsity <- ((total_elements - num_nonzero) / total_elements) * 100

# Fit the logistic regression model with glmnet

baseline_model <- glmnet(
  train_sparse, 
  y_train, 
  family = "binomial", 
  alpha = baseline_alpha,
  lambda.min.ratio = baseline_lambda,
  maxit = baseline_maxit,
  parallel = TRUE
)

# Predict on training and test datasets
train_predictions_prob <- predict(baseline_model, train_sparse, s = min(baseline_model$lambda), type = "response")
test_predictions_prob <- predict(baseline_model, test_sparse, s = min(baseline_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse)
non_zero_features <- length(nonzero_feature_indicies(baseline_model, min(baseline_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
covid_alt_baseline_non_zero_features <- non_zero_features
covid_alt_baseline_test_accuracy_score <- test_accuracy_score
covid_alt_baseline_accuracy_difference <- accuracy_difference

#CORRELATION FEATURE SELECTION
cfs_feature_subsets <- cache_parameter('covid_alt_cfs_feature_subsets')
if (is.null(cfs_feature_subsets)) {
  cfs_feature_subsets <- cache_parameter('covid_alt_cfs_feature_subsets', CFS_binary_logistic(train_sparse, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(cfs_feature_subsets)[which.max(cfs_feature_subsets)])

# Step 4: Use the optimal threshold to train the final model
train_sparse_selected <- train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected <- test_sparse[, optimal_subset, drop=FALSE]

covid_alt_num_features1 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_alt_sparsity1 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, parallel = TRUE)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
covid_alt_cfs_non_zero_features <- non_zero_features
covid_alt_cfs_test_accuracy_score <- test_accuracy_score
covid_alt_cfs_accuracy_difference <- accuracy_difference
covid_alt_cfs_plot_data <- subset_performance(cfs_feature_subsets)


# RECURSIVE FEATURE ELIMINATION

feature_subsets <- cache_parameter('covid_alt_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  feature_subsets <- cache_parameter('covid_alt_rfe_feature_subsets', RFE_binary_logistic(train_sparse, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
train_sparse_selected = train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected = test_sparse[, optimal_subset, drop=FALSE]

covid_alt_num_features2 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_alt_sparsity2 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
covid_alt_rfe_non_zero_features <- non_zero_features
covid_alt_rfe_test_accuracy_score <- test_accuracy_score
covid_alt_rfe_accuracy_difference <- accuracy_difference
covid_alt_rfe_plot_data <- subset_performance(feature_subsets)


#LASSO REGRESSION
lambdas <- cache_parameter('covid_alt_lasso_lambdas')
feature_subsets_sizes <- cache_parameter('covid_alt_lasso_feature_subsets_sizes')
if (is.null(lambdas)) {
  cv_lasso <- cv.glmnet(train_sparse, y_train, family = "binomial", alpha = 1, maxit = baseline_maxit, nfolds = validation_folds)
  lambda_values <- cv_lasso$lambda
  non_zero_features <- sapply(lambda_values, nonzero_feature_indicies, model = cv_lasso)
  non_zero_feature_sizes <- sapply(non_zero_features, length)
  performance <- cv_lasso$cvm
  lambdas <- cache_parameter('covid_alt_lasso_lambdas', setNames(as.list(performance), as.character(lambda_values)))
  feature_subsets_sizes <- cache_parameter('covid_alt_lasso_feature_subsets_sizes', setNames(as.list(performance), as.character(non_zero_feature_sizes)))
}
optimal_lambda <- as.double(names(lambdas)[which.min(lambdas)])


# Fit the final Lasso model using the best lambda
final_model <- glmnet(train_sparse, y_train, family = "binomial", maxit = baseline_maxit, alpha = 1, lambda.min.ratio = optimal_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse, s = optimal_lambda, type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse, s = optimal_lambda, type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

covid_alt_num_features3 <-non_zero_features

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save the results
covid_alt_lasso_non_zero_features <- non_zero_features
covid_alt_lasso_test_accuracy_score <- test_accuracy_score
covid_alt_lasso_accuracy_difference <- accuracy_difference
covid_alt_lasso_plot_data <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), performance = as.numeric(feature_subsets_sizes))
covid_alt_lasso_plot_data_lambda <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), lambdas = as.double(names(lambdas)))

#CFS + RFE REGRESSION

starting_features = fromJSON(names(cfs_feature_subsets)[3])
feature_subsets <- cache_parameter('covid_alt_cfs_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  train_sparse_cfs_selected <- train_sparse[, starting_features, drop = FALSE]
  feature_subsets <- cache_parameter('covid_alt_cfs_rfe_feature_subsets', RFE_binary_logistic(train_sparse_cfs_selected, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = FALSE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
train_sparse_selected = train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected = test_sparse[, optimal_subset, drop=FALSE]

covid_alt_num_features4 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_alt_sparsity4 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save results
covid_alt_cfs_rfe_non_zero_features <- non_zero_features
covid_alt_cfs_rfe_test_accuracy_score <- test_accuracy_score
covid_alt_cfs_rfe_accuracy_difference <- accuracy_difference
covid_alt_cfs_rfe_plot_data <- subset_performance(feature_subsets)

```

## Introduction

What is feature selection? Why do we care?

- Feature selection is a crucial step in data preprocessing, especially in machine learning and statistical modeling. It involves selecting a subset of relevant features (variables, predictors) for building a model.
- It is important because it reduces the risk of overfitting and enhances the performance of machine learning models and pattern recognition systems.
- The goal/ objective of the study is to compare the performance of various feature selection methods on a variety of real-world datasets.


## Introduction: Early Techniques

Forward, backward, and stepwise variable selection in linear models.

- The forward method starts with no variables in the model, and we will add them one by one until no improvement is shown.
- Backward is when we start with all variables and remove the least significance iteratively.
- Stepwise selection combines both approaches, iteratively adding and removing variables to optimize the models performance.

## Introduction: Early Techniques

Univariate screening procedures (USP).

- USP represents an important milestone for feature selection.
- These methods involve evaluating each predictor variable individually to determine its relationship with the target variable. The process selects variables that meet a specific statistical threshold.
- While these methods are simple and quick to use, they often miss the complex connections between variables.

## Introduction: Modern Techniques

Classification of methods.

- The **filter** model selects features based on the general properties of the training data, independent of any learning algorithm, making it computationally efficient for large datasets. 
- The **wrapper** model, on the other hand, uses a specific learning algorithm to evaluate and determine which features to keep, often leading to better performance but at a higher computational cost. 
- **Embedded** methods perform feature selection during the model training process, integrating selection directly with learning.
- **Hybrid** methods combine the best aspects of filter and wrapper methods to achieve optimal performance with manageable computational complexity.


## Methods: Correlation - Based Feature Selection (CFS)

-   Filter method.
-   Uses the correlation coefficient to measure the relationship between
    each variable and the target, independently.
-  Features that are highly correlated are considered important.

CFS's feature subset evaluation function is:

$\text{Ms} = \frac{k\bar{\text{rcf}}}{\sqrt k + k(k - 1)\bar{\text{rff}}}$

## Methods: Recursive Feature Elimination (RFE)

-   Wrapper method.
-   Removes variables iteratively.

Steps:

1.  Train the classifier.
2.  compute the ranking criterion for all features.
3.  remove the feature(s) with smallest ranking values.
4.  Repeat until desired number of features selected.

- optimal subset of features - one that provides the highest accuracy.


## Methods: Least Absolute Shrinkage and Selection Operator (LASSO)

-   Embedded method
-   LASSO finds the coefficient of features
-   Uses L1 Regularization to shrink coefficients to zero

The LASSO estimate is defined by the solution to the l1 optimization
problem

minimize $\frac{\| Y - X\beta \|_2^2}{n}$ subject to
$X \sum_{j=1}^{k} \|\beta_j\|_1 < t$

where $t$ is the upper bound for the sum of the coefficients.

## Methods: CFS & RFE

-   Hybrid method.
-   Combines filter and wrapper methods.

Step 1: Filtering using the correlation coefficient. (e.g. trim the
"low hanging fruit"). 

Step 2: Remove remaining variables iteratively using RFE.

## Analysis: Spambase Dataset

-   4601 instances of emails
-   57 features for classification tasks.
-   Binary classification: email is spam (1) or not (0).
-   80/20 train/test split.
-   Increased number of features by adding all $(\binom{57}{2} = 1,596)$ two-way interactions.

## Analysis: Spambase Dataset

Figure 1. Frequency of spam targets.
```{r}
ggplot(spam_data, aes(x = flag_spam, fill = flag_spam)) +
  geom_bar(color = "black") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 10, color = "darkblue"),
    axis.text.y = element_text(size = 10, color = "darkblue"),
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "lightgrey"),
    panel.grid.minor = element_blank()
  ) +
  labs(x = "Spam Or Not", y = "Count", title = "Distribution of Spam vs Not Spam") +
  scale_y_continuous(labels = scales::comma) +
  scale_x_discrete(labels = c("Not Spam" = "Not Spam", "Spam" = "Spam")) +
  scale_fill_manual(values = c("Not Spam" = "red", "Spam" = "blue")) +
  guides(fill = guide_legend(title = NULL))
```

## Analysis: COVID-19 NLP Text Classification Dataset

-   45k tweets related to COVID-19, labeled for sentiment analysis.
-   Five Sentiment label classes, ranging from extremely positive to
    extremely negative.
-   Recoded to a binary classification task, positive (1) or negative (0).
-   33,444 (91%) training and 3,179 (9%) testing records after recoding.

## Analysis: COVID-19 NLP Text Classification Dataset
Figure 2. Frequency of sentiment targets.
```{r}

ggplot(corona_train, aes(x = Sentiment, fill = Sentiment)) +
  geom_bar(color = "black") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 10, color = "darkblue"),
    axis.text.y = element_text(size = 10, color = "darkblue"),
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "lightgrey"),
    panel.grid.minor = element_blank()
  ) +
  labs(x = "Sentiment", y = "Count", title = "Distribution of Sentiment Labels") +
  scale_y_continuous(labels = scales::comma) +
  scale_x_discrete(labels = c("Positive" = "Positive", "Negative" = "Negative")) +
  scale_fill_manual(values = c("Negative" = "red", "Positive" = "blue")) +
  guides(fill = guide_legend(title = NULL))
```


## Analysis: COVID-19 NLP Text Classification Dataset

"Bag of words"

-   Text data converted into a matrix of word frequencies.
-   Each row represents a document.
-   Each column represents a unique word from the entire corpus.
-   Large (several thousand variables), sparse (> 99% of values = 0) feature set.

## Statistical Modeling

Three metrics:

- **Accuracy** on the test set.
- Difference between accuracy on the training and test set (**overfitting**).
- Number of variables selected (model **complexity**).

5 models:

- **Baseline** Full logistics model with no feature selection.
- **CFS** Select best of 20 correlation thresholds using cross validation.
- **RFE** Select best of 20 sized subsets using cross validation. 
- **LASSO** Select best penalty term using cross validation. 
- **CFS + RFE** Remove 20% of variables with lowest correlation, Select best of 20 sized subsets using cross validation.


## Results: Spambase Dataset

Table 1. Results of spam classification task.
```{r}


# Create visuals for Spambase dataset
var_method <- c('Baseline', 'CFS', 'RFE', 'LASSO', 'CFS + RFE')
num_features <- c(spam_baseline_non_zero_features, spam_cfs_non_zero_features, spam_rfe_non_zero_features, spam_lasso_non_zero_features, spam_cfs_rfe_non_zero_features)
accuracy_score <- c(spam_baseline_test_accuracy_score, spam_cfs_test_accuracy_score, spam_rfe_test_accuracy_score, spam_lasso_test_accuracy_score, spam_cfs_rfe_test_accuracy_score)
accuracy_diff <- c(spam_baseline_accuracy_difference, spam_cfs_accuracy_difference, spam_rfe_accuracy_difference, spam_lasso_accuracy_difference, spam_cfs_rfe_accuracy_difference)

data <- data.frame(var_method, num_features, accuracy_score, accuracy_diff)

# Format numbers
data <- data %>%
  mutate(
    accuracy_score = round(accuracy_score, 3),
    accuracy_diff = round(accuracy_diff, 3)
  )

# Create and render the table
kable(data, format = "html", col.names = c("Method", "Number of Features", "Test Accuracy Score", "Accuracy Decrease from Train")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## Results: COVID-19 NLP Text Classification Dataset

Table 2. Results of sentiment classification task.
```{r}
# Create visuals for Covid dataset
var_method <- c('Baseline', 'CFS', 'RFE', 'LASSO', 'CFS + RFE')
num_features <- c(covid_baseline_non_zero_features, covid_cfs_non_zero_features, covid_rfe_non_zero_features, covid_lasso_non_zero_features, covid_cfs_rfe_non_zero_features)
accuracy_score <- c(covid_baseline_test_accuracy_score, covid_cfs_test_accuracy_score, covid_rfe_test_accuracy_score, covid_lasso_test_accuracy_score, covid_cfs_rfe_test_accuracy_score)
accuracy_diff <- c(covid_baseline_accuracy_difference, covid_cfs_accuracy_difference, covid_rfe_accuracy_difference, covid_lasso_accuracy_difference, covid_cfs_rfe_accuracy_difference)

data <- data.frame(var_method, num_features, accuracy_score, accuracy_diff)

# Format numbers
data <- data %>%
  mutate(
    accuracy_score = round(accuracy_score, 3),
    accuracy_diff = round(accuracy_diff, 3)
  )

# Create and render the table
kable(data, format = "html", col.names = c("Method", "Number of Features", "Test Accuracy Score", "Accuracy Decrease from Train")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## Results: Observations per Feature

- Decreased the ratio of observations per feature.
- Selected 10% of records for spam task.
- Increased features for sentiment task by decreasing threshold for "rare" words from 10 to 2.
- Ran analysis on new datasets.
- Reported overfitting as a function of observations per feature.


## Results: Observations per Feature
Figure 3. Plot of overfitting as a function of observations per feature.
```{r}
# Create visuals for trending
datasets <- c('Spam Baseline', 'Covid Baseline', 'Spam Alt Baseline', 'Covid Alt Baseline',
              'Spam CFS', 'Covid CFS', 'Spam Alt CFS', 'Covid Alt CFS',
              'Spam RFE', 'Covid RFE', 'Spam Alt RFE', 'Covid Alt RFE',
              'Spam LASSO', 'Covid LASSO', 'Spam Alt LASSO', 'Covid Alt LASSO',
              'Spam RFECFS', 'Covid RFECFS', 'Spam Alt RFECFS', 'Covid Alt RFECFS')
var_ratio <- c(spam_num_records / spam_num_features, covid_num_records / covid_num_features, spam_alt_num_records / spam_alt_num_features, covid_alt_num_records / covid_alt_num_features,
               spam_num_records / spam_num_features1, covid_num_records / covid_num_features1, spam_alt_num_records / spam_alt_num_features1, covid_alt_num_records / covid_alt_num_features1,
               spam_num_records / spam_num_features2, covid_num_records / covid_num_features2, spam_alt_num_records / spam_alt_num_features2, covid_alt_num_records / covid_alt_num_features2,
               spam_num_records / spam_num_features3, covid_num_records / covid_num_features3, spam_alt_num_records / spam_alt_num_features3, covid_alt_num_records / covid_alt_num_features3,
               spam_num_records / spam_num_features4, covid_num_records / covid_num_features4, spam_alt_num_records / spam_alt_num_features4, covid_alt_num_records / covid_alt_num_features4)
accuracy_diff <- c(spam_baseline_accuracy_difference, covid_baseline_accuracy_difference, spam_alt_baseline_accuracy_difference, covid_alt_baseline_accuracy_difference,
                   spam_cfs_accuracy_difference, covid_cfs_accuracy_difference, spam_alt_cfs_accuracy_difference, covid_alt_cfs_accuracy_difference,
                   spam_rfe_accuracy_difference, covid_rfe_accuracy_difference, spam_alt_rfe_accuracy_difference, covid_alt_rfe_accuracy_difference,
                   spam_lasso_accuracy_difference, covid_lasso_accuracy_difference, spam_alt_lasso_accuracy_difference, covid_alt_lasso_accuracy_difference,
                   spam_cfs_rfe_accuracy_difference, covid_cfs_rfe_accuracy_difference, spam_alt_cfs_rfe_accuracy_difference, covid_alt_cfs_rfe_accuracy_difference)
sparsity <- c(spam_sparsity, covid_sparsity, spam_alt_sparsity, covid_alt_sparsity,
              spam_sparsity1, covid_sparsity1, spam_alt_sparsity1, covid_alt_sparsity1,
              spam_sparsity2, covid_sparsity2, spam_alt_sparsity2, covid_alt_sparsity2,
              NA, NA, NA, NA,
              spam_sparsity4, covid_sparsity4, spam_alt_sparsity4, covid_alt_sparsity4)

data <- data.frame(datasets, var_ratio, accuracy_diff, sparsity)

# Format numbers
data <- data %>%
  mutate(
    var_ratio = round(var_ratio, 1),
    accuracy_diff = round(accuracy_diff, 3),
    sparsity = round(sparsity, 1)
  )

ggplot(data, aes(x = var_ratio, y = accuracy_diff)) +
  geom_point(aes(color = datasets)) +
  # geom_text(aes(label = paste(datasets, "\nSparsity:", paste0(sparsity, "%"))), vjust = -0.3, hjust = 0) +
  geom_smooth(method = "lm", formula = y ~ log(x), se = FALSE, color = "blue") +
  labs(x = "Observations per Feature", y = "Baseline Accuracy Decrease from Train", title = "Accuracy Decrease vs Observations per Feature") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 10, color = "darkblue"),
    axis.text.y = element_text(size = 10, color = "darkblue"),
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "lightgrey"),
    panel.grid.minor = element_blank()
  ) +
  # expand_limits(y = c(0, max(data$accuracy_diff) + 0.05), x = c(0, max(data$var_ratio) + 2)) +
  guides(color = "none")

```

## Summary

- Selecting fewer features always results in less overfitting.
- Sometimes it can improve performance.
- There is not a "best" feature elimination method.
- Trade-offs required between model complexity, test accuracy, overfitting, and required compute resources.

## Conclusion

Is feature selection still relevant?

- Large "big data" datasets have a high ratio of observations to features, even for thousands of features.
- Models still need to find an optimum solution; a smaller feature space means less exploration is needed to find the best model.
- Pragmatic constraints: Documentation and dependencies.


**Thanks for listening!**

