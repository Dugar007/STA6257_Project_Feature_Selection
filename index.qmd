---
title: "Feature Selection: An Exploration of Algorithm Performance, Selection Bias, and Multicollinearity "
author: "Jason Case, Abhishek Dugar. Daniel Nkuah, Khoa Tran"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---
## Introduction
The discipline of feature selection in machine learning has evolved significantly over the past few decades, transforming from a practice once stigmatized as "data dredging" to an essential component of modern predictive modeling. Early methods for variable selection were initially met with skepticism by the statistical community, which viewed them as potentially spurious means of uncovering patterns in data. These reservations stemmed from concerns about overfitting and the robustness of statistical inferences drawn from such techniques.
 
### Early Methods
Feature selection's early forays began with relatively simple approaches like forward, backward, and stepwise variable selection in linear models. Forward selection starts with no variables in the model, adding them one by one based on specific criteria until no significant improvement is observed. Conversely, backward selection begins with all candidate variables, removing the least significant ones iteratively. Stepwise selection combines both approaches, iteratively adding and removing variables to optimize model performance.
 
These methods were foundational but had limitations, particularly when applied to datasets with high dimensionality. For instance, @foster2004variable in their study on predicting bankruptcy highlighted the complexity of variable selection when dealing with large sets of predictors. They utilized a 20:80 train/validation split to ensure robust model evaluation and critiqued traditional criteria like the Akaike Information Criterion (AIC) for being too liberal, risking overfitting. Instead, they advocated for methods like the Bonferroni correction, which, though conservative, provided more reliable results by dynamically adjusting p-values and avoiding inflated values from high leverage points.
 
Univariate screening procedures marked another step in the evolution of feature selection. These methods involve assessing each predictor variable independently for its relationship with the target variable, selecting those that meet a certain statistical threshold. While straightforward and computationally inexpensive, univariate methods often fail to capture the complex interdependencies between variables. 
 
In the context of gene selection for cancer classification, @guyon2002gene demonstrated the limitations of univariate methods. Prior to their work, gene selection relied heavily on simple statistical measures such as correlations, which often resulted in large, redundant sets of genes. The advent of Recursive Feature Elimination (RFE) represented a significant improvement. RFE uses Support Vector Machines (SVMs) to iteratively eliminate the least important features, refining the feature set to a more relevant subset and thus enhancing model performance. This method highlighted the critical role of robust feature selection in bioinformatics, where the choice of features often influenced classification outcomes more than the specific classifier used.
 
### Modern Methods
Modern feature selection methods have become essential for data preprocessing, extracting the most important information from big data. By reducing the number of features, these methods mitigate the risk of overfitting and enhance the performance of machine learning models and pattern recognition systems [@venkatesh2019review].
 
Advanced feature selection methods include similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based approaches. Similarity-based approaches select features based on their similarity or dissimilarity, while information-theoretical-based approaches use concepts like entropy and mutual information to evaluate feature importance. Sparse-learning-based approaches focus on identifying a small, essential set of features by limiting the number of features selected, and statistical-based approaches utilize statistical tests and models to determine the significance of features for predicting the target variable [@li2017feature].
 
Early and modern approaches are now categorized into filter, wrapper, embedded, and hybrid approaches. The filter model selects features based on the general properties of the training data, independent of any learning algorithm, making it computationally efficient for large datasets [@das2001filters; @kohavi1997wrappers]. Within the filter model, feature selection algorithms can be further divided into feature weighting algorithms, which assign weights to each feature based on their relevance, and subset search algorithms, which explore candidate feature subsets based on specific evaluation measures [@kohavi1997wrappers] **(Kohavi et al., 1997; Liu et al., 1998)**. The wrapper model, on the other hand, uses a specific learning algorithm to evaluate and determine which features to keep, often leading to better performance but at a higher computational cost **(Langley, 1994)**. Embedded methods perform feature selection during the model training process, integrating selection directly with learning. Hybrid methods combine the best aspects of filter and wrapper methods to achieve optimal performance with manageable computational complexity [@venkatesh2019review; @jovic2015review].
 
Feature selection can also be divided into supervised, semi-supervised, and unsupervised categories. Supervised learning involves labeled data, semi-supervised learning utilizes both labeled and unlabeled data, and unsupervised learning works with unlabeled data. @miao2016survey emphasized that the effectiveness of feature selection methods varies depending on the dataset and model used, necessitating tailored approaches to feature selection.
 
As feature selection methods evolved, addressing multicollinearity—a scenario where predictor variables are highly correlated—became a critical focus. Traditional methods often assumed independence among features, leading to inaccurate importance measures. @basu2022multicollinearity tackled this issue by introducing a framework to adjust for feature correlations when calculating Shapley values. Their method, termed Multicollinearity Corrected (MCC) Shapley values, provided more accurate measures of feature importance by accounting for existing correlations. Their experiments on datasets with moderate to high feature correlations demonstrated the superiority of MCC Shapley values over non-corrected ones, offering a more reliable interpretation of feature importance in the presence of multicollinearity.
 
A novel feature selection algorithm leveraging neural networks aims to enhance classification accuracy while reducing computational load by selecting only the most relevant features. This method utilizes feed-forward neural networks (FFNNs) and integrates seamlessly with other classifiers. The algorithm starts with a minimal feature set and iteratively includes additional features based on their contribution to classification accuracy, ensuring that only significant features are selected [@das2001filters].
 
Experiments on both artificial datasets, like the Monks problems, and real-world datasets, such as the University of Wisconsin Breast Cancer Dataset, the US Congressional Voting Records Dataset, and the Pima Indians Diabetes Dataset, demonstrated significant reductions in the number of features while improving classification accuracy. This approach emphasizes the importance of feature selection in enhancing classification accuracy and reducing computational load, making it a valuable tool for various classification tasks and data collection processes.
 
In more recent developments, multiple criteria decision-making (MCDM) methods have been applied to feature selection, particularly in the context of text classification with small datasets. @kou2020evaluation evaluated various feature selection methods using MCDM techniques across multiple text classification datasets. By employing methods like PROMETHEE, they ranked feature selection techniques based on a range of performance measures, including accuracy, stability, and efficiency. Their findings underscored the importance of comprehensive evaluation criteria in selecting the most appropriate feature selection method, tailored to specific dataset characteristics.
 
The trajectory of feature selection in machine learning illustrates a progression from simple, univariate methods to sophisticated, model-based approaches that address complex issues like multicollinearity and multi-criteria decision making. Early skepticism about "data dredging" has given way to an appreciation of the nuanced techniques necessary for robust predictive modeling. Feature selection methods have evolved to include similarity-based, information-theoretical-based, sparse-learning-based, statistical-based, filter, wrapper, embedded, hybrid, supervised, semi-supervised, and unsupervised approaches. Modern methods, such as neural networks, further enhance the accuracy and efficiency of feature selection, ensuring that only the most relevant features are utilized. As the field continues to advance, it is clear that feature selection will remain a cornerstone of machine learning, improving model performance and reducing computational complexity across a wide range of applications.

Our exploration of feature selection will begin by comparing the performance of various feature selection methods on a variety of datasets. We will look at four different aspects of performance: overall model performance on the holdout set, variance in performance between the training and holdout set, stability in the features selected, and parsimony of the final model. We will then examine feature selection bias of various methods by generating datasets with known parameters and determining if there is a systematic pattern to the features that are selected or omitted. Finally, we will examine the impact of multicollinearity on the feature selection process.



## Methods

### Datasets

**Birds' Songs Numeric Dataset**

Birds use calls and songs for a plethora of purposes: from attracting potential partners to asserting dominance over a territory.
This Dataset is comprised of a balanced training set and a test set of 88 species grouped in 61 genera of spectral features extracted from the birds' songs.

https://www.kaggle.com/datasets/fleanend/birds-songs-numeric-dataset

**Spambase**

Classifying Email as Spam or Non-Spam

https://archive.ics.uci.edu/dataset/94/spambase

**Coronavirus Tweets**

Perform Text Classification on the data. The tweets have been pulled from Twitter and manual tagging has been done then.

https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification


### Experiment 1: Selection Bias

### Experiment 2: Multicollinearity

## Analysis and Results

### Data and Visualization

### Statistical Modeling

```{r}

```

### Conclusion

## References



