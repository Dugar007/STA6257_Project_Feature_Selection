---
title: "Feature Selection: An Exploration of Algorithm Performance"
author: "Jason Case, Abhishek Dugar, Daniel Nkuah, Khoa Tran"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
  # eval: false
editor: 
  markdown: 
    wrap: 72
---

[slides](slides.html)

# Introduction

The discipline of feature selection in machine learning has evolved
significantly over the past few decades, transforming from a practice
once stigmatized as "data dredging" to an essential component of modern
predictive modeling. Early methods for variable selection were initially
met with skepticism by the statistical community, which viewed them as
potentially spurious means of uncovering patterns in data. These
reservations stemmed from concerns about overfitting and the robustness
of statistical inferences drawn from such techniques.

**Early Methods**

Feature selection's early forays began with relatively simple approaches
like forward, backward, and stepwise variable selection in linear
models. Forward selection starts with no variables in the model, adding
them one by one based on specific criteria until no significant
improvement is observed. Conversely, backward selection begins with all
candidate variables, removing the least significant ones iteratively.
Stepwise selection combines both approaches, iteratively adding and
removing variables to optimize model performance.

These methods were foundational but had limitations, particularly when
applied to datasets with high dimensionality. For instance,
@foster2004variable in their study on predicting bankruptcy highlighted
the complexity of variable selection when dealing with large sets of
predictors. They utilized a 20:80 train/validation split to ensure
robust model evaluation and critiqued traditional criteria like the
Akaike Information Criterion (AIC) for being too liberal, risking
overfitting. Instead, they advocated for methods like the Bonferroni
correction, which, though conservative, provided more reliable results
by dynamically adjusting p-values and avoiding inflated values from high
leverage points.

Univariate screening procedures marked another step in the evolution of
feature selection. These methods involve assessing each predictor
variable independently for its relationship with the target variable,
selecting those that meet a certain statistical threshold. While
straightforward and computationally inexpensive, univariate methods
often fail to capture the complex interdependencies between variables.

In the context of gene selection for cancer classification,
@guyon2002gene demonstrated the limitations of univariate methods. Prior
to their work, gene selection relied heavily on simple statistical
measures such as correlations, which often resulted in large, redundant
sets of genes. The advent of Recursive Feature Elimination (RFE)
represented a significant improvement. RFE uses Support Vector Machines
(SVMs) to iteratively eliminate the least important features, refining
the feature set to a more relevant subset and thus enhancing model
performance. There were multiple tests that demonstrated that SVMs
outperforms the baseline methods. First, the best leave-one-out
performance results in a 100% accuracy for SVMs and only 90% for the
baseline method (combination of univariate classifiers). The second
example was using the statistical test, (1-n), where n equals the number
of features, we can state with 99.3% confidence that SVMs are better
than the baseline method. Additionally, when comparing the SVM-selected
genes and baseline-selected genes, the SVM genes are better with a 84.1%
confidence based on test error rate and a 99.2% confidence based on the
test rejection rate. Figure 1 shows the performance comparison between
SVMs and the baseline method, success rate, acceptance rate, extremal
margin, and median margin and also indicates that there is a significant
difference between the gene selection. Table 1 summarizes the results of
the comparison between SVM and the baseline method, and it also
indicates that SVMs are significantly better based on the est error rate
and that it achieves a higher accuracy in classifying the samples. This
method highlighted the critical role of robust feature selection in
bioinformatics, where the choice of features often influenced
classification outcomes more than the specific classifier used.

**Figure 1. Reproduction of Figure 2 in @guyon2002gene.**

::: {#figure-1 .figure}
![Figure 1](Figure_1.jpeg){width="100%"} *This figure shows a
performance comparison between SVMs and the baseline method using
Leukemia data. Classifiers were trained on gene subsets selected by SVMs
and the baseline method using the training set of the Leukemia data. The
number of genes is color-coded and indicated in the legend. Quality
indicators are displayed radially: channels 1–4 show cross-validation
results using the leave-one-out method; channels 5–8 show test set
results. The indicators include success rate (suc), acceptance rate
(acc), extremal margin (ext), and median margin (med). Coefficients are
rescaled to have zero mean and variance 1 across all plots. For each
classifier, a larger colored area indicates better performance. The
figure reveals no significant difference in classifier performance on
this dataset, but a significant difference in gene selections.*
:::

**Table 1. Reproduction of Table 6 in @guyon2002gene.**

::: {#table-1 .figure}
![Table 1](Table_1.jpeg){width="100%"} *This table presents the
performance of the top classifiers on test data consisting of 34
samples. The table includes combinations of SVM or Baseline genes with
SVM or Baseline classifiers. For each combination, the table shows the
number of selected genes, the number of errors at zero rejection, and
the number of rejections at zero error. The number of genes represents
the subset selected by the method that provides the best classification
performance. Patient ID numbers corresponding to classification errors
are indicated in brackets. Additionally, results without any gene
selection are provided for comparison.*
:::

**Modern Methods**

Modern feature selection methods have become essential for data
preprocessing, extracting the most important information from big data.
By reducing the number of features, these methods mitigate the risk of
overfitting and enhance the performance of machine learning models and
pattern recognition systems [@venkatesh2019review].

Advanced feature selection methods include similarity-based,
information-theoretical-based, sparse-learning-based, and
statistical-based approaches. Similarity-based approaches select
features based on their similarity or dissimilarity, while
information-theoretical-based approaches use concepts like entropy and
mutual information to evaluate feature importance. Sparse-learning-based
approaches focus on identifying a small, essential set of features by
limiting the number of features selected, and statistical-based
approaches utilize statistical tests and models to determine the
significance of features for predicting the target variable
[@li2017feature].

Early and modern approaches are now categorized into filter, wrapper,
embedded, and hybrid approaches. The filter model selects features based
on the general properties of the training data, independent of any
learning algorithm, making it computationally efficient for large
datasets [@das2001filters; @kohavi1997wrappers]. Within the filter
model, feature selection algorithms can be further divided into feature
weighting algorithms, which assign weights to each feature based on
their relevance, and subset search algorithms, which explore candidate
feature subsets based on specific evaluation measures
[@kohavi1997wrappers; @liu2012feature]. The wrapper model, on the other
hand, uses a specific learning algorithm to evaluate and determine which
features to keep, often leading to better performance but at a higher
computational cost [@langley1994selection]. Embedded methods perform
feature selection during the model training process, integrating
selection directly with learning. Hybrid methods combine the best
aspects of filter and wrapper methods to achieve optimal performance
with manageable computational complexity [@venkatesh2019review;
@jovic2015review].

Feature selection can also be divided into supervised, semi-supervised,
and unsupervised categories. Supervised learning involves labeled data,
semi-supervised learning utilizes both labeled and unlabeled data, and
unsupervised learning works with unlabeled data. @miao2016survey
emphasized that the effectiveness of feature selection methods varies
depending on the dataset and model used, necessitating tailored
approaches to feature selection.

As feature selection methods evolved, addressing multicollinearity—a
scenario where predictor variables are highly correlated—became a
critical focus. Traditional methods often assumed independence among
features, leading to inaccurate importance measures.
@basu2022multicollinearity tackled this issue by introducing a framework
to adjust for feature correlations when calculating Shapley values.
Their method, termed Multicollinearity Corrected (MCC) Shapley values,
provided more accurate measures of feature importance by accounting for
existing correlations. Their experiments on datasets with moderate to
high feature correlations demonstrated the superiority of MCC Shapley
values over non-corrected ones, offering a more reliable interpretation
of feature importance in the presence of multicollinearity.

A novel feature selection algorithm leveraging neural networks aims to
enhance classification accuracy while reducing computational load by
selecting only the most relevant features. This method utilizes
feed-forward neural networks (FFNNs) and integrates seamlessly with
other classifiers. The algorithm starts with a minimal feature set and
iteratively includes additional features based on their contribution to
classification accuracy, ensuring that only significant features are
selected [@onnia2001feature].

Experiments on both artificial datasets, like the Monks problems, and
real-world datasets, such as the University of Wisconsin Breast Cancer
Dataset, the US Congressional Voting Records Dataset, and the Pima
Indians Diabetes Dataset, demonstrated significant reductions in the
number of features while improving classification accuracy. This
approach emphasizes the importance of feature selection in enhancing
classification accuracy and reducing computational load, making it a
valuable tool for various classification tasks and data collection
processes.

In more recent developments, multiple criteria decision-making (MCDM)
methods have been applied to feature selection, particularly in the
context of text classification with small datasets. @kou2020evaluation
evaluated various feature selection methods using MCDM techniques across
multiple text classification datasets. By employing methods like
PROMETHEE, they ranked feature selection techniques based on a range of
performance measures, including accuracy, stability, and efficiency.
Their findings underscored the importance of comprehensive evaluation
criteria in selecting the most appropriate feature selection method,
tailored to specific dataset characteristics.

The trajectory of feature selection in machine learning illustrates a
progression from simple, univariate methods to sophisticated,
model-based approaches that address complex issues like
multicollinearity and multi-criteria decision making. Early skepticism
about "data dredging" has given way to an appreciation of the nuanced
techniques necessary for robust predictive modeling. Feature selection
methods have evolved to include similarity-based,
information-theoretical-based, sparse-learning-based, statistical-based,
filter, wrapper, embedded, hybrid, supervised, semi-supervised, and
unsupervised approaches. Modern methods, such as neural networks,
further enhance the accuracy and efficiency of feature selection,
ensuring that only the most relevant features are utilized. As the field
continues to advance, it is clear that feature selection will remain a
cornerstone of machine learning, improving model performance and
reducing computational complexity across a wide range of applications.

# Methods

Our exploration of feature selection will begin by comparing the
performance of various feature selection methods on a variety of
real-world datasets. We will look at four different aspects of
performance: overall model performance on the holdout set, variance in
performance between the training and holdout set, and parsimony of the
final model. We explored an algorithm from each category of feature
selection methods.

**Correlation - Based Feature Selection (CFS)**

A common filtering method uses the correlation coefficient to measure
the relationship between each variable and the target, independently.

CFS's feature subset evaluation function is:

$\text{Ms} = \frac{k\bar{\text{rcf}}}{\sqrt k + k(k - 1)\bar{\text{rff}}}$

where Ms is the heuristic ”merit” of a feature subset S containing k
features, $\bar{\text{rcf}}$ is the mean feature-class correlation and
$\bar{\text{rff}}$ is the average feature - feature intercorrelation.
The numerator of this equation reflects the extent to which a set of
features is indicative of the class, while the denominator measures the
level of redundancy among the features [@hall1999correlation].

**Recursive Feature Elimination (RFE)**

RFE is a common wrapper method that removes variables iteratively.

For an unknown input vector x, a linear classifier takes the form

$y(x)=$sign$(w⋅x−b)$

The recursive feature elimination method for support vector machines can
be executed through these iterative steps[@guyon2002gene].

1.  Train the SVM classifier
2.  compute the ranking criterion for all features
3.  remove the features with smallest ranking values

For LS-SVM, substituting the KKT conditions into the Lagrangian gives
the objective function

$L = -\frac{1}{2} \sum \sum \alpha_i \alpha_j (K(x_i, x_j) + \frac{\sigma_{ij}}{C}) + \sum \alpha_i y_i$

Where ${\sigma_{ij}} = 1$ if $i=j$ and $0$ otherwise.

The ranking criterion can be obtained by:

$D(L^{-m}) = -\frac{1}{2} \sum \sum \alpha_i \alpha_j (K(x_i, x_j) - (K(x_i^{-m}, x_j^{-m})$

Where $x_i^{-m}$ , $x_j^{-m}$ are the vectors in which $m$-th feature
has been removed.

**Least Absolute Shrinkage and Selection Operator (LASSO)**

LASSO is an embedded method that uses L1 Regularization to shrink
coefficients to zero, effectively performing feature selection during
parameter estimation. LASSO minimizes the sum of squared errors while
imposing an upper limit on the sum of the absolute values of the model
parameters.

The LASSO estimate is defined by the solution to the l1 optimization
problem

minimize $\frac{\| Y - X\beta \|_2^2}{n}$ subject to
$X \sum_{j=1}^{k} \|\beta_j\|_1 < t$

where $t$ is the upper bound for the sum of the coefficients
[@buhlmann2011statistics]

**CFS & RFE**

For the hybrid method, we will combine the filter and wrapper methods;
first filtering using the correlation coefficient and then removing
variables iteratively using RFE.

# Analysis and Results

```{r}
# #| include: false
#environment setup (hidden from output)
library(dplyr)
library(quanteda)
library(caret)
library(wordcloud)
library(ggplot2)
library(glmnet)
library(jsonlite)
library(knitr)
library(kableExtra)

# Global parameter to force overwriting of existing cache files
FORCE_OVERWRITE <- FALSE

baseline_lambda = 0
baseline_maxit = 1000000
baseline_alpha = 0
validation_folds = 10

# Function to cache parameters
cache_parameter <- function(name, value = NULL, path = "cache/", prefix = "param_") {
  # Ensure the cache directory exists
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE)
  }
  
  # Construct the full file name
  file_name <- paste0(path, prefix, name, ".json")
  
  # Check for file existence
  if (file.exists(file_name) && !FORCE_OVERWRITE) {
    # Load and return the value from the file
    cached_value <- fromJSON(file_name)
    # Cast to the appropriate type
    if (cached_value$type == "numeric") {
      return(as.numeric(cached_value$value))
    } else if (cached_value$type == "integer") {
      return(as.integer(cached_value$value))
    } else if (cached_value$type == "list") {
      return(as.list(cached_value$value))
    } else if (cached_value$type == "vector_numeric") {
      return(as.numeric(cached_value$value))
    } else if (cached_value$type == "vector_integer") {
      return(as.integer(cached_value$value))
    } else if (cached_value$type == "named_list_numeric") {
      return(as.list(setNames(as.numeric(cached_value$value), cached_value$names)))
    } else {
      stop("Unsupported cached value type.")
    }
  } else {
    cached_value <- NULL
  }
  
  if (is.null(value)) {
    return(cached_value)
  } else {
    # Determine the type of the value and write it to the file
    if (is.numeric(value) && length(value) == 1) {
      value_type <- "numeric"
    } else if (is.integer(value) && length(value) == 1) {
      value_type <- "integer"
    } else if (is.list(value)) {
      if (!is.null(names(value)) && all(sapply(value, is.numeric))) {
        value_type <- "named_list_numeric"
      } else {
        value_type <- "list"
      }
    } else if (is.numeric(value) && length(value) > 1) {
      value_type <- "vector_numeric"
    } else if (is.integer(value) && length(value) > 1) {
      value_type <- "vector_integer"
    } else {
      stop("Unsupported value type. Only numeric, integer, vectors, and list are supported.")
    }
    
    # Prepare the data to be written as JSON
    if (value_type == "named_list_numeric") {
      json_data <- list(type = value_type, value = unname(value), names = names(value))
    } else {
      json_data <- list(type = value_type, value = value)
    }
    
    # Write the value to the file as JSON
    write_json(json_data, file_name)
    
    # Return the value
    return(value)
  }
}

geometric_sizes <- function(start_size, end_size, num_steps) {
  ratio <- (end_size / start_size)^(1 / (num_steps - 1))
  sizes <- start_size * (ratio ^ (0:(num_steps - 1)))
  return(round(sizes))
}

# Calculate performance score function
calculate_binary_performance <- function(actual, predicted) {
  confusion <- confusionMatrix(predicted, actual)
  performance <- confusion$overall['Accuracy']
  return(performance)
}


# Function to return the list of features with non-zero coefficients
nonzero_feature_indicies <- function(model, lambda) {
  coefs <- coef(model, s = lambda)
  nonzero_indices <- which(coefs != 0)
  # Exclude the intercept (first coefficient)
  nonzero_indices <- nonzero_indices[-1]
  return(nonzero_indices)
}


# Function to calculate performance at different thresholds
CFS_binary_logistic <- function(X, y, num_folds, num_bins, min_vars, geometric_spacing) {
  
  y_levels <- levels(y)  # Get the levels from y
  
  # Step 1: Calculate correlations
  correlations <- apply(X, 2, function(x) cor(x, as.numeric(y)))
  abs_correlations <- abs(correlations)
  
  # Step 2: Select features based on a correlation threshold
  select_features <- function(threshold) {
    selected_features <- which(abs_correlations >= threshold)
    return(selected_features)
  }
  
  # Step 3: Sweep through various thresholds and perform 10-fold cross-validation
  if (geometric_spacing){
    sizes <- geometric_sizes(ncol(X), min_vars, num_bins)
  }
  else {
    sizes <- round(seq(from = ncol(X), to = min_vars, length.out = num_bins))
  }
  
  percentiles <- 1 - sizes / ncol(X)
  thresholds <- quantile(abs_correlations, percentiles)
  results <- list()
  
  for (threshold in thresholds) {
    selected_features <- select_features(threshold)
    
    if (length(selected_features) == 0) {
      next
    }
    
    X_selected <- X[, selected_features, drop = FALSE]
    
    if (ncol(X_selected) < 2) {
      next
    }
    
    # Perform k-fold cross-validation
    # train_control <- trainControl(method = "cv", number = num_folds, verboseIter = TRUE)
    
    scores <- c()
    
    for (i in 1:num_folds) {
      folds <- createFolds(y, k = num_folds, list = TRUE, returnTrain = TRUE)
      fold_scores <- c()
      
      for (j in 1:num_folds) {
        train_index <- folds[[j]]
        test_index <- setdiff(seq_len(nrow(X_selected)), train_index)
        
        x_train_cv <- X_selected[train_index, ]
        y_train_cv <- y[train_index]
        x_test_cv <- X_selected[test_index, ]
        y_test_cv <- y[test_index]
        model_cv <- glmnet(x_train_cv, y_train_cv, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, alpha = baseline_alpha, family = "binomial", parallel = TRUE)
        
        pred_cv <- predict(model_cv, x_test_cv, s = min(model_cv$lambda), type = "response")
        pred_cv <- factor(ifelse(pred_cv > 0.5, y_levels[2], y_levels[1]), levels = y_levels)
        
        fold_scores <- c(fold_scores, calculate_binary_performance(y_test_cv, pred_cv))
      }
      
      scores <- c(scores, mean(fold_scores))
    }
    
    feature_indices <- toJSON(selected_features)
    results[[feature_indices]] <- mean(scores)
  }
  
  return(results)
}


RFE_binary_logistic <- function(X, y, num_folds, num_bins, min_vars, geometric_spacing) {
  results <- list()
  
  y_levels <- levels(y)  # Get the levels from y
  
  if (geometric_spacing){
    sizes <- geometric_sizes(ncol(X), min_vars, num_bins)
  }
  else {
    sizes <- round(seq(from = ncol(X), to = min_vars, length.out = num_bins))
  }
  
  current_features <- seq_len(ncol(X))  # Start with all features
  
  for (size in sizes) {
    cat("Evaluating size:", size, "\n")
    
    # Fit glmnet model to get coefficients
    model <- glmnet(X[, current_features, drop = FALSE], y, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)
    coefs <- as.matrix(coef(model, s = model$lambda.min))
    
    # Get indices of the top 'size' features by their absolute coefficient values
    if (length(current_features) > size) {
      selected_features <- order(abs(coefs[-1, 1]), decreasing = TRUE)[1:size]
      current_features <- current_features[selected_features]
    }
    
    x_selected <- X[, current_features, drop = FALSE]
    
    folds <- createFolds(y, k = num_folds, list = TRUE, returnTrain = TRUE)
    scores <- c()
    
    for (fold_idx in seq_along(folds)) {
      train_idx <- folds[[fold_idx]]
      test_idx <- setdiff(seq_len(nrow(x_selected)), train_idx)
      
      x_train_cv <- x_selected[train_idx, ]
      y_train_cv <- y[train_idx]
      x_test_cv <- x_selected[test_idx, ]
      y_test_cv <- y[test_idx]
      
      model_cv <- glmnet(x_train_cv, y_train_cv, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)
      
      if (length(model_cv$lambda) == 0) {
        next
      }
      
      pred_cv_prob <- predict(model_cv, x_test_cv, s = min(model_cv$lambda), type = "response")
      pred_cv <- factor(ifelse(pred_cv_prob > 0.5, y_levels[2], y_levels[1]), levels = y_levels)
      
      scores <- c(scores, calculate_binary_performance(y_test_cv, pred_cv))
    }
    
    mean_score <- mean(scores, na.rm = TRUE)
    feature_indices <- toJSON(current_features)
    results[[feature_indices]] <- mean_score
  }
  
  return(results)
}

subset_performance <- function(named_list) {
  # Extract lengths of the lists and corresponding performance scores
  lengths <- sapply(names(named_list), function(x) {
    list_obj <- fromJSON(x)
    length(list_obj)
  })
  
  performance <- as.numeric(named_list)
  
  # Create a data frame for plotting
  data <- data.frame(lengths = lengths, performance = performance)
  
  return(data)
}

download_and_extract_zip <- function(url, dest_dir) {
  # Ensure the destination directory exists
  if (!dir.exists(dest_dir)) {
    dir.create(dest_dir, recursive = TRUE)
  }
  
  # Create a temporary file to hold the downloaded zip file
  temp_zip <- tempfile(fileext = ".zip")
  
  # Download the zip file
  download.file(url, temp_zip, mode = "wb")
  
  # Extract the contents of the zip file
  unzip(temp_zip, exdir = dest_dir)
  
  # Remove the temporary zip file
  unlink(temp_zip)
}

download_kaggle_dataset <- function(dataset, path) {
  # Check if the kaggle command is available
  if (system("which kaggle", intern = TRUE) == "") {
    stop("Kaggle API is not installed or not in PATH. Please install it first.")
  }
  
  # Ensure the destination directory exists
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE)
  }
  
  # Construct the download command
  command <- sprintf("kaggle datasets download -d %s -p %s", dataset, path)
  
  # Execute the command
  system(command)
  
  # Unzip the downloaded file
  zipfile <- list.files(path, pattern = "*.zip", full.names = TRUE)
  if (length(zipfile) > 0) {
    unzip(zipfile, exdir = path)
    file.remove(zipfile)
  }
}

# download datasets, if necessary
# download_and_extract_zip("https://archive.ics.uci.edu/static/public/94/spambase.zip", "./data/spambase")
# download_kaggle_dataset("datatattle/covid-19-nlp-text-classification", "./data/covid")

```

## Data Discussion

We evaluated the performance of the methods on two classification tasks.
The datasets and associated tasks used were:

**Spambase Dataset**

The Spambase dataset from the UCI Machine Learning Repository contains
4601 instances of emails, with 57 features for classification tasks to
determine whether an email is spam or not (Figure 1). Features include word
and character frequency counts, as well as measures of consecutive
capital letters. It is commonly used for building and testing spam
detection models [@misc_spambase_94].

Data scientists often include interaction terms in a model to capture
the combined effect of two or more variables on the target variable.
Interaction terms help to identify if the effect of one predictor on the
outcome changes at different levels of another predictor, providing a
more nuanced and accurate representation of the relationships within the
data. This can lead to improved model performance and better insights
into complex dependencies between variables. However, including
interaction terms in a model significantly increases the number of
features, as it involves creating new variables that represent the
interactions between the original variables. To explore this effect, we
added all possible pairwise interaction terms for the 52 variables in
the Spambase data, resulting in $(\binom{57}{2} = 1,596)$
additional features. We centered and scaled the original features prior
to adding the terms. We use a binary logistic regression to predict if
an email is spam (1) or not spam (0). We held out 20% of the data at random for testing.

```{r}
#Importing Spam Data
last_57_lines <- tail(readLines("./data/spambase/spambase.names"), 57)
# Function to extract and sanitize column names
extract_column_names <- function(line) {
  # Extract the name before the colon
  name <- strsplit(line, ":")[[1]][1]
  # Sanitize the name by replacing special characters with underscores
  sanitized_name <- gsub("[^a-zA-Z0-9_]", "_", name)
  return(sanitized_name)
}

# Apply the function to each line to get the column names
raw_column_names <- c(sapply(last_57_lines, extract_column_names), 'flag_spam')


# Ensure unique column names
unique_column_names <- make.unique(raw_column_names)
spam_data <- read.csv("./data/spambase/spambase.data", header = FALSE, col.names = unique_column_names) %>%
  mutate(flag_spam = factor(flag_spam, levels = c(0, 1), labels = c("Not Spam", "Spam")))
```



```{r}
ggplot(spam_data, aes(x = flag_spam, fill = flag_spam)) +
  geom_bar(color = "black") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 10, color = "darkblue"),
    axis.text.y = element_text(size = 10, color = "darkblue"),
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "lightgrey"),
    panel.grid.minor = element_blank()
  ) +
  labs(x = "Spam Or Not", y = "Count", title = "Distribution of Spam vs Not Spam") +
  scale_y_continuous(labels = scales::comma) +
  scale_x_discrete(labels = c("Not Spam" = "Not Spam", "Spam" = "Spam")) +
  scale_fill_manual(values = c("Not Spam" = "red", "Spam" = "blue")) +
  guides(fill = guide_legend(title = NULL))
```
*Figure 1. Frequency of spam targets.*

**COVID-19 NLP Text Classification Dataset**

The COVID-19 NLP Text Classification dataset on Kaggle consists of
tweets related to COVID-19, labeled for sentiment analysis. It includes
features like tweet content, sentiment labels (positive, negative,
neutral), and additional metadata. This dataset is designed for natural
language processing tasks to analyze public sentiment during the
pandemic [@datatattle2024covid].

We recoded sentiment labels to simplify the classification task:
"Extremely Negative" and "Extremely Positive" are combined with
"Negative" and "Positive" respectively, and "Neutral" sentiments are
filtered out (Figure 2). We then processed the text into a "bag of words". In a "bag of words" NLP classification task, text data is converted into a
matrix of word frequencies or occurrences, where each row represents a
document and each column represents a unique word from the entire
corpus. The classification model then uses this matrix to predict the
category of each document. We processed the text by tokenizing the tweets,
removing punctuation, numbers, and symbols, converting to lowercase,
removing English stopwords, and stemming the words. This results in a
Document-Feature Matrix (DFM), which is further refined by trimming
infrequent terms in the training data and matching the test data
features to those in the training set (9% of data was held out for testing). The distribution of words per "bag" is illustrated in Figure 3.

One of the primary challenges in this
approach is data sparsity, as most documents contain only a small subset
of all possible words, resulting in a sparse matrix with many zero
entries. Additionally, the size of the feature space can become
extremely large, especially with extensive vocabularies, which can lead
to increased computational demands, memory consumption, and the risk of
overfitting, necessitating feature selection or dimensionality reduction
techniques.


```{r}
# Importing Train Data
corona_train <- read.csv("./data/covid/Corona_NLP_train.csv")[, c("OriginalTweet", "Sentiment")]

# Importing Test Data
corona_test <- read.csv("./data/covid/Corona_NLP_test.csv")[, c("OriginalTweet", "Sentiment")]

# Recode Sentiment to factors
corona_train <- corona_train %>%
  filter(Sentiment != "Neutral") %>%
  mutate(Sentiment = recode(Sentiment,
                            "Extremely Negative" = "Negative",
                            "Extremely Positive" = "Positive"),
         Sentiment = factor(Sentiment, levels = c("Negative", "Positive")))

corona_test <- corona_test %>%
  filter(Sentiment != "Neutral") %>%
  mutate(Sentiment = recode(Sentiment,
                            "Extremely Negative" = "Negative",
                            "Extremely Positive" = "Positive"),
         Sentiment = factor(Sentiment, levels = c("Negative", "Positive")))

# Preprocessing and tokenization using quanteda
preprocess_text <- function(text_column) {
  tokens <- tokens(text_column, 
                   what = "word", 
                   remove_punct = TRUE, 
                   remove_numbers = TRUE,
                   remove_symbols = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stopwords("english")) %>%
    tokens_wordstem()
  
  # Create a Document-Feature Matrix (DFM)
  dfm <- dfm(tokens)
  
  return(dfm)
}

corona_train_dfm <- dfm_trim(preprocess_text(corona_train$OriginalTweet), min_termfreq = 10)
corona_test_dfm <- dfm_match(preprocess_text(corona_test$OriginalTweet), features = featnames(corona_train_dfm))

# Convert DFM to sparse matrix
corona_train_sparse <- as(corona_train_dfm, "dgCMatrix")
corona_test_sparse <- as(corona_test_dfm, "dgCMatrix")
```


```{r}

ggplot(corona_train, aes(x = Sentiment, fill = Sentiment)) +
  geom_bar(color = "black") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 10, color = "darkblue"),
    axis.text.y = element_text(size = 10, color = "darkblue"),
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "lightgrey"),
    panel.grid.minor = element_blank()
  ) +
  labs(x = "Sentiment", y = "Count", title = "Distribution of Sentiment Labels") +
  scale_y_continuous(labels = scales::comma) +
  scale_x_discrete(labels = c("Positive" = "Positive", "Negative" = "Negative")) +
  scale_fill_manual(values = c("Negative" = "red", "Positive" = "blue")) +
  guides(fill = guide_legend(title = NULL))
```
*Figure 2. Frequency of sentiment targets.*

```{r}
# Calculate and visualize mean, standard deviation, and range of tokens per document
token_counts <- rowSums(corona_train_dfm)

ggplot(data = data.frame(token_counts), aes(x = token_counts)) +
  geom_histogram(binwidth = 5, color = "black", fill = "blue") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 10, color = "darkblue"),
    axis.text.y = element_text(size = 10, color = "darkblue"),
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "lightgrey"),
    panel.grid.minor = element_blank()
  ) +
  labs(title = "Histogram of Token Counts",
       x = "Token Counts",
       y = "Frequency") +
  scale_y_continuous(labels = scales::comma)
```
*Figure 3. Distribution of words per Tweet.*

## Statistical Modeling

We evaluated the feature selection methods using three metrics:

- **Accuracy** on the test set.
- Difference between accuracy on the training and test set (**overfitting**).
- Number of variables selected (model **complexity**).

To establish a baseline for performance, we first trained a binomial
logistic regression model using all available variables. We then use the
four selection methods, training a binomial logistic regression model
on the features selected with each method. In total, we produced 5 models for each dataset:

- **Baseline** Full logistics model with no feature selection. This provided a baseline for comparison.
- **CFS** Selected best of 20 correlation thresholds using 10-fold cross validation.
- **RFE** Select best of 20 sized subsets using 10-fold cross validation. 
- **LASSO** Select best penalty term using 10-fold cross validation. 
- **CFS + RFE** Remove 20% of variables with lowest correlation, Select best of 20 sized subsets using 10-fold cross validation.

Figure 5 illustrates parameter tuning using the twenty thresholds. In this example, a threshold that selected just under 2,000 features was used. Note the geometric progression in threshold selections; we selected thresholds in a way that removed a fixed proportion of features, in contrast to removing a fixed number of features. In figure 6, we see an example of the LASSO model selecting features by forcing parameter estimates to zero as the L1 Norm decreases.
```{r}
set.seed(42)

# Create a train-test split
trainIndex <- sample(1:nrow(spam_data), size = 0.8 * nrow(spam_data))

# Split the data
spam_train <- spam_data[trainIndex, ]
spam_test <- spam_data[-trainIndex, ]

X_train_raw <- spam_train %>% select(-flag_spam)
X_test_raw  <- spam_test %>% select(-flag_spam)

y_train  <- spam_train$flag_spam
y_test <- spam_test$flag_spam

levels = levels(y_train)

# scale the data, calculate all two-way interactions, and drop the intercept
preprocess_params <- preProcess(X_train_raw, method = c("center", "scale"))
X_train <- model.matrix( ~ .^2, predict(preprocess_params, X_train_raw))[, -1]
X_test <- model.matrix( ~ .^2, predict(preprocess_params, X_test_raw))[, -1]

spam_num_features <- ncol(X_train) 
spam_num_records<- nrow(X_train) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1] != 0)
total_elements <- prod(dim(X_train))
spam_sparsity <- ((total_elements - num_nonzero) / total_elements) * 100


baseline_model <- glmnet(
  X_train, 
  y_train, 
  family = "binomial", 
  alpha = baseline_alpha,
  lambda.min.ratio = baseline_lambda,
  maxit = baseline_maxit,
  parallel = TRUE
)

# Predict on training and test datasets
train_predictions_prob <- predict(baseline_model, X_train, s = min(baseline_model$lambda), type = "response")
test_predictions_prob <- predict(baseline_model, X_test, s = min(baseline_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train)
non_zero_features <- length(nonzero_feature_indicies(baseline_model, min(baseline_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

#save the results
spam_baseline_non_zero_features <- non_zero_features
spam_baseline_test_accuracy_score <- test_accuracy_score
spam_baseline_accuracy_difference <- accuracy_difference



#CORRELATION FEATURE SELECTION
cfs_feature_subsets <- cache_parameter('spam_cfs_feature_subsets')
if (is.null(cfs_feature_subsets)) {
  cfs_feature_subsets <- cache_parameter('spam_cfs_feature_subsets', CFS_binary_logistic(X_train, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(cfs_feature_subsets)[which.max(cfs_feature_subsets)])

# Step 4: Use the optimal threshold to train the final model
X_train_selected <- X_train[, optimal_subset, drop=FALSE]
X_test_selected <- X_test[, optimal_subset, drop=FALSE]

spam_num_features1 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_sparsity1 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, parallel = TRUE)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
spam_cfs_non_zero_features <- non_zero_features
spam_cfs_test_accuracy_score <- test_accuracy_score
spam_cfs_accuracy_difference <- accuracy_difference
spam_cfs_plot_data <- subset_performance(cfs_feature_subsets)


# RECURSIVE FEATURE ELIMINATION

feature_subsets <- cache_parameter('spam_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  feature_subsets <- cache_parameter('spam_rfe_feature_subsets', RFE_binary_logistic(X_train, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
X_train_selected = X_train[, optimal_subset, drop=FALSE]
X_test_selected = X_test[, optimal_subset, drop=FALSE]

spam_num_features2 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_sparsity2 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
spam_rfe_non_zero_features <- non_zero_features
spam_rfe_test_accuracy_score <- test_accuracy_score
spam_rfe_accuracy_difference <- accuracy_difference
spam_rfe_plot_data <- subset_performance(feature_subsets)


#LASSO REGRESSION
lambdas <- cache_parameter('spam_lasso_lambdas')
feature_subsets_sizes <- cache_parameter('spam_lasso_feature_subsets_sizes')
if (is.null(lambdas)) {
  cv_lasso <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 1, maxit = baseline_maxit, nfolds = validation_folds)
  lambda_values <- cv_lasso$lambda
  non_zero_features <- sapply(lambda_values, nonzero_feature_indicies, model = cv_lasso)
  non_zero_feature_sizes <- sapply(non_zero_features, length)
  performance <- cv_lasso$cvm
  lambdas <- cache_parameter('spam_lasso_lambdas', setNames(as.list(performance), as.character(lambda_values)))
  feature_subsets_sizes <- cache_parameter('spam_lasso_feature_subsets_sizes', setNames(as.list(performance), as.character(non_zero_feature_sizes)))
}
optimal_lambda <- as.double(names(lambdas)[which.min(lambdas)])


# Fit the final Lasso model using the best lambda
spam_lasso_final_model <- glmnet(X_train, y_train, family = "binomial", maxit = baseline_maxit, alpha = 1, lambda.min.ratio = optimal_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(spam_lasso_final_model, newx = X_train, s = optimal_lambda, type = "response")
test_predictions_prob <- predict(spam_lasso_final_model, newx = X_test, s = optimal_lambda, type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train) 
non_zero_features <- length(nonzero_feature_indicies(spam_lasso_final_model, min(spam_lasso_final_model$lambda)))

spam_num_features3 <- non_zero_features

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save the results
spam_lasso_non_zero_features <- non_zero_features
spam_lasso_test_accuracy_score <- test_accuracy_score
spam_lasso_accuracy_difference <- accuracy_difference
spam_lasso_plot_data <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), performance = as.numeric(feature_subsets_sizes))
spam_lasso_plot_data_lambda <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), lambdas = as.double(names(lambdas)))


#CFS + RFE REGRESSION

starting_features = fromJSON(names(cfs_feature_subsets)[3])
feature_subsets <- cache_parameter('spam_cfs_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  X_train_cfs_selected <- X_train[, starting_features, drop = FALSE]
  feature_subsets <- cache_parameter('spam_cfs_rfe_feature_subsets', RFE_binary_logistic(X_train_cfs_selected, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = FALSE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
X_train_selected = X_train[, optimal_subset, drop=FALSE]
X_test_selected = X_test[, optimal_subset, drop=FALSE]

spam_num_features4 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_sparsity4 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
spam_cfs_rfe_non_zero_features <- non_zero_features
spam_cfs_rfe_test_accuracy_score <- test_accuracy_score
spam_cfs_rfe_accuracy_difference <- accuracy_difference
spam_cfs_rfe_plot_data <- subset_performance(feature_subsets)

```


```{r}
set.seed(42)

# Convert DFM to sparse matrix
train_sparse <- corona_train_sparse
test_sparse <- corona_test_sparse

# Convert Sentiment to numeric for glmnet
y_train <- corona_train$Sentiment
y_test <- corona_test$Sentiment

levels = levels(y_train)

covid_num_features <- ncol(train_sparse) 
covid_num_records<- nrow(train_sparse) 
num_nonzero <- sum(train_sparse != 0)
total_elements <- prod(dim(train_sparse))
covid_sparsity <- ((total_elements - num_nonzero) / total_elements) * 100

# Fit the logistic regression model with glmnet

baseline_model <- glmnet(
  train_sparse, 
  y_train, 
  family = "binomial", 
  alpha = baseline_alpha,
  lambda.min.ratio = baseline_lambda,
  maxit = baseline_maxit,
  parallel = TRUE
)

# Predict on training and test datasets
train_predictions_prob <- predict(baseline_model, train_sparse, s = min(baseline_model$lambda), type = "response")
test_predictions_prob <- predict(baseline_model, test_sparse, s = min(baseline_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse)
non_zero_features <- length(nonzero_feature_indicies(baseline_model, min(baseline_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
covid_baseline_non_zero_features <- non_zero_features
covid_baseline_test_accuracy_score <- test_accuracy_score
covid_baseline_accuracy_difference <- accuracy_difference

#CORRELATION FEATURE SELECTION
cfs_feature_subsets <- cache_parameter('covid_cfs_feature_subsets')
if (is.null(cfs_feature_subsets)) {
  cfs_feature_subsets <- cache_parameter('covid_cfs_feature_subsets', CFS_binary_logistic(train_sparse, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(cfs_feature_subsets)[which.max(cfs_feature_subsets)])

# Step 4: Use the optimal threshold to train the final model
train_sparse_selected <- train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected <- test_sparse[, optimal_subset, drop=FALSE]

covid_num_features1 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_sparsity1 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, parallel = TRUE)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
covid_cfs_non_zero_features <- non_zero_features
covid_cfs_test_accuracy_score <- test_accuracy_score
covid_cfs_accuracy_difference <- accuracy_difference
covid_cfs_plot_data <- subset_performance(cfs_feature_subsets)


# RECURSIVE FEATURE ELIMINATION

feature_subsets <- cache_parameter('covid_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  feature_subsets <- cache_parameter('covid_rfe_feature_subsets', RFE_binary_logistic(train_sparse, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
train_sparse_selected = train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected = test_sparse[, optimal_subset, drop=FALSE]

covid_num_features2 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_sparsity2 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
covid_rfe_non_zero_features <- non_zero_features
covid_rfe_test_accuracy_score <- test_accuracy_score
covid_rfe_accuracy_difference <- accuracy_difference
covid_rfe_plot_data <- subset_performance(feature_subsets)


#LASSO REGRESSION
lambdas <- cache_parameter('covid_lasso_lambdas')
feature_subsets_sizes <- cache_parameter('covid_lasso_feature_subsets_sizes')
if (is.null(lambdas)) {
  cv_lasso <- cv.glmnet(train_sparse, y_train, family = "binomial", alpha = 1, maxit = baseline_maxit, nfolds = validation_folds)
  lambda_values <- cv_lasso$lambda
  non_zero_features <- sapply(lambda_values, nonzero_feature_indicies, model = cv_lasso)
  non_zero_feature_sizes <- sapply(non_zero_features, length)
  performance <- cv_lasso$cvm
  lambdas <- cache_parameter('covid_lasso_lambdas', setNames(as.list(performance), as.character(lambda_values)))
  feature_subsets_sizes <- cache_parameter('covid_lasso_feature_subsets_sizes', setNames(as.list(performance), as.character(non_zero_feature_sizes)))
}
optimal_lambda <- as.double(names(lambdas)[which.min(lambdas)])


# Fit the final Lasso model using the best lambda
final_model <- glmnet(train_sparse, y_train, family = "binomial", maxit = baseline_maxit, alpha = 1, lambda.min.ratio = optimal_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse, s = optimal_lambda, type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse, s = optimal_lambda, type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

covid_num_features3 <- non_zero_features

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save the results
covid_lasso_non_zero_features <- non_zero_features
covid_lasso_test_accuracy_score <- test_accuracy_score
covid_lasso_accuracy_difference <- accuracy_difference
covid_lasso_plot_data <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), performance = as.numeric(feature_subsets_sizes))
covid_lasso_plot_data_lambda <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), lambdas = as.double(names(lambdas)))

#CFS + RFE REGRESSION

starting_features = fromJSON(names(cfs_feature_subsets)[3])
feature_subsets <- cache_parameter('covid_cfs_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  train_sparse_cfs_selected <- train_sparse[, starting_features, drop = FALSE]
  feature_subsets <- cache_parameter('covid_cfs_rfe_feature_subsets', RFE_binary_logistic(train_sparse_cfs_selected, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = FALSE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
train_sparse_selected = train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected = test_sparse[, optimal_subset, drop=FALSE]

covid_num_features4 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_sparsity4 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save results
covid_cfs_rfe_non_zero_features <- non_zero_features
covid_cfs_rfe_test_accuracy_score <- test_accuracy_score
covid_cfs_rfe_accuracy_difference <- accuracy_difference
covid_cfs_rfe_plot_data <- subset_performance(feature_subsets)

```



```{r}
ggplot(covid_cfs_plot_data, aes(x = lengths, y = performance)) +
  geom_point() +
  geom_line() +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 10, color = "darkblue"),
    axis.text.y = element_text(size = 10, color = "darkblue"),
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "lightgrey"),
    panel.grid.minor = element_blank()
  ) +
  labs(title = "Accuracy vs Number of Features",
       x = "Number of Features",
       y = "Cross-validation Accuracy") +
  scale_y_continuous(labels = scales::comma)
```
*Figure 4. Example of cross-validation accuracy with CFS model for the Sentiment task.*


```{r}
plot(spam_lasso_final_model)
```
*Figure 5. Example of coefficient behavior with LASSO model for the spam task.*

## Results

**Spambase Dataset**

Table 1 displays the results of feature selection for the spam classification task. Each row represents a feature selection method, and the columns represent the various metrics. For model complexity, as measured by number of features, we consider a lower number to be better. For test accuracy, higher is better. And finally, for overfitting, as measured by the difference between the training and test scores, we consider lower better.

As you can see, the baseline performance, without feature selection, has decent performance but is somewhat overfitted. The CFS method produces the best performance and a fairly parsimonious model with only 126 variables. The LASSO model is  the most parsimonious, but has the lowest performance suggesting it may have eliminated too many features.

```{r}


# Create visuals for Spambase dataset
var_method <- c('Baseline', 'CFS', 'RFE', 'LASSO', 'CFS + RFE')
num_features <- c(spam_baseline_non_zero_features, spam_cfs_non_zero_features, spam_rfe_non_zero_features, spam_lasso_non_zero_features, spam_cfs_rfe_non_zero_features)
accuracy_score <- c(spam_baseline_test_accuracy_score, spam_cfs_test_accuracy_score, spam_rfe_test_accuracy_score, spam_lasso_test_accuracy_score, spam_cfs_rfe_test_accuracy_score)
accuracy_diff <- c(spam_baseline_accuracy_difference, spam_cfs_accuracy_difference, spam_rfe_accuracy_difference, spam_lasso_accuracy_difference, spam_cfs_rfe_accuracy_difference)

data <- data.frame(var_method, num_features, accuracy_score, accuracy_diff)

# Format numbers
data <- data %>%
  mutate(
    accuracy_score = round(accuracy_score, 3),
    accuracy_diff = round(accuracy_diff, 3)
  )

# Create and render the table
kable(data, format = "html", col.names = c("Method", "Number of Features", "Test Accuracy Score", "Accuracy Decrease from Train")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```
*Table 1. Results of spam classification task.*

**COVID-19 NLP Text Classification Dataset**

Table 2 displays the results of the feature selection on the sentiment task. Baseline Results are similar to the spam classification results; with performance being decent but overfitted. We see the CFS produces the simplistic model and it performs better than the baseline, but it is not the best performing model. The LASSO has the best performance, but it has removed very few features. Finally, we observed that the RFE performs much better than in the spam classification task, but the hybrid method performs somewhat worse.

```{r}
# Create visuals for Covid dataset
var_method <- c('Baseline', 'CFS', 'RFE', 'LASSO', 'CFS + RFE')
num_features <- c(covid_baseline_non_zero_features, covid_cfs_non_zero_features, covid_rfe_non_zero_features, covid_lasso_non_zero_features, covid_cfs_rfe_non_zero_features)
accuracy_score <- c(covid_baseline_test_accuracy_score, covid_cfs_test_accuracy_score, covid_rfe_test_accuracy_score, covid_lasso_test_accuracy_score, covid_cfs_rfe_test_accuracy_score)
accuracy_diff <- c(covid_baseline_accuracy_difference, covid_cfs_accuracy_difference, covid_rfe_accuracy_difference, covid_lasso_accuracy_difference, covid_cfs_rfe_accuracy_difference)

data <- data.frame(var_method, num_features, accuracy_score, accuracy_diff)

# Format numbers
data <- data %>%
  mutate(
    accuracy_score = round(accuracy_score, 3),
    accuracy_diff = round(accuracy_diff, 3)
  )

# Create and render the table
kable(data, format = "html", col.names = c("Method", "Number of Features", "Test Accuracy Score", "Accuracy Decrease from Train")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```
*Table 2. Results of sentiment classification task.*


**Observations per Feature**

Fundamentally, what we are doing when we selecting features is increasing the number of observations we have to estimate each parameter. We wanted to explore this relationship a bit more, so we took our two tasks and altered them to decrease the initial ratio of observations per feature to stress test feature selection. For the Spam task, we decreased the ratio by selecting 10% of the training records at random. We Increased features for sentiment task by decreasing threshold for "rare" words from ten to two, thereby decreasing the ratio of observations per features. We then re-ran the analysis, and looked specifically at how the ratio of observations per feature impacted overfitting.

The relationship between the two variables is quite clear (Figure 6). We have 20 data point, 5 for each dataset for each analysis. The datapoints tending to the left are the result of our new analysis, while the points tending to the right are from the original analysis. It is clear that overfitting decreases dramatically as the number of observations per feature increase.

```{r}
set.seed(42)

spam_alt_data <- sample_frac(spam_data, .1)


# Create a train-test split
trainIndex <- sample(1:nrow(spam_alt_data), size = 0.8 * nrow(spam_alt_data))

# Split the data
spam_alt_train <- spam_alt_data[trainIndex, ]
spam_alt_test <- spam_alt_data[-trainIndex, ]

X_train_raw <- spam_alt_train %>% select(-flag_spam)
X_test_raw  <- spam_alt_test %>% select(-flag_spam)

y_train  <- spam_alt_train$flag_spam
y_test <- spam_alt_test$flag_spam

levels = levels(y_train)

# scale the data, calculate all two-way interactions, and drop the intercept
preprocess_params <- preProcess(X_train_raw, method = c("center", "scale"))
X_train <- model.matrix( ~ .^2, predict(preprocess_params, X_train_raw))[, -1]
X_test <- model.matrix( ~ .^2, predict(preprocess_params, X_test_raw))[, -1]

spam_alt_num_features <- ncol(X_train) 
spam_alt_num_records<- nrow(X_train) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1] != 0)
total_elements <- prod(dim(X_train))
spam_alt_sparsity <- ((total_elements - num_nonzero) / total_elements) * 100


baseline_model <- glmnet(
  X_train, 
  y_train, 
  family = "binomial", 
  alpha = baseline_alpha,
  lambda.min.ratio = baseline_lambda,
  maxit = baseline_maxit,
  parallel = TRUE
)

# Predict on training and test datasets
train_predictions_prob <- predict(baseline_model, X_train, s = min(baseline_model$lambda), type = "response")
test_predictions_prob <- predict(baseline_model, X_test, s = min(baseline_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_alt_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_alt_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train)
non_zero_features <- length(nonzero_feature_indicies(baseline_model, min(baseline_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

#save the results
spam_alt_baseline_non_zero_features <- non_zero_features
spam_alt_baseline_test_accuracy_score <- test_accuracy_score
spam_alt_baseline_accuracy_difference <- accuracy_difference



#CORRELATION FEATURE SELECTION
cfs_feature_subsets <- cache_parameter('spam_alt_cfs_feature_subsets')
if (is.null(cfs_feature_subsets)) {
  cfs_feature_subsets <- cache_parameter('spam_alt_cfs_feature_subsets', CFS_binary_logistic(X_train, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(cfs_feature_subsets)[which.max(cfs_feature_subsets)])

# Step 4: Use the optimal threshold to train the final model
X_train_selected <- X_train[, optimal_subset, drop=FALSE]
X_test_selected <- X_test[, optimal_subset, drop=FALSE]

spam_alt_num_features1 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_alt_sparsity1 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, parallel = TRUE)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_alt_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_alt_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
spam_alt_cfs_non_zero_features <- non_zero_features
spam_alt_cfs_test_accuracy_score <- test_accuracy_score
spam_alt_cfs_accuracy_difference <- accuracy_difference
spam_alt_cfs_plot_data <- subset_performance(cfs_feature_subsets)


# RECURSIVE FEATURE ELIMINATION

feature_subsets <- cache_parameter('spam_alt_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  feature_subsets <- cache_parameter('spam_alt_rfe_feature_subsets', RFE_binary_logistic(X_train, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
X_train_selected = X_train[, optimal_subset, drop=FALSE]
X_test_selected = X_test[, optimal_subset, drop=FALSE]

spam_alt_num_features2 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_alt_sparsity2 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_alt_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_alt_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
spam_alt_rfe_non_zero_features <- non_zero_features
spam_alt_rfe_test_accuracy_score <- test_accuracy_score
spam_alt_rfe_accuracy_difference <- accuracy_difference
spam_alt_rfe_plot_data <- subset_performance(feature_subsets)


#LASSO REGRESSION
lambdas <- cache_parameter('spam_alt_lasso_lambdas')
feature_subsets_sizes <- cache_parameter('spam_alt_lasso_feature_subsets_sizes')
if (is.null(lambdas)) {
  cv_lasso <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 1, maxit = baseline_maxit, nfolds = validation_folds)
  lambda_values <- cv_lasso$lambda
  non_zero_features <- sapply(lambda_values, nonzero_feature_indicies, model = cv_lasso)
  non_zero_feature_sizes <- sapply(non_zero_features, length)
  performance <- cv_lasso$cvm
  lambdas <- cache_parameter('spam_alt_lasso_lambdas', setNames(as.list(performance), as.character(lambda_values)))
  feature_subsets_sizes <- cache_parameter('spam_alt_lasso_feature_subsets_sizes', setNames(as.list(performance), as.character(non_zero_feature_sizes)))
}
optimal_lambda <- as.double(names(lambdas)[which.min(lambdas)])


# Fit the final Lasso model using the best lambda
final_model <- glmnet(X_train, y_train, family = "binomial", maxit = baseline_maxit, alpha = 1, lambda.min.ratio = optimal_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train, s = optimal_lambda, type = "response")
test_predictions_prob <- predict(final_model, newx = X_test, s = optimal_lambda, type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_alt_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_alt_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

spam_alt_num_features3 <- non_zero_features

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save the results
spam_alt_lasso_non_zero_features <- non_zero_features
spam_alt_lasso_test_accuracy_score <- test_accuracy_score
spam_alt_lasso_accuracy_difference <- accuracy_difference
spam_alt_lasso_plot_data <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), performance = as.numeric(feature_subsets_sizes))
spam_alt_lasso_plot_data_lambda <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), lambdas = as.double(names(lambdas)))


#CFS + RFE REGRESSION

starting_features = fromJSON(names(cfs_feature_subsets)[3])
feature_subsets <- cache_parameter('spam_alt_cfs_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  X_train_cfs_selected <- X_train[, starting_features, drop = FALSE]
  feature_subsets <- cache_parameter('spam_alt_cfs_rfe_feature_subsets', RFE_binary_logistic(X_train_cfs_selected, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = FALSE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
X_train_selected = X_train[, optimal_subset, drop=FALSE]
X_test_selected = X_test[, optimal_subset, drop=FALSE]

spam_alt_num_features4 <- ncol(X_train_selected) 
num_nonzero <- sum(model.matrix( ~ .^2, X_train_raw)[, -1][, optimal_subset, drop=FALSE] != 0)
total_elements <- prod(dim(X_train_selected))
spam_alt_sparsity4 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(spam_alt_train$flag_spam, train_predictions)
test_accuracy_score <- calculate_binary_performance(spam_alt_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
spam_alt_cfs_rfe_non_zero_features <- non_zero_features
spam_alt_cfs_rfe_test_accuracy_score <- test_accuracy_score
spam_alt_cfs_rfe_accuracy_difference <- accuracy_difference
spam_alt_cfs_rfe_plot_data <- subset_performance(feature_subsets)

```

```{r}
set.seed(42)

train_dfm <- dfm_trim(preprocess_text(corona_train$OriginalTweet), min_termfreq = 2)
test_dfm <- dfm_match(preprocess_text(corona_test$OriginalTweet), features = featnames(train_dfm))

# Convert DFM to sparse matrix
train_sparse <- as(train_dfm, "dgCMatrix")
test_sparse <- as(test_dfm, "dgCMatrix")

# Convert Sentiment to numeric for glmnet
y_train <- corona_train$Sentiment
y_test <- corona_test$Sentiment

levels = levels(y_train)


covid_alt_num_features <- ncol(train_sparse) 
covid_alt_num_records <- nrow(train_sparse) 
num_nonzero <- sum(train_sparse != 0)
total_elements <- prod(dim(train_sparse))
covid_alt_sparsity <- ((total_elements - num_nonzero) / total_elements) * 100

# Fit the logistic regression model with glmnet

baseline_model <- glmnet(
  train_sparse, 
  y_train, 
  family = "binomial", 
  alpha = baseline_alpha,
  lambda.min.ratio = baseline_lambda,
  maxit = baseline_maxit,
  parallel = TRUE
)

# Predict on training and test datasets
train_predictions_prob <- predict(baseline_model, train_sparse, s = min(baseline_model$lambda), type = "response")
test_predictions_prob <- predict(baseline_model, test_sparse, s = min(baseline_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse)
non_zero_features <- length(nonzero_feature_indicies(baseline_model, min(baseline_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
covid_alt_baseline_non_zero_features <- non_zero_features
covid_alt_baseline_test_accuracy_score <- test_accuracy_score
covid_alt_baseline_accuracy_difference <- accuracy_difference

#CORRELATION FEATURE SELECTION
cfs_feature_subsets <- cache_parameter('covid_alt_cfs_feature_subsets')
if (is.null(cfs_feature_subsets)) {
  cfs_feature_subsets <- cache_parameter('covid_alt_cfs_feature_subsets', CFS_binary_logistic(train_sparse, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(cfs_feature_subsets)[which.max(cfs_feature_subsets)])

# Step 4: Use the optimal threshold to train the final model
train_sparse_selected <- train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected <- test_sparse[, optimal_subset, drop=FALSE]

covid_alt_num_features1 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_alt_sparsity1 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, parallel = TRUE)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save the results
covid_alt_cfs_non_zero_features <- non_zero_features
covid_alt_cfs_test_accuracy_score <- test_accuracy_score
covid_alt_cfs_accuracy_difference <- accuracy_difference
covid_alt_cfs_plot_data <- subset_performance(cfs_feature_subsets)


# RECURSIVE FEATURE ELIMINATION

feature_subsets <- cache_parameter('covid_alt_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  feature_subsets <- cache_parameter('covid_alt_rfe_feature_subsets', RFE_binary_logistic(train_sparse, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = TRUE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
train_sparse_selected = train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected = test_sparse[, optimal_subset, drop=FALSE]

covid_alt_num_features2 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_alt_sparsity2 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# save results
covid_alt_rfe_non_zero_features <- non_zero_features
covid_alt_rfe_test_accuracy_score <- test_accuracy_score
covid_alt_rfe_accuracy_difference <- accuracy_difference
covid_alt_rfe_plot_data <- subset_performance(feature_subsets)


#LASSO REGRESSION
lambdas <- cache_parameter('covid_alt_lasso_lambdas')
feature_subsets_sizes <- cache_parameter('covid_alt_lasso_feature_subsets_sizes')
if (is.null(lambdas)) {
  cv_lasso <- cv.glmnet(train_sparse, y_train, family = "binomial", alpha = 1, maxit = baseline_maxit, nfolds = validation_folds)
  lambda_values <- cv_lasso$lambda
  non_zero_features <- sapply(lambda_values, nonzero_feature_indicies, model = cv_lasso)
  non_zero_feature_sizes <- sapply(non_zero_features, length)
  performance <- cv_lasso$cvm
  lambdas <- cache_parameter('covid_alt_lasso_lambdas', setNames(as.list(performance), as.character(lambda_values)))
  feature_subsets_sizes <- cache_parameter('covid_alt_lasso_feature_subsets_sizes', setNames(as.list(performance), as.character(non_zero_feature_sizes)))
}
optimal_lambda <- as.double(names(lambdas)[which.min(lambdas)])


# Fit the final Lasso model using the best lambda
final_model <- glmnet(train_sparse, y_train, family = "binomial", maxit = baseline_maxit, alpha = 1, lambda.min.ratio = optimal_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse, s = optimal_lambda, type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse, s = optimal_lambda, type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

covid_alt_num_features3 <-non_zero_features

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save the results
covid_alt_lasso_non_zero_features <- non_zero_features
covid_alt_lasso_test_accuracy_score <- test_accuracy_score
covid_alt_lasso_accuracy_difference <- accuracy_difference
covid_alt_lasso_plot_data <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), performance = as.numeric(feature_subsets_sizes))
covid_alt_lasso_plot_data_lambda <- data.frame(lengths = as.integer(names(feature_subsets_sizes)), lambdas = as.double(names(lambdas)))

#CFS + RFE REGRESSION

starting_features = fromJSON(names(cfs_feature_subsets)[3])
feature_subsets <- cache_parameter('covid_alt_cfs_rfe_feature_subsets')
if (is.null(feature_subsets)) {
  train_sparse_cfs_selected <- train_sparse[, starting_features, drop = FALSE]
  feature_subsets <- cache_parameter('covid_alt_cfs_rfe_feature_subsets', RFE_binary_logistic(train_sparse_cfs_selected, y_train, num_folds=validation_folds, num_bins = 20, min_vars = 50, geometric_spacing = FALSE))
}
optimal_subset <- fromJSON(names(feature_subsets)[which.max(feature_subsets)])

# Fit the final model using selected features
train_sparse_selected = train_sparse[, optimal_subset, drop=FALSE]
test_sparse_selected = test_sparse[, optimal_subset, drop=FALSE]

covid_alt_num_features4 <- ncol(train_sparse_selected) 
num_nonzero <- sum(train_sparse_selected != 0)
total_elements <- prod(dim(train_sparse_selected))
covid_alt_sparsity4 <- ((total_elements - num_nonzero) / total_elements) * 100

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, levels[2], levels[1]), levels = levels)

# Calculate accuracy scores for training and test datasets
train_accuracy_score <- calculate_binary_performance(corona_train$Sentiment, train_predictions)
test_accuracy_score <- calculate_binary_performance(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 
non_zero_features <- length(nonzero_feature_indicies(final_model, min(final_model$lambda)))

# Calculate the difference in accuracy score between train and test data
accuracy_difference <- train_accuracy_score - test_accuracy_score

# Save results
covid_alt_cfs_rfe_non_zero_features <- non_zero_features
covid_alt_cfs_rfe_test_accuracy_score <- test_accuracy_score
covid_alt_cfs_rfe_accuracy_difference <- accuracy_difference
covid_alt_cfs_rfe_plot_data <- subset_performance(feature_subsets)

```


```{r}
# Create visuals for trending
datasets <- c('Spam Baseline', 'Covid Baseline', 'Spam Alt Baseline', 'Covid Alt Baseline',
              'Spam CFS', 'Covid CFS', 'Spam Alt CFS', 'Covid Alt CFS',
              'Spam RFE', 'Covid RFE', 'Spam Alt RFE', 'Covid Alt RFE',
              'Spam LASSO', 'Covid LASSO', 'Spam Alt LASSO', 'Covid Alt LASSO',
              'Spam RFECFS', 'Covid RFECFS', 'Spam Alt RFECFS', 'Covid Alt RFECFS')
var_ratio <- c(spam_num_records / spam_num_features, covid_num_records / covid_num_features, spam_alt_num_records / spam_alt_num_features, covid_alt_num_records / covid_alt_num_features,
               spam_num_records / spam_num_features1, covid_num_records / covid_num_features1, spam_alt_num_records / spam_alt_num_features1, covid_alt_num_records / covid_alt_num_features1,
               spam_num_records / spam_num_features2, covid_num_records / covid_num_features2, spam_alt_num_records / spam_alt_num_features2, covid_alt_num_records / covid_alt_num_features2,
               spam_num_records / spam_num_features3, covid_num_records / covid_num_features3, spam_alt_num_records / spam_alt_num_features3, covid_alt_num_records / covid_alt_num_features3,
               spam_num_records / spam_num_features4, covid_num_records / covid_num_features4, spam_alt_num_records / spam_alt_num_features4, covid_alt_num_records / covid_alt_num_features4)
accuracy_diff <- c(spam_baseline_accuracy_difference, covid_baseline_accuracy_difference, spam_alt_baseline_accuracy_difference, covid_alt_baseline_accuracy_difference,
                   spam_cfs_accuracy_difference, covid_cfs_accuracy_difference, spam_alt_cfs_accuracy_difference, covid_alt_cfs_accuracy_difference,
                   spam_rfe_accuracy_difference, covid_rfe_accuracy_difference, spam_alt_rfe_accuracy_difference, covid_alt_rfe_accuracy_difference,
                   spam_lasso_accuracy_difference, covid_lasso_accuracy_difference, spam_alt_lasso_accuracy_difference, covid_alt_lasso_accuracy_difference,
                   spam_cfs_rfe_accuracy_difference, covid_cfs_rfe_accuracy_difference, spam_alt_cfs_rfe_accuracy_difference, covid_alt_cfs_rfe_accuracy_difference)
sparsity <- c(spam_sparsity, covid_sparsity, spam_alt_sparsity, covid_alt_sparsity,
              spam_sparsity1, covid_sparsity1, spam_alt_sparsity1, covid_alt_sparsity1,
              spam_sparsity2, covid_sparsity2, spam_alt_sparsity2, covid_alt_sparsity2,
              NA, NA, NA, NA,
              spam_sparsity4, covid_sparsity4, spam_alt_sparsity4, covid_alt_sparsity4)

data <- data.frame(datasets, var_ratio, accuracy_diff, sparsity)

# Format numbers
data <- data %>%
  mutate(
    var_ratio = round(var_ratio, 1),
    accuracy_diff = round(accuracy_diff, 3),
    sparsity = round(sparsity, 1)
  )

ggplot(data, aes(x = var_ratio, y = accuracy_diff)) +
  geom_point(aes(color = datasets)) +
  # geom_text(aes(label = paste(datasets, "\nSparsity:", paste0(sparsity, "%"))), vjust = -0.3, hjust = 0) +
  geom_smooth(method = "lm", formula = y ~ log(x), se = FALSE, color = "blue") +
  labs(x = "Observations per Feature", y = "Baseline Accuracy Decrease from Train", title = "Accuracy Decrease vs Observations per Feature") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 10, color = "darkblue"),
    axis.text.y = element_text(size = 10, color = "darkblue"),
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "lightgrey"),
    panel.grid.minor = element_blank()
  ) +
  guides(color = "none")

```
*Figure 6. Plot of overfitting as a function of observations per feature.*



# Conclusion

Feature selection is a crucial step in model building, and it often involves balancing several factors. Selecting fewer features tends to reduce overfitting by simplifying the model, which can sometimes lead to improved performance. However, there is no universally "best" feature elimination method, as the effectiveness of different techniques can vary depending on the specific dataset and problem. Ultimately, feature selection requires careful consideration of trade-offs between model complexity, test accuracy, overfitting, and the computational resources required for training and prediction. This balancing act ensures that the model remains both efficient and effective.

Despite the rise of large "big data" datasets with a high ratio of observations to features, feature selection remains highly relevant. Even with thousands of features, reducing the feature space can significantly streamline the modeling process. A smaller feature space requires less exploration, allowing models to more efficiently find optimal solutions. Additionally, pragmatic constraints such as documentation and managing dependencies make feature selection beneficial. By focusing on the most relevant features, the complexity of both model interpretation and maintenance can be reduced, leading to more efficient workflows and clearer insights.

# References
