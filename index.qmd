---
title: "Feature Selection - Data Science Capstone"
author: "Jason Case, Abhishek Dugar. Daniel Nkuah, Khoa Tran"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

The discipline of feature selection in machine learning has evolved significantly over the past few decades, transforming from a practice once stigmatized as "data dredging" to an essential component of modern predictive modeling. Early methods for variable selection were initially met with skepticism by the statistical community, which viewed them as potentially spurious means of uncovering patterns in data. These reservations stemmed from concerns about overfitting and the robustness of statistical inferences drawn from such techniques.
 
### Early Methods
Feature selection's early forays began with relatively simple approaches like forward, backward, and stepwise variable selection in linear models. Forward selection starts with no variables in the model, adding them one by one based on specific criteria until no significant improvement is observed. Conversely, backward selection begins with all candidate variables, removing the least significant ones iteratively. Stepwise selection combines both approaches, iteratively adding and removing variables to optimize model performance.
 
These methods were foundational but had limitations, particularly when applied to datasets with high dimensionality. For instance, Foster and Stine (2004) in their study on predicting bankruptcy highlighted the complexity of variable selection when dealing with large sets of predictors. They utilized a 20:80 train/validation split to ensure robust model evaluation and critiqued traditional criteria like the Akaike Information Criterion (AIC) for being too liberal, risking overfitting. Instead, they advocated for methods like the Bonferroni correction, which, though conservative, provided more reliable results by dynamically adjusting p-values and avoiding inflated values from high leverage points.
 
Univariate screening procedures marked another step in the evolution of feature selection. These methods involve assessing each predictor variable independently for its relationship with the target variable, selecting those that meet a certain statistical threshold. While straightforward and computationally inexpensive, univariate methods often fail to capture the complex interdependencies between variables. 
 
In the context of gene selection for cancer classification, Guyon et al. (2022) demonstrated the limitations of univariate methods. Prior to their work, gene selection relied heavily on simple statistical measures such as correlations, which often resulted in large, redundant sets of genes. The advent of Recursive Feature Elimination (RFE) represented a significant improvement. RFE uses Support Vector Machines (SVMs) to iteratively eliminate the least important features, refining the feature set to a more relevant subset and thus enhancing model performance. This method highlighted the critical role of robust feature selection in bioinformatics, where the choice of features often influenced classification outcomes more than the specific classifier used.
 
### Modern Methods
Modern feature selection methods have become essential for data preprocessing, extracting the most important information from big data. By reducing the number of features, these methods mitigate the risk of overfitting and enhance the performance of machine learning models and pattern recognition systems (Venkatesh et al., 2019).
 
Advanced feature selection methods include similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based approaches. Similarity-based approaches select features based on their similarity or dissimilarity, while information-theoretical-based approaches use concepts like entropy and mutual information to evaluate feature importance. Sparse-learning-based approaches focus on identifying a small, essential set of features by limiting the number of features selected, and statistical-based approaches utilize statistical tests and models to determine the significance of features for predicting the target variable (Li et al., 2017).
 
Early and modern approaches are now categorized into filter, wrapper, embedded, and hybrid approaches. The filter model selects features based on the general properties of the training data, independent of any learning algorithm, making it computationally efficient for large datasets (Das, 2001; Kohavi et al., 1997). Within the filter model, feature selection algorithms can be further divided into feature weighting algorithms, which assign weights to each feature based on their relevance, and subset search algorithms, which explore candidate feature subsets based on specific evaluation measures (Kohavi et al., 1997; Liu et al., 1998). The wrapper model, on the other hand, uses a specific learning algorithm to evaluate and determine which features to keep, often leading to better performance but at a higher computational cost (Langley, 1994). Embedded methods perform feature selection during the model training process, integrating selection directly with learning. Hybrid methods combine the best aspects of filter and wrapper methods to achieve optimal performance with manageable computational complexity (Venkatesh et al., 2019; Jović et al., 2015).
 
Feature selection can also be divided into supervised, semi-supervised, and unsupervised categories. Supervised learning involves labeled data, semi-supervised learning utilizes both labeled and unlabeled data, and unsupervised learning works with unlabeled data. Miao et al. (2016) emphasized that the effectiveness of feature selection methods varies depending on the dataset and model used, necessitating tailored approaches to feature selection.
 
As feature selection methods evolved, addressing multicollinearity—a scenario where predictor variables are highly correlated—became a critical focus. Traditional methods often assumed independence among features, leading to inaccurate importance measures. Basu and Maji (2022) tackled this issue by introducing a framework to adjust for feature correlations when calculating Shapley values. Their method, termed Multicollinearity Corrected (MCC) Shapley values, provided more accurate measures of feature importance by accounting for existing correlations. Their experiments on datasets with moderate to high feature correlations demonstrated the superiority of MCC Shapley values over non-corrected ones, offering a more reliable interpretation of feature importance in the presence of multicollinearity.
 
A novel feature selection algorithm leveraging neural networks aims to enhance classification accuracy while reducing computational load by selecting only the most relevant features. This method utilizes feed-forward neural networks (FFNNs) and integrates seamlessly with other classifiers. The algorithm starts with a minimal feature set and iteratively includes additional features based on their contribution to classification accuracy, ensuring that only significant features are selected (Das, 2001).
 
Experiments on both artificial datasets, like the Monks problems, and real-world datasets, such as the University of Wisconsin Breast Cancer Dataset, the US Congressional Voting Records Dataset, and the Pima Indians Diabetes Dataset, demonstrated significant reductions in the number of features while improving classification accuracy. This approach emphasizes the importance of feature selection in enhancing classification accuracy and reducing computational load, making it a valuable tool for various classification tasks and data collection processes.
 
In more recent developments, multiple criteria decision-making (MCDM) methods have been applied to feature selection, particularly in the context of text classification with small datasets. Kou et al. (2020) evaluated various feature selection methods using MCDM techniques across multiple text classification datasets. By employing methods like PROMETHEE, they ranked feature selection techniques based on a range of performance measures, including accuracy, stability, and efficiency. Their findings underscored the importance of comprehensive evaluation criteria in selecting the most appropriate feature selection method, tailored to specific dataset characteristics.
 
### Conclusion
The trajectory of feature selection in machine learning illustrates a progression from simple, univariate methods to sophisticated, model-based approaches that address complex issues like multicollinearity and multi-criteria decision making. Early skepticism about "data dredging" has given way to an appreciation of the nuanced techniques necessary for robust predictive modeling. Feature selection methods have evolved to include similarity-based, information-theoretical-based, sparse-learning-based, statistical-based, filter, wrapper, embedded, hybrid, supervised, semi-supervised, and unsupervised approaches. Modern methods, such as neural networks, further enhance the accuracy and efficiency of feature selection, ensuring that only the most relevant features are utilized. As the field continues to advance, it is clear that feature selection will remain a cornerstone of machine learning, improving model performance and reducing computational complexity across a wide range of applications.


## Methods

### Datasets

**Birds' Songs Numeric Dataset**

Birds use calls and songs for a plethora of purposes: from attracting potential partners to asserting dominance over a territory.
This Dataset is comprised of a balanced training set and a test set of 88 species grouped in 61 genera of spectral features extracted from the birds' songs.

https://www.kaggle.com/datasets/fleanend/birds-songs-numeric-dataset

**Spambase**

Classifying Email as Spam or Non-Spam

https://archive.ics.uci.edu/dataset/94/spambase

**Coronavirus Tweets**

Perform Text Classification on the data. The tweets have been pulled from Twitter and manual tagging has been done then.

https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification


### Experiment 1: Selection Bias

### Experiment 2: Multicollinearity

## Analysis and Results

### Data and Visualization

### Statistical Modeling

```{r}

```

### Conclusion

## References


Foster, D. P., & Stine, R. A. (2004). Variable Selection in Data Mining: Building a Predictive Model for Bankruptcy. Journal of the American Statistical Association, 99(466), 303–313. http://www.jstor.org/stable/27590387

Guyon, I., Weston, J., Barnhill, S. et al. (2022) Gene Selection for Cancer Classification using Support Vector Machines. Machine Learning 46, 389–422. https://doi.org/10.1023/A:1012487302797

Basu, I., Maji, S. (2022). Multicollinearity Correction and Combined Feature Effect in Shapley Values. In: Long, G., Yu, X., Wang, S. (eds) AI 2021: Advances in Artificial Intelligence. AI 2022. Lecture Notes in Computer Science(), vol 13151. Springer, Cham. https://doi.org/10.1007/978-3-030-97546-3_7

Kou, G., Yang, P., Peng, Y., Xiao, F., Chen, Y., & Alsaadi, F. E. (2020). Evaluation of feature selection methods for text classification with small datasets using multiple criteria decision-making methods. Applied Soft Computing, 86, 105836. https://doi.org/10.1016/j.asoc.2019.105836

V. Onnia, M. Tico and J. Saarinen, "Feature selection method using neural network," Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205), Thessaloniki, Greece, 2001, pp. 513-516 vol.1, doi: 10.1109/ICIP.2001.959066.

BÜYÜKKEÇECİ, Mustafa & Okur, Mehmet. (2022). A Comprehensive Review of Feature Selection and Feature Selection Stability in Machine Learning. GAZI UNIVERSITY JOURNAL OF SCIENCE. 36. 10.35378/gujs.993763.

Shao, Sisi & Ribeiro, Pedro & Ramirez, Christina & Moore, Jason. (2024). A review of feature selection strategies utilizing graph data structures and knowledge graphs.

A. Benkessirat and N. Benblidia, "Fundamentals of Feature Selection: An Overview and Comparison," 2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA), Abu Dhabi, United Arab Emirates, 2019, pp. 1-6, doi: 10.1109/AICCSA47632.2019.9035281. 


Abid-Althaqafi, N.R.,Alsalamah, H.A. (2024). The Effect of Feature Selection on the Accuracy of X-Platform User Credibility Detection with Supervised Machine Learning. Retrieved from: https:// doi.org/10.3390/electronics13010205

 Yu, L., Liu, H. (2003). Feature selection for high-dimensional data: A fast correlation-based filter solution. In Proceedings of the 20th International Conference on Machine Learning (ICML-03) (pp. 856-863).

Ambarwati, Y. S., Uyun, S. (2020). Feature selection on Magelang duck egg candling image using variance threshold method. 2020 3rd International Seminar on Research of Information Technology and Intelligent Systems (ISRITI). Retrieved from: https://doi.org/10.1109/ISRITI51436.2020.9315486

Demir – Kavuk, O., Kamanda, M., Akutsu, T., Knapp, E.W. (2011). Prediction using step-wise L1, L2 regularization and feature selection for small data sets with large number of features. BMC Bioinformatics.

Li, Jundong & Cheng, Kewei & Wang, Suhang & Morstatter, Fred & Trevino, Robert & Tang, Jiliang & Liu, Huan. (2016). Feature Selection: A Data Perspective. ACM Computing Surveys. 50. 10.1145/3136625.

Venkatesh, B & Anuradha, J.. (2019). A Review of Feature Selection and Its Methods. Cybernetics and Information Technologies. 19. 3. 10.2478/cait-2019-0001.

A. Jović, K. Brkić and N. Bogunović, "A review of feature selection methods with applications," 2015 38th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), Opatija, Croatia, 2015, pp. 1200-1205, doi: 10.1109/MIPRO.2015.7160458. 

Miao, J., & Niu, L. (2016). A survey on feature selection. *Procedia Computer Science, 91*, 919-926. https://doi.org/10.1016/j.procs.2016.07.111

