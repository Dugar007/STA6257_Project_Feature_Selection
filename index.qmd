---
title: "Feature Selection: An Exploration of Algorithm Performance"
author: "Jason Case, Abhishek Dugar, Daniel Nkuah, Khoa Tran"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---
# Introduction
The discipline of feature selection in machine learning has evolved significantly over the past few decades, transforming from a practice once stigmatized as "data dredging" to an essential component of modern predictive modeling. Early methods for variable selection were initially met with skepticism by the statistical community, which viewed them as potentially spurious means of uncovering patterns in data. These reservations stemmed from concerns about overfitting and the robustness of statistical inferences drawn from such techniques.
 
**Early Methods**

Feature selection's early forays began with relatively simple approaches like forward, backward, and stepwise variable selection in linear models. Forward selection starts with no variables in the model, adding them one by one based on specific criteria until no significant improvement is observed. Conversely, backward selection begins with all candidate variables, removing the least significant ones iteratively. Stepwise selection combines both approaches, iteratively adding and removing variables to optimize model performance.
 
These methods were foundational but had limitations, particularly when applied to datasets with high dimensionality. For instance, @foster2004variable in their study on predicting bankruptcy highlighted the complexity of variable selection when dealing with large sets of predictors. They utilized a 20:80 train/validation split to ensure robust model evaluation and critiqued traditional criteria like the Akaike Information Criterion (AIC) for being too liberal, risking overfitting. Instead, they advocated for methods like the Bonferroni correction, which, though conservative, provided more reliable results by dynamically adjusting p-values and avoiding inflated values from high leverage points.
 
Univariate screening procedures marked another step in the evolution of feature selection. These methods involve assessing each predictor variable independently for its relationship with the target variable, selecting those that meet a certain statistical threshold. While straightforward and computationally inexpensive, univariate methods often fail to capture the complex interdependencies between variables. 
 
In the context of gene selection for cancer classification, @guyon2002gene demonstrated the limitations of univariate methods. Prior to their work, gene selection relied heavily on simple statistical measures such as correlations, which often resulted in large, redundant sets of genes. The advent of Recursive Feature Elimination (RFE) represented a significant improvement. RFE uses Support Vector Machines (SVMs) to iteratively eliminate the least important features, refining the feature set to a more relevant subset and thus enhancing model performance. There were multiple tests that demonstrated that SVMs outperforms the baseline methods. First, the best leave-one-out performance results in a 100% accuracy for SVMs and only 90% for the baseline method (combination of univariate classifiers). The second example was using the statistical test, (1-n), where n equals the number of features, we can state with 99.3% confidence that SVMs are better than the baseline method. Additionally, when comparing the SVM-selected genes and baseline-selected genes, the SVM genes are better with a 84.1% confidence based on test error rate and a 99.2% confidence based on the test rejection rate. Figure 1 shows the performance comparison between SVMs and the baseline method, success rate, acceptance rate, extremal margin, and median margin and also indicates that there is a significant difference between the gene selection. Table 1 summarizes the results of the comparison between SVM and the baseline method, and it also indicates that SVMs are significantly better based on the est error rate and that it achieves a higher accuracy in classifying the samples. This method highlighted the critical role of robust feature selection in bioinformatics, where the choice of features often influenced classification outcomes more than the specific classifier used.

**Figure 1. Reproduction of Figure 2 in @guyon2002gene.**

::: {#figure-1 .figure}
![Figure 1](Figure_1.jpeg){ width=100% }
*This figure shows a performance comparison between SVMs and the baseline method using Leukemia data. Classifiers were trained on gene subsets selected by SVMs and the baseline method using the training set of the Leukemia data. The number of genes is color-coded and indicated in the legend. Quality indicators are displayed radially: channels 1–4 show cross-validation results using the leave-one-out method; channels 5–8 show test set results. The indicators include success rate (suc), acceptance rate (acc), extremal margin (ext), and median margin (med). Coefficients are rescaled to have zero mean and variance 1 across all plots. For each classifier, a larger colored area indicates better performance. The figure reveals no significant difference in classifier performance on this dataset, but a significant difference in gene selections.*
:::


**Table 1. Reproduction of Table 6 in @guyon2002gene.**

::: {#table-1 .figure}
![Table 1](Table_1.jpeg){ width=100% }
*This table presents the performance of the top classifiers on test data consisting of 34 samples. The table includes combinations of SVM or Baseline genes with SVM or Baseline classifiers. For each combination, the table shows the number of selected genes, the number of errors at zero rejection, and the number of rejections at zero error. The number of genes represents the subset selected by the method that provides the best classification performance. Patient ID numbers corresponding to classification errors are indicated in brackets. Additionally, results without any gene selection are provided for comparison.*
:::
 
**Modern Methods**

Modern feature selection methods have become essential for data preprocessing, extracting the most important information from big data. By reducing the number of features, these methods mitigate the risk of overfitting and enhance the performance of machine learning models and pattern recognition systems [@venkatesh2019review].
 
Advanced feature selection methods include similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based approaches. Similarity-based approaches select features based on their similarity or dissimilarity, while information-theoretical-based approaches use concepts like entropy and mutual information to evaluate feature importance. Sparse-learning-based approaches focus on identifying a small, essential set of features by limiting the number of features selected, and statistical-based approaches utilize statistical tests and models to determine the significance of features for predicting the target variable [@li2017feature].
 
Early and modern approaches are now categorized into filter, wrapper, embedded, and hybrid approaches. The filter model selects features based on the general properties of the training data, independent of any learning algorithm, making it computationally efficient for large datasets [@das2001filters; @kohavi1997wrappers]. Within the filter model, feature selection algorithms can be further divided into feature weighting algorithms, which assign weights to each feature based on their relevance, and subset search algorithms, which explore candidate feature subsets based on specific evaluation measures [@kohavi1997wrappers; @liu2012feature]. The wrapper model, on the other hand, uses a specific learning algorithm to evaluate and determine which features to keep, often leading to better performance but at a higher computational cost [@langley1994selection]. Embedded methods perform feature selection during the model training process, integrating selection directly with learning. Hybrid methods combine the best aspects of filter and wrapper methods to achieve optimal performance with manageable computational complexity [@venkatesh2019review; @jovic2015review].
 
Feature selection can also be divided into supervised, semi-supervised, and unsupervised categories. Supervised learning involves labeled data, semi-supervised learning utilizes both labeled and unlabeled data, and unsupervised learning works with unlabeled data. @miao2016survey emphasized that the effectiveness of feature selection methods varies depending on the dataset and model used, necessitating tailored approaches to feature selection.
 
As feature selection methods evolved, addressing multicollinearity—a scenario where predictor variables are highly correlated—became a critical focus. Traditional methods often assumed independence among features, leading to inaccurate importance measures. @basu2022multicollinearity tackled this issue by introducing a framework to adjust for feature correlations when calculating Shapley values. Their method, termed Multicollinearity Corrected (MCC) Shapley values, provided more accurate measures of feature importance by accounting for existing correlations. Their experiments on datasets with moderate to high feature correlations demonstrated the superiority of MCC Shapley values over non-corrected ones, offering a more reliable interpretation of feature importance in the presence of multicollinearity.
 
A novel feature selection algorithm leveraging neural networks aims to enhance classification accuracy while reducing computational load by selecting only the most relevant features. This method utilizes feed-forward neural networks (FFNNs) and integrates seamlessly with other classifiers. The algorithm starts with a minimal feature set and iteratively includes additional features based on their contribution to classification accuracy, ensuring that only significant features are selected [@onnia2001feature].
 
Experiments on both artificial datasets, like the Monks problems, and real-world datasets, such as the University of Wisconsin Breast Cancer Dataset, the US Congressional Voting Records Dataset, and the Pima Indians Diabetes Dataset, demonstrated significant reductions in the number of features while improving classification accuracy. This approach emphasizes the importance of feature selection in enhancing classification accuracy and reducing computational load, making it a valuable tool for various classification tasks and data collection processes.
 
In more recent developments, multiple criteria decision-making (MCDM) methods have been applied to feature selection, particularly in the context of text classification with small datasets. @kou2020evaluation evaluated various feature selection methods using MCDM techniques across multiple text classification datasets. By employing methods like PROMETHEE, they ranked feature selection techniques based on a range of performance measures, including accuracy, stability, and efficiency. Their findings underscored the importance of comprehensive evaluation criteria in selecting the most appropriate feature selection method, tailored to specific dataset characteristics.
 
The trajectory of feature selection in machine learning illustrates a progression from simple, univariate methods to sophisticated, model-based approaches that address complex issues like multicollinearity and multi-criteria decision making. Early skepticism about "data dredging" has given way to an appreciation of the nuanced techniques necessary for robust predictive modeling. Feature selection methods have evolved to include similarity-based, information-theoretical-based, sparse-learning-based, statistical-based, filter, wrapper, embedded, hybrid, supervised, semi-supervised, and unsupervised approaches. Modern methods, such as neural networks, further enhance the accuracy and efficiency of feature selection, ensuring that only the most relevant features are utilized. As the field continues to advance, it is clear that feature selection will remain a cornerstone of machine learning, improving model performance and reducing computational complexity across a wide range of applications.


# Methods
Our exploration of feature selection will begin by comparing the performance of various feature selection methods on a variety of real-world datasets. We will look at four different aspects of performance: overall model performance on the holdout set, variance in performance between the training and holdout set, and parsimony of the final model. We explored an algorithm from each category of feature selection methods.

**Correlation - Based Feature Selection (CFS)**

A common filtering method uses the correlation coefficient to measure the relationship between each variable and the target, independently.

CFS's feature subset evaluation function is:

$\text{Ms} = \frac{k\bar{\text{rcf}}}{\sqrt k + k(k - 1)\bar{\text{rff}}}$

where Ms is the heuristic ”merit” of a feature subset S containing k features, $\bar{\text{rcf}}$ is the mean feature-class correlation and $\bar{\text{rff}}$ is the average feature - feature intercorrelation. The numerator of this equation reflects the extent to which a set of features is indicative of the class, while the denominator measures the level of redundancy among the features [@hall1999correlation].
 

**Recursive Feature Elimination (RFE)**

RFE is a common wrapper method that removes variables iteratively.

For an unknown input vector x, a linear classifier takes the form

$y(x)=$sign$(w⋅x−b)$

The recursive feature elimination method for support vector machines can be executed through these iterative steps[@guyon2002gene].

1. Train the SVM classifier
2. compute the ranking criterion for all features
3. remove the features with smallest ranking values

For LS-SVM, substituting the KKT conditions into the Lagrangian gives the objective function

$L = -\frac{1}{2} \sum \sum \alpha_i \alpha_j (K(x_i, x_j) + \frac{\sigma_{ij}}{C}) + \sum \alpha_i y_i$

Where ${\sigma_{ij}} = 1$ if $i=j$ and $0$ otherwise.

The ranking criterion can be obtained by:

$D(L^{-m}) = -\frac{1}{2} \sum \sum \alpha_i \alpha_j (K(x_i, x_j) - (K(x_i^{-m}, x_j^{-m})$

Where $x_i^{-m}$ , $x_j^{-m}$ are the vectors in which $m$-th feature has been removed.

**Least Absolute Shrinkage and Selection Operator (LASSO)**

LASSO is an embedded method that uses L1 Regularization to shrink coefficients to zero, effectively performing feature selection during parameter estimation. LASSO minimizes the sum of squared errors while imposing an upper limit on the sum of the absolute values of the model parameters.

The LASSO estimate is defined by the solution to the l1 optimization problem

minimize  $\frac{\| Y - X\beta \|_2^2}{n}$ subject to $X \sum_{j=1}^{k} \|\beta_j\|_1 < t$

where $t$ is the upper bound for the sum of the coefficients [@buhlmann2011statistics]

**CFS & RFE**

For the hybrid method, we will combine the filter and wrapper methods; first filtering using the correlation coefficient and then removing variables iteratively using RFE.

# Analysis and Results

```{r}
#| include: false
#environment setup (hidden from output)
library(dplyr)
library(quanteda)
library(caret)
library(wordcloud)
library(ggplot2)
library(glmnet)
library(doParallel)
library(jsonlite)
library(knitr)
library(kableExtra)


# Global parameter to force overwriting of existing cache files
FORCE_OVERWRITE <- FALSE

numCores <- max(c(1, detectCores() - 2))
cl <- makeCluster(numCores)
registerDoParallel(cl)

#define some helper functions

download_kaggle_dataset <- function(dataset, path) {
  # Check if the kaggle command is available
  if (system("which kaggle", intern = TRUE) == "") {
    stop("Kaggle API is not installed or not in PATH. Please install it first.")
  }
  
  # Ensure the destination directory exists
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE)
  }
  
  # Construct the download command
  command <- sprintf("kaggle datasets download -d %s -p %s", dataset, path)
  
  # Execute the command
  system(command)
  
  # Unzip the downloaded file
  zipfile <- list.files(path, pattern = "*.zip", full.names = TRUE)
  if (length(zipfile) > 0) {
    unzip(zipfile, exdir = path)
    file.remove(zipfile)
  }
}

download_and_extract_zip <- function(url, dest_dir) {
  # Ensure the destination directory exists
  if (!dir.exists(dest_dir)) {
    dir.create(dest_dir, recursive = TRUE)
  }
  
  # Create a temporary file to hold the downloaded zip file
  temp_zip <- tempfile(fileext = ".zip")
  
  # Download the zip file
  download.file(url, temp_zip, mode = "wb")
  
  # Extract the contents of the zip file
  unzip(temp_zip, exdir = dest_dir)
  
  # Remove the temporary zip file
  unlink(temp_zip)
}

# Function to cache parameters
cache_parameter <- function(name, value = NULL, path = "cache/", prefix = "param_") {
  # Ensure the cache directory exists
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE)
  }
  
  # Construct the full file name
  file_name <- paste0(path, prefix, name, ".json")
  
  # Check for file existence
  if (file.exists(file_name) && !FORCE_OVERWRITE) {
      # Load and return the value from the file
      cached_value <- fromJSON(file_name)
      # Cast to the appropriate type
      if (cached_value$type == "numeric") {
        return(as.numeric(cached_value$value))
      } else if (cached_value$type == "integer") {
        return(as.integer(cached_value$value))
      } else if (cached_value$type == "list") {
        return(as.list(cached_value$value))
      } else if (cached_value$type == "vector_numeric") {
        return(as.numeric(cached_value$value))
      } else if (cached_value$type == "vector_integer") {
        return(as.integer(cached_value$value))
      } else {
        stop("Unsupported cached value type.")
      }
    } else {
      cached_value <- NULL
    }

  if (is.null(value)) {
    return(cached_value)
  } else
  {
    # Determine the type of the value and write it to the file
    if (is.numeric(value) && length(value) == 1) {
      value_type <- "numeric"
    } else if (is.integer(value) && length(value) == 1) {
      value_type <- "integer"
    } else if (is.list(value)) {
      value_type <- "list"
    } else if (is.numeric(value) && length(value) > 1) {
      value_type <- "vector_numeric"
    } else if (is.integer(value) && length(value) > 1) {
      value_type <- "vector_integer"
    } else {
      stop("Unsupported value type. Only numeric, integer, vectors, and list are supported.")
    }
    # Write the value to the file as JSON
    write_json(list(type = value_type, value = value), file_name)
    
    # Return the value
    return(value)
  }
  

}

# download datasets, if necessary
# download_and_extract_zip("https://archive.ics.uci.edu/static/public/94/spambase.zip", "./data/spambase")
# download_kaggle_dataset("datatattle/covid-19-nlp-text-classification", "./data/covid")

```

## Data and Visualization

We evaluated the performance of the methods on two classification tasks. The datasets and associated tasks used were:

**Spambase Dataset**

The Spambase dataset from the UCI Machine Learning Repository contains 4601 instances of emails, with 57 features for classification tasks to determine whether an email is spam (1) or not (0). Features include word and character frequency counts, as well as measures of consecutive capital letters. It is commonly used for building and testing spam detection models [@misc_spambase_94]. This dataset was selected because it contains a dense set of numeric predictor variables.

```{r}
#Importing Spam Data
last_57_lines <- tail(readLines("./data/spambase/spambase.names"), 57)
# Function to extract and sanitize column names
extract_column_names <- function(line) {
  # Extract the name before the colon
  name <- strsplit(line, ":")[[1]][1]
  # Sanitize the name by replacing special characters with underscores
  sanitized_name <- gsub("[^a-zA-Z0-9_]", "_", name)
  return(sanitized_name)
}

# Apply the function to each line to get the column names
raw_column_names <- c(sapply(last_57_lines, extract_column_names), 'flag_spam')


# Ensure unique column names
unique_column_names <- make.unique(raw_column_names)
spam_data <- read.csv("./data/spambase/spambase.data", header = FALSE, col.names = unique_column_names) %>%
  mutate(flag_spam = factor(flag_spam, levels = c(0, 1)))


# Plotting in graph to see the distribution and find outlier 
ggplot(spam_data, aes(x = flag_spam)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Spam Or Not", y = "Count", title = "Distribution")

num_columns <- ncol(spam_data) - 1
num_nonzero <- sum((spam_data %>% select(-flag_spam)) != 0)
total_elements <- prod(dim(spam_data %>% select(-flag_spam)))
sparsity <- ((total_elements - num_nonzero) / total_elements) * 100

cat("Number of columns in the training dataset:", num_columns, "\n")
cat("Average percent of values that are 0 in the training dataset:", sparsity, "%\n")
```

**COVID-19 NLP Text Classification Dataset**

The COVID-19 NLP Text Classification dataset on Kaggle consists of tweets related to COVID-19, labeled for sentiment analysis. It includes features like tweet content, sentiment labels (positive, negative, neutral), and additional metadata. This dataset is designed for natural language processing tasks to analyze public sentiment during the pandemic [@datatattle2024covid]. An NLP task was chosen because the vector of feature space is sparse and of high dimension.


```{r}
# Importing Train Data
corona_train <- read.csv("./data/covid/Corona_NLP_train.csv")[, c("OriginalTweet", "Sentiment")]

# Importing Test Data
corona_test <- read.csv("./data/covid/Corona_NLP_test.csv")[, c("OriginalTweet", "Sentiment")]

# Recode Sentiment to factors
corona_train <- corona_train %>%
  filter(Sentiment != "Neutral") %>%
  mutate(Sentiment = recode(Sentiment,
                            "Extremely Negative" = "Negative",
                            "Extremely Positive" = "Positive"),
         Sentiment = factor(Sentiment, levels = c("Negative", "Positive")))

corona_test <- corona_test %>%
  filter(Sentiment != "Neutral") %>%
  mutate(Sentiment = recode(Sentiment,
                            "Extremely Negative" = "Negative",
                            "Extremely Positive" = "Positive"),
         Sentiment = factor(Sentiment, levels = c("Negative", "Positive")))

# Preprocessing and tokenization using quanteda
preprocess_text <- function(text_column) {
  tokens <- tokens(text_column, 
                   what = "word", 
                   remove_punct = TRUE, 
                   remove_numbers = TRUE,
                   remove_symbols = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stopwords("english")) %>%
    tokens_wordstem()
  
  # Create a Document-Feature Matrix (DFM)
  dfm <- dfm(tokens)
  
  return(dfm)
}

corona_train_dfm <- dfm_trim(preprocess_text(corona_train$OriginalTweet), min_termfreq = 10)
corona_test_dfm <- dfm_match(preprocess_text(corona_test$OriginalTweet), features = featnames(corona_train_dfm))

# Convert DFM to sparse matrix
corona_train_sparse <- as(corona_train_dfm, "dgCMatrix")
corona_test_sparse <- as(corona_test_dfm, "dgCMatrix")

# Convert DFM to data frame for visualization
corona_train_data <- convert(corona_train_dfm, to = "data.frame")
corona_test_data <- convert(corona_test_dfm, to = "data.frame") 

# Plotting in graph to see the distribution and find outlier

ggplot(corona_train, aes(x = Sentiment)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Sentiment", y = "Count", title = "Distribution of Sentiment Labels")


# Calculate and visualize mean, standard deviation, and range of tokens per document
token_counts <- rowSums(corona_train_dfm)


ggplot(data = data.frame(token_counts), aes(x = token_counts)) +
  geom_histogram(binwidth = 5, color = "black") +
  labs(title = "Histogram of Token Counts",
       x = "Token Counts",
       y = "Frequency") 


# Calculate and report sparsity
num_columns <- ncol(corona_train_dfm)
num_nonzero <- sum(corona_train_dfm@x != 0)
total_elements <- prod(dim(corona_train_dfm))
sparsity <- ((total_elements - num_nonzero) / total_elements) * 100

cat("Number of columns in the training dataset:", num_columns, "\n")
cat("Average percent of values that are 0 in the training dataset:", sparsity, "%\n")

# Create a word frequency cloud
# word_freq <- colSums(corona_train_dfm)
# wordcloud(names(word_freq), freq = word_freq, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
```



## Statistical Modeling

For each classification task, we held out 30% of the data at random. We evaluated performance on four metrics: f1 score on the holdout set, difference between f1 score on the training and holdout set, and number of variables selected 

To establish a baseline for performance, we first trained a multinomial logistic regression model using all available variables. We then use the four selection methods, training a multinomial logistic regression model on the features selected with each method. 

We used 10-fold cross validation accuracy to select the variables in all methods requiring a validation dataset (RFE, LASSO).


**Spambase Dataset**

Data scientists often include interaction terms in a model to capture the combined effect of two or more variables on the target variable. Interaction terms help to identify if the effect of one predictor on the outcome changes at different levels of another predictor, providing a more nuanced and accurate representation of the relationships within the data. This can lead to improved model performance and better insights into complex dependencies between variables. However, including interaction terms in a model significantly increases the number of features, as it involves creating new variables that represent the interactions between the original variables. To explore this effect, we added all possible pairwise interaction terms for the 52 variables in the Spambase data, resulting in $\( \frac{52 \times 51}{2} = 1,326 \)$ additional features. We centered and scaled the original features prior to adding the terms. We use a binary logistic regression to predict if an email is spam (1) or not spam (0).

```{r}
set.seed(42)
# Create a train-test split
trainIndex <- sample(1:nrow(spam_data), size = 0.8 * nrow(spam_data))

# Split the data
spam_train <- spam_data[trainIndex, ]
spam_test <- spam_data[-trainIndex, ]

X_train_raw <- spam_train %>% select(-flag_spam)
X_test_raw  <- spam_test %>% select(-flag_spam)

y_train  <- spam_train$flag_spam
y_test <- spam_test$flag_spam

# scale the data, calculate all two-way interactions, and drop the intercept
preprocess_params <- preProcess(X_train_raw, method = c("center", "scale"))
X_train <- model.matrix( ~ .^2, predict(preprocess_params, X_train_raw))[, -1]
X_test <- model.matrix( ~ .^2, predict(preprocess_params, X_test_raw))[, -1]

# Fit the logistic regression model with glmnet
baseline_lambda = 0
baseline_maxit = 500000
baseline_alpha = 0
validation_folds = 10

baseline_model <- glmnet(
  X_train, 
  y_train, 
  family = "binomial", 
  alpha = baseline_alpha,
  lambda.min.ratio = baseline_lambda,
  maxit = baseline_maxit,
  parallel = TRUE
)

# Predict on training and test datasets
train_predictions_prob <- predict(baseline_model, X_train, s = min(baseline_model$lambda), type = "response")
test_predictions_prob <- predict(baseline_model, X_test, s = min(baseline_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, 1, 0), levels = c(0, 1))
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, 1, 0), levels = c(0, 1))

# Calculate F1 score function
calculate_f1 <- function(actual, predicted) {
  confusion <- confusionMatrix(predicted, actual)
  f1 <- confusion$byClass["F1"]
  return(f1)
}


# Calculate F1 scores for training and test datasets
train_f1_score <- calculate_f1(spam_train$flag_spam, train_predictions)
test_f1_score <- calculate_f1(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train)

coefficients <- coef(baseline_model, s = min(baseline_model$lambda))
non_zero_features <- sum(coefficients != 0) - 1  # Subtract 1 for the intercept

# Calculate the difference in F1 score between train and test data
f1_difference <- train_f1_score - test_f1_score

# Print the results
# cat("BASELINE RESULTS\n")
# cat("Number of features:", num_features, "\n")
# cat("Number of non-zero features:", non_zero_features, "\n")
# cat("F1 score on training data:", train_f1_score, "\n")
# cat("F1 score on test data:", test_f1_score, "\n")
# cat("Difference in F1 score between train and test data:", f1_difference, "\n")

spam_baseline_non_zero_features <- non_zero_features
spam_baseline_test_f1_score <- test_f1_score
spam_baseline_f1_difference <- f1_difference


#CORRELATION FEATURE SELECTION

correlations <- apply(X_train, 2, function(x) cor(x, as.numeric(y_train)))
abs_correlations <- abs(correlations)

# Step 2: Select features based on a correlation threshold
select_features <- function(threshold) {
  selected_features <- which(abs_correlations > threshold)
  return(selected_features)
}

# Step 3: Sweep through various thresholds and perform 10-fold cross-validation
percentiles <- seq(0, 1, length.out = 20)
thresholds <- quantile(abs_correlations, percentiles)
cv_results <- data.frame(threshold = numeric(), mean_f1 = numeric())
optimal_threshold <- cache_parameter('Spambase_cfs_optimal_threshold')
if (is.null(optimal_threshold)) {
  for (threshold in thresholds) {
    selected_features <- select_features(threshold)
    
    if (length(selected_features) == 0) {
      next
    }
    
    X_train_selected <- X_train[, selected_features, drop = FALSE]
    
    if (ncol(X_train_selected) < 2) {
      next
    }
    
    # Perform k-fold cross-validation
    train_control <- trainControl(method = "cv", number = validation_folds, verboseIter = TRUE)
    
    f1_scores <- c()
    
    # Define custom F1 score summary function
    f1_summary <- function(data, lev = NULL, model = NULL) {
      confusion <- confusionMatrix(data$pred, data$obs)
      f1 <- confusion$byClass["F1"]
      c(F1 = f1)
    }
    
    for (i in 1:validation_folds) {
      folds <- createFolds(y_train, k = validation_folds, list = TRUE, returnTrain = TRUE)
      f1_fold <- c()
      
      for (j in 1:validation_folds) {
        train_index <- folds[[j]]
        test_index <- setdiff(seq_len(nrow(X_train_selected)), train_index)
        
        x_train_cv <- X_train_selected[train_index, ]
        y_train_cv <- y_train[train_index]
        x_test_cv <- X_train_selected[test_index, ]
        y_test_cv <- y_train[test_index]
        
        model_cv <- glmnet(x_train_cv, y_train_cv, alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, family = "binomial", parallel = TRUE)
        
        pred_cv <- predict(model_cv, x_test_cv, s = min(model_cv$lambda), type = "response")
        pred_cv <- factor(ifelse(pred_cv > 0.5, 0, 1), levels = c(0, 1))
        actual_cv <- factor(ifelse(y_test_cv == 1, 1, 0), levels = c(0, 1))
        
        f1_fold <- c(f1_fold, calculate_f1(actual_cv, pred_cv))
      }
      
      f1_scores <- c(f1_scores, mean(f1_fold))
    }
    
    cv_results <- rbind(cv_results, data.frame(threshold = threshold, mean_f1 = mean(f1_scores)))
  }
  
  # Determine the optimal threshold
  optimal_threshold <-  cache_parameter('Spambase_cfs_optimal_threshold', cv_results$threshold[which.max(cv_results$mean_f1)]) 
}



# Step 4: Use the optimal threshold to train the final model
selected_features <- select_features(optimal_threshold)
X_train_selected <- X_train[, selected_features]
X_test_selected <- X_test[, selected_features]

final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, parallel = TRUE)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, 1, 0), levels = c(0, 1))
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, 1, 0), levels = c(0, 1))

# Calculate F1 scores for training and test datasets
train_f1_score <- calculate_f1(spam_train$flag_spam, train_predictions)
test_f1_score <- calculate_f1(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 

coefficients <- coef(final_model, s = min(final_model$lambda))
non_zero_features <- sum(coefficients != 0) - 1  # Subtract 1 for the intercept

# Calculate the difference in F1 score between train and test data
f1_difference <- train_f1_score - test_f1_score

# Print the results
# cat("CFS RESULTS\n")
# cat("Optimal Threshold:", optimal_threshold, "\n")
# cat("Number of features:", num_features, "\n")
# cat("Number of non-zero features:", non_zero_features, "\n")
# cat("F1 score on training data:", train_f1_score, "\n")
# cat("F1 score on test data:", test_f1_score, "\n")
# cat("Difference in F1 score between train and test data:", f1_difference, "\n")

spam_cfs_non_zero_features <- non_zero_features
spam_cfs_test_f1_score <- test_f1_score
spam_cfs_f1_difference <- f1_difference


# RECURSIVE FEATURE ELIMINATION
# Define custom RFE function
custom_rfe <- function(x, y, sizes, fold = validation_folds, parallel = TRUE) {
  results <- data.frame(num_features = integer(), mean_f1 = double())
  
  current_features <- seq_len(ncol(x))  # Start with all features
  best_features <- current_features
  
  for (size in sizes) {
    cat("Evaluating size:", size, "\n")
    
    # Fit glmnet model to get coefficients
    model <- glmnet(x[, current_features, drop = FALSE], y, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)
    coefs <- as.matrix(coef(model, s = model$lambda.min))
    
    # Get indices of the top 'size' features by their absolute coefficient values
    if (length(current_features) > size) {
      selected_features <- order(abs(coefs[-1, 1]), decreasing = TRUE)[1:size]
      current_features <- current_features[selected_features]
    }
    
    x_selected <- x[, current_features, drop = FALSE]
    
    folds <- createFolds(y, k = fold, list = TRUE, returnTrain = TRUE)
    f1_scores <- c()
    
    for (fold_idx in seq_along(folds)) {
      train_idx <- folds[[fold_idx]]
      test_idx <- setdiff(seq_len(nrow(x_selected)), train_idx)
      
      x_train_cv <- x_selected[train_idx, ]
      y_train_cv <- y[train_idx]
      x_test_cv <- x_selected[test_idx, ]
      y_test_cv <- y[test_idx]
      
      model_cv <- glmnet(x_train_cv, y_train_cv, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)
      
      if (length(model_cv$lambda) == 0) {
        next
      }
      
      pred_cv_prob <- predict(model_cv, x_test_cv, s = min(model_cv$lambda), type = "response")
      pred_cv <- factor(ifelse(pred_cv_prob > 0.5, 1, 0), levels = c(0, 1))
      
      f1 <- calculate_f1(y_test_cv, pred_cv)
      f1_scores <- c(f1_scores, f1)
    }
    
    mean_f1 <- mean(f1_scores, na.rm = TRUE)
    results <- rbind(results, data.frame(num_features = size, mean_f1 = mean_f1))
    
    if (mean_f1 == max(results$mean_f1, na.rm = TRUE)) {
      best_features <- current_features
    }
  }
  
  best_size <- results$num_features[which.max(results$mean_f1)]
  return(list(best_size = best_size, best_features = best_features, results = results))
}


# Generate sizes in geometric progression
generate_sizes <- function(start_size, end_size, num_steps) {
  ratio <- (end_size / start_size)^(1 / (num_steps - 1))
  sizes <- start_size * (ratio ^ (0:(num_steps - 1)))
  return(round(sizes))
}


# Perform recursive feature elimination
sizes <- generate_sizes(ncol(X_train), 100, 20)

best_features <- cache_parameter('Spambase_rfe_best_features')
if (is.null(best_features)) {
  rfe_results <- custom_rfe(X_train, y_train, sizes = sizes)
  best_features <- cache_parameter('Spambase_rfe_best_features', rfe_results$best_features)
}

# Fit the final model using selected features
X_train_selected = X_train[, best_features, drop=FALSE]
X_test_selected = X_test[, best_features, drop=FALSE]
final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, 1, 0), levels = c(0, 1))
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, 1, 0), levels = c(0, 1))


# Calculate F1 scores for training and test datasets
train_f1_score <- calculate_f1(spam_train$flag_spam, train_predictions)
test_f1_score <- calculate_f1(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 

coefficients <- coef(final_model, s = min(final_model$lambda))
non_zero_features <- sum(coefficients != 0) - 1  # Subtract 1 for the intercept

# Calculate the difference in F1 score between train and test data
f1_difference <- train_f1_score - test_f1_score

# cat("RFE RESULTS\n")
# cat("Number of features:", num_features, "\n")
# cat("Number of non-zero features:", non_zero_features, "\n")
# cat("F1 score on training data:", train_f1_score, "\n")
# cat("F1 score on test data:", test_f1_score, "\n")
# cat("Difference in F1 score between train and test data:", f1_difference, "\n")

spam_rfe_non_zero_features <- non_zero_features
spam_rfe_test_f1_score <- test_f1_score
spam_rfe_f1_difference <- f1_difference

#LASSO REGRESSION
best_lambda <- cache_parameter('Spambase_lasso_best_lambda')
if (is.null(best_lambda)) {
  cv_lasso <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 1, maxit = baseline_maxit, nfolds = validation_folds)
  best_lambda <- cache_parameter('Spambase_lasso_best_lambda', cv_lasso$lambda.min)
}

# Fit the final Lasso model using the best lambda
final_model <- glmnet(X_train, y_train, family = "binomial", maxit = baseline_maxit, alpha = 1, lambda = best_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train, s = best_lambda, type = "response")
test_predictions_prob <- predict(final_model, newx = X_test, s = best_lambda, type = "response")

# Calculate F1 scores for training and test datasets
train_f1_score <- calculate_f1(spam_train$flag_spam, train_predictions)
test_f1_score <- calculate_f1(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train) 

coefficients <- coef(final_model, s = best_lambda)
non_zero_features <- sum(coefficients != 0) - 1  # Subtract 1 for the intercept

# Calculate the difference in F1 score between train and test data
f1_difference <- train_f1_score - test_f1_score

# cat("LASSO RESULTS\n")
# cat("Best lambda from cross-validation: ", best_lambda, "\n")
# cat("Number of features:", num_features, "\n")
# cat("Number of non-zero features:", non_zero_features, "\n")
# cat("F1 score on training data:", train_f1_score, "\n")
# cat("F1 score on test data:", test_f1_score, "\n")
# cat("Difference in F1 score between train and test data:", f1_difference, "\n")

spam_lasso_non_zero_features <- non_zero_features
spam_lasso_test_f1_score <- test_f1_score
spam_lasso_f1_difference <- f1_difference


#CFS + RFE REGRESSION

correlation_threshold = thresholds[5]
best_features <- cache_parameter('Spambase_cfs_rfe_best_features')
if (is.null(best_features)) {
  cfs_selected_features <- select_features(correlation_threshold)
  
  X_train_cfs_selected <- X_train[, cfs_selected_features, drop = FALSE]
  
  # Perform recursive feature elimination
  sizes <- generate_sizes(ncol(X_train_cfs_selected), 100, 20)
  rfe_results <- custom_rfe(X_train_cfs_selected, y_train, sizes = sizes)
  
  best_features <- cache_parameter('Spambase_cfs_rfe_best_features', rfe_results$best_features)
}


# Fit the final model using selected features
X_train_selected = X_train[, best_features, drop=FALSE]
X_test_selected = X_test[, best_features, drop=FALSE]
final_model <- glmnet(X_train_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = X_train_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = X_test_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, 1, 0), levels = c(0, 1))
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, 1, 0), levels = c(0, 1))


# Calculate F1 scores for training and test datasets
train_f1_score <- calculate_f1(spam_train$flag_spam, train_predictions)
test_f1_score <- calculate_f1(spam_test$flag_spam, test_predictions)

# Calculate the number of features
num_features <- ncol(X_train_selected) 

coefficients <- coef(final_model, s = min(final_model$lambda))
non_zero_features <- sum(coefficients != 0) - 1  # Subtract 1 for the intercept

# Calculate the difference in F1 score between train and test data
f1_difference <- train_f1_score - test_f1_score

# cat("CFS + RFE RESULTS\n")
# cat("Correlation Threshold:", correlation_threshold, "\n")
# cat("Number of features:", num_features, "\n")
# cat("Number of non-zero features:", non_zero_features, "\n")
# cat("F1 score on training data:", train_f1_score, "\n")
# cat("F1 score on test data:", test_f1_score, "\n")
# cat("Difference in F1 score between train and test data:", f1_difference, "\n")

spam_cfs_rfe_non_zero_features <- non_zero_features
spam_cfs_rfe_test_f1_score <- test_f1_score
spam_cfs_rfe_f1_difference <- f1_difference


var_method <- c('Baseline', 'CFS', 'RFE', 'LASSO', 'CFS + RFE')
num_features <- c(spam_baseline_non_zero_features, spam_cfs_non_zero_features, spam_rfe_non_zero_features, spam_lasso_non_zero_features, spam_cfs_rfe_non_zero_features)
f1_score <- c(spam_baseline_test_f1_score, spam_cfs_test_f1_score, spam_rfe_test_f1_score, spam_lasso_test_f1_score, spam_cfs_rfe_test_f1_score)
f1_diff <- c(spam_baseline_f1_difference, spam_cfs_f1_difference, spam_rfe_f1_difference, spam_lasso_f1_difference, spam_cfs_rfe_f1_difference)

data <- data.frame(var_method, num_features, f1_score, f1_diff)

# Format numbers
data <- data %>%
  mutate(
    f1_score = round(f1_score, 3),
    f1_diff = round(f1_diff, 3)
  )

# Create and render the table
kable(data, format = "html", col.names = c("Method", "Number of Features", "Test F1 Score", "F1 Decrease from Train")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

**COVID-19 NLP Text Classification Dataset**

In a bag-of-words NLP classification task, text data is converted into a matrix of word frequencies or occurrences, where each row represents a document and each column represents a unique word from the entire corpus. The classification model then uses this matrix to predict the category of each document. One of the primary challenges in this approach is data sparsity, as most documents contain only a small subset of all possible words, resulting in a sparse matrix with many zero entries. Additionally, the size of the feature space can become extremely large, especially with extensive vocabularies, which can lead to increased computational demands, memory consumption, and the risk of overfitting, necessitating feature selection or dimensionality reduction techniques. 

We explored feature reduction methods on an nlp classifcation task. We first recoded sentiment labels to simplify the classification task: "Extremely Negative" and "Extremely Positive" are combined with "Negative" and "Positive" respectively, and "Neutral" sentiments are filtered out. We then process the Text by tokenizing the tweets, removing punctuation, numbers, and symbols, converting to lowercase, removing English stopwords, and stemming the words. This results in a Document-Feature Matrix (DFM), which is further refined by trimming infrequent terms in the training data and matching the test data features to those in the training set. We use a binary logistic regression to predict if the sentiment of a tweet is positive (1) or negative (0).

```{r}
# Fit the logistic regression model with glmnet
baseline_lambda = 0
baseline_maxit = 500000
baseline_alpha = 0
validation_folds = 10

set.seed(42)
# Get targets
y_train <- corona_train$Sentiment
y_test <- corona_test$Sentiment

train_sparse <- corona_train_sparse
test_sparse <- corona_test_sparse

baseline_model <- glmnet(
  train_sparse, 
  y_train, 
  family = "binomial", 
  alpha = baseline_alpha,
  lambda.min.ratio = baseline_lambda,
  maxit = baseline_maxit,
  parallel = TRUE
)

# Predict on training and test datasets
train_predictions_prob <- predict(baseline_model, train_sparse, s = min(baseline_model$lambda), type = "response")
test_predictions_prob <- predict(baseline_model, test_sparse, s = min(baseline_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, "Positive", "Negative"), levels = c("Negative", "Positive"))
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, "Positive", "Negative"), levels = c("Negative", "Positive"))

# Calculate F1 score function
calculate_f1 <- function(actual, predicted) {
  confusion <- confusionMatrix(predicted, actual)
  f1 <- confusion$byClass["F1"]
  return(f1)
}

# Calculate F1 scores for training and test datasets
train_f1_score <- calculate_f1(corona_train$Sentiment, train_predictions)
test_f1_score <- calculate_f1(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse)

coefficients <- coef(baseline_model, s = min(baseline_model$lambda))
non_zero_features <- sum(coefficients != 0) - 1  # Subtract 1 for the intercept

# Calculate the difference in F1 score between train and test data
f1_difference <- train_f1_score - test_f1_score

# Print the results
# cat("BASELINE RESULTS\n")
# cat("Number of features:", num_features, "\n")
# cat("Number of non-zero features:", non_zero_features, "\n")
# cat("F1 score on training data:", train_f1_score, "\n")
# cat("F1 score on test data:", test_f1_score, "\n")
# cat("Difference in F1 score between train and test data:", f1_difference, "\n")

covid_baseline_non_zero_features <- non_zero_features
covid_baseline_test_f1_score <- test_f1_score
covid_baseline_f1_difference <- f1_difference



#CORRELATION FEATURE SELECTION

correlations <- apply(train_sparse, 2, function(x) cor(x, as.numeric(y_train)))
abs_correlations <- abs(correlations)

# Step 2: Select features based on a correlation threshold
select_features <- function(threshold) {
  selected_features <- which(abs_correlations > threshold)
  return(selected_features)
}

# Step 3: Sweep through various thresholds and perform 10-fold cross-validation
percentiles <- seq(0, 1, length.out = 20)
thresholds <- quantile(abs_correlations, percentiles)
cv_results <- data.frame(threshold = numeric(), mean_f1 = numeric())

optimal_threshold <- cache_parameter('covid_cfs_optimal_threshold')
if (is.null(optimal_threshold)) {
  for (threshold in thresholds) {
    selected_features <- select_features(threshold)
    
    if (length(selected_features) == 0) {
      next
    }
    
    train_sparse_selected <- train_sparse[, selected_features, drop = FALSE]
    
    if (ncol(train_sparse_selected) < 2) {
      next
    }
    
    # Perform k-fold cross-validation
    train_control <- trainControl(method = "cv", number = validation_folds, verboseIter = TRUE)
    
    f1_scores <- c()
    
    # Define custom F1 score summary function
    f1_summary <- function(data, lev = NULL, model = NULL) {
      confusion <- confusionMatrix(data$pred, data$obs)
      f1 <- confusion$byClass["F1"]
      c(F1 = f1)
    }
    
    for (i in 1:validation_folds) {
      folds <- createFolds(y_train, k = validation_folds, list = TRUE, returnTrain = TRUE)
      f1_fold <- c()
      
      for (j in 1:validation_folds) {
        train_index <- folds[[j]]
        test_index <- setdiff(seq_len(nrow(train_sparse_selected)), train_index)
        
        x_train_cv <- train_sparse_selected[train_index, ]
        y_train_cv <- y_train[train_index]
        x_test_cv <- train_sparse_selected[test_index, ]
        y_test_cv <- y_train[test_index]
        
        model_cv <- glmnet(x_train_cv, y_train_cv, alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, family = "binomial", parallel = TRUE)
        
        pred_cv <- predict(model_cv, x_test_cv, s = min(model_cv$lambda), type = "response")
        pred_cv <- factor(ifelse(pred_cv > 0.5, "Positive", "Negative"), levels = c("Negative", "Positive"))
        actual_cv <- factor(ifelse(y_test_cv == 1, "Positive", "Negative"), levels = c("Negative", "Positive"))
        
        f1_fold <- c(f1_fold, calculate_f1(actual_cv, pred_cv))
      }
      
      f1_scores <- c(f1_scores, mean(f1_fold))
    }
    
    cv_results <- rbind(cv_results, data.frame(threshold = threshold, mean_f1 = mean(f1_scores)))
  }
  
  # Determine the optimal threshold
  optimal_threshold <- cache_parameter('covid_cfs_optimal_threshold', cv_results$threshold[which.max(cv_results$mean_f1)])
}


# Step 4: Use the optimal threshold to train the final model
selected_features <- select_features(optimal_threshold)
train_sparse_selected <- train_sparse[, selected_features]
test_sparse_selected <- test_sparse[, selected_features]

final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda, parallel = TRUE)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, "Positive", "Negative"), levels = c("Negative", "Positive"))
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, "Positive", "Negative"), levels = c("Negative", "Positive"))

# Calculate F1 scores for training and test datasets
train_f1_score <- calculate_f1(corona_train$Sentiment, train_predictions)
test_f1_score <- calculate_f1(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 

coefficients <- coef(final_model, s = min(final_model$lambda))
non_zero_features <- sum(coefficients != 0) - 1  # Subtract 1 for the intercept

# Calculate the difference in F1 score between train and test data
f1_difference <- train_f1_score - test_f1_score

# Print the results
# cat("CFS RESULTS\n")
# cat("Optimal Threshold:", optimal_threshold, "\n")
# cat("Number of features:", num_features, "\n")
# cat("Number of non-zero features:", non_zero_features, "\n")
# cat("F1 score on training data:", train_f1_score, "\n")
# cat("F1 score on test data:", test_f1_score, "\n")
# cat("Difference in F1 score between train and test data:", f1_difference, "\n")

covid_cfs_non_zero_features <- non_zero_features
covid_cfs_test_f1_score <- test_f1_score
covid_cfs_f1_difference <- f1_difference


# RECURSIVE FEATURE ELIMINATION
# Define custom RFE function
custom_rfe <- function(x, y, sizes, fold = validation_folds, parallel = TRUE) {
  results <- data.frame(num_features = integer(), mean_f1 = double())
  
  current_features <- seq_len(ncol(x))  # Start with all features
  best_features <- current_features
  
  for (size in sizes) {
    cat("Evaluating size:", size, "\n")
    
    # Fit glmnet model to get coefficients
    model <- glmnet(x[, current_features, drop = FALSE], y, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)
    coefs <- as.matrix(coef(model, s = model$lambda.min))
    
    # Get indices of the top 'size' features by their absolute coefficient values
    if (length(current_features) > size) {
      selected_features <- order(abs(coefs[-1, 1]), decreasing = TRUE)[1:size]
      current_features <- current_features[selected_features]
    }
    
    x_selected <- x[, current_features, drop = FALSE]
    
    folds <- createFolds(y, k = fold, list = TRUE, returnTrain = TRUE)
    f1_scores <- c()
    
    for (fold_idx in seq_along(folds)) {
      train_idx <- folds[[fold_idx]]
      test_idx <- setdiff(seq_len(nrow(x_selected)), train_idx)
      
      x_train_cv <- x_selected[train_idx, ]
      y_train_cv <- y[train_idx]
      x_test_cv <- x_selected[test_idx, ]
      y_test_cv <- y[test_idx]
      
      model_cv <- glmnet(x_train_cv, y_train_cv, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)
      
      if (length(model_cv$lambda) == 0) {
        next
      }
      
      pred_cv_prob <- predict(model_cv, x_test_cv, s = min(model_cv$lambda), type = "response")
      pred_cv <- factor(ifelse(pred_cv_prob > 0.5, "Positive", "Negative"), levels = c("Negative", "Positive"))
      
      f1 <- calculate_f1(y_test_cv, pred_cv)
      f1_scores <- c(f1_scores, f1)
    }
    
    mean_f1 <- mean(f1_scores, na.rm = TRUE)
    results <- rbind(results, data.frame(num_features = size, mean_f1 = mean_f1))
    
    if (mean_f1 == max(results$mean_f1, na.rm = TRUE)) {
      best_features <- current_features
    }
  }
  
  best_size <- results$num_features[which.max(results$mean_f1)]
  return(list(best_size = best_size, best_features = best_features, results = results))
}


# Generate sizes in geometric progression
generate_sizes <- function(start_size, end_size, num_steps) {
  ratio <- (end_size / start_size)^(1 / (num_steps - 1))
  sizes <- start_size * (ratio ^ (0:(num_steps - 1)))
  return(round(sizes))
}

# Perform recursive feature elimination
sizes <- generate_sizes(ncol(train_sparse), 100, 20)

best_features <- cache_parameter('covid_rfe_best_features')
if (is.null(best_features)) {
  rfe_results <- custom_rfe(train_sparse, y_train, sizes = sizes)
  best_features <- cache_parameter('covid_rfe_best_features', rfe_results$best_features)
}

# Fit the final model using selected features
train_sparse_selected = train_sparse[, best_features, drop=FALSE]
test_sparse_selected = test_sparse[, best_features, drop=FALSE]
final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, "Positive", "Negative"), levels = c("Negative", "Positive"))
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, "Positive", "Negative"), levels = c("Negative", "Positive"))


# Calculate F1 scores for training and test datasets
train_f1_score <- calculate_f1(corona_train$Sentiment, train_predictions)
test_f1_score <- calculate_f1(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 

coefficients <- coef(final_model, s = min(final_model$lambda))
non_zero_features <- sum(coefficients != 0) - 1  # Subtract 1 for the intercept

# Calculate the difference in F1 score between train and test data
f1_difference <- train_f1_score - test_f1_score

# cat("RFE RESULTS\n")
# cat("Number of features:", num_features, "\n")
# cat("Number of non-zero features:", non_zero_features, "\n")
# cat("F1 score on training data:", train_f1_score, "\n")
# cat("F1 score on test data:", test_f1_score, "\n")
# cat("Difference in F1 score between train and test data:", f1_difference, "\n")

covid_rfe_non_zero_features <- non_zero_features
covid_rfe_test_f1_score <- test_f1_score
covid_rfe_f1_difference <- f1_difference

#LASSO REGRESSION

best_lambda <- cache_parameter('covid_lasso_best_lambda')
if (is.null(best_lambda)) {
  cv_lasso <- cv.glmnet(train_sparse, y_train, family = "binomial", alpha = 1, maxit = baseline_maxit, nfolds = validation_folds)
  best_lambda <- cache_parameter('covid_lasso_best_lambda', cv_lasso$lambda.min)
}

# Fit the final Lasso model using the best lambda
final_model <- glmnet(train_sparse, y_train, family = "binomial", maxit = baseline_maxit, alpha = 1, lambda = best_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse, s = best_lambda, type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse, s = best_lambda, type = "response")

# Calculate F1 scores for training and test datasets
train_f1_score <- calculate_f1(corona_train$Sentiment, train_predictions)
test_f1_score <- calculate_f1(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse) 

coefficients <- coef(final_model, s = best_lambda)
non_zero_features <- sum(coefficients != 0) - 1  # Subtract 1 for the intercept

# Calculate the difference in F1 score between train and test data
f1_difference <- train_f1_score - test_f1_score

# cat("LASSO RESULTS\n")
# cat("Best lambda from cross-validation: ", best_lambda, "\n")
# cat("Number of features:", num_features, "\n")
# cat("Number of non-zero features:", non_zero_features, "\n")
# cat("F1 score on training data:", train_f1_score, "\n")
# cat("F1 score on test data:", test_f1_score, "\n")
# cat("Difference in F1 score between train and test data:", f1_difference, "\n")

covid_lasso_non_zero_features <- non_zero_features
covid_lasso_test_f1_score <- test_f1_score
covid_lasso_f1_difference <- f1_difference

#CFS + RFE REGRESSION
correlation_threshold = thresholds[5]
best_features <- cache_parameter('covid_cfs_rfe_best_features')
if (is.null(best_features)) {
  cfs_selected_features <- select_features(correlation_threshold)
  
  train_sparse_cfs_selected <- train_sparse[, cfs_selected_features, drop = FALSE]

  # Perform recursive feature elimination
  sizes <- generate_sizes(ncol(train_sparse_cfs_selected), 100, 20)
  rfe_results <- custom_rfe(train_sparse_cfs_selected, y_train, sizes = sizes)
  
  best_features <- cache_parameter('covid_cfs_rfe_best_features', rfe_results$best_features)
}

# Fit the final model using selected features
train_sparse_selected = train_sparse[, best_features, drop=FALSE]
test_sparse_selected = test_sparse[, best_features, drop=FALSE]
final_model <- glmnet(train_sparse_selected, y_train, family = "binomial", alpha = baseline_alpha, maxit = baseline_maxit, lambda.min.ratio = baseline_lambda)

# Predict on training and test datasets
train_predictions_prob <- predict(final_model, newx = train_sparse_selected, s = min(final_model$lambda), type = "response")
test_predictions_prob <- predict(final_model, newx = test_sparse_selected, s = min(final_model$lambda), type = "response")

# Convert probabilities to class labels (0 or 1)
train_predictions <- factor(ifelse(train_predictions_prob > 0.5, "Positive", "Negative"), levels = c("Negative", "Positive"))
test_predictions <- factor(ifelse(test_predictions_prob > 0.5, "Positive", "Negative"), levels = c("Negative", "Positive"))


# Calculate F1 scores for training and test datasets
train_f1_score <- calculate_f1(corona_train$Sentiment, train_predictions)
test_f1_score <- calculate_f1(corona_test$Sentiment, test_predictions)

# Calculate the number of features
num_features <- ncol(train_sparse_selected) 

coefficients <- coef(final_model, s = min(final_model$lambda))
non_zero_features <- sum(coefficients != 0) - 1  # Subtract 1 for the intercept

# Calculate the difference in F1 score between train and test data
f1_difference <- train_f1_score - test_f1_score

# cat("CFS + RFE RESULTS\n")
# cat("Correlation Threshold:", correlation_threshold, "\n")
# cat("Number of features:", num_features, "\n")
# cat("Number of non-zero features:", non_zero_features, "\n")
# cat("F1 score on training data:", train_f1_score, "\n")
# cat("F1 score on test data:", test_f1_score, "\n")
# cat("Difference in F1 score between train and test data:", f1_difference, "\n")

covid_cfs_rfe_non_zero_features <- non_zero_features
covid_cfs_rfe_test_f1_score <- test_f1_score
covid_cfs_rfe_f1_difference <- f1_difference


var_method <- c('Baseline', 'CFS', 'RFE', 'LASSO', 'CFS + RFE')
num_features <- c(covid_baseline_non_zero_features, covid_cfs_non_zero_features, covid_rfe_non_zero_features, covid_lasso_non_zero_features, covid_cfs_rfe_non_zero_features)
f1_score <- c(covid_baseline_test_f1_score, covid_cfs_test_f1_score, covid_rfe_test_f1_score, covid_lasso_test_f1_score, covid_cfs_rfe_test_f1_score)
f1_diff <- c(covid_baseline_f1_difference, covid_cfs_f1_difference, covid_rfe_f1_difference, covid_lasso_f1_difference, covid_cfs_rfe_f1_difference)

data <- data.frame(var_method, num_features, f1_score, f1_diff)

# Format numbers
data <- data %>%
  mutate(
    f1_score = round(f1_score, 3),
    f1_diff = round(f1_diff, 3)
  )

# Create and render the table
kable(data, format = "html", col.names = c("Method", "Number of Features", "Test F1 Score", "F1 Decrease from Train")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```



```{r}
#| include: false
#environment cleanup (hidden from output)
# Stop the cluster to clean up resources
stopCluster(cl)

# Unregister the parallel backend (optional, to ensure no residual registration)
registerDoSEQ()
```


# Conclusion

# References


