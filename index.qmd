---
title: "Feature Selection: An Exploration of Algorithm Performance"
author: "Jason Case, Abhishek Dugar, Daniel Nkuah, Khoa Tran"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---
# Introduction
The discipline of feature selection in machine learning has evolved significantly over the past few decades, transforming from a practice once stigmatized as "data dredging" to an essential component of modern predictive modeling. Early methods for variable selection were initially met with skepticism by the statistical community, which viewed them as potentially spurious means of uncovering patterns in data. These reservations stemmed from concerns about overfitting and the robustness of statistical inferences drawn from such techniques.
 
**Early Methods**

Feature selection's early forays began with relatively simple approaches like forward, backward, and stepwise variable selection in linear models. Forward selection starts with no variables in the model, adding them one by one based on specific criteria until no significant improvement is observed. Conversely, backward selection begins with all candidate variables, removing the least significant ones iteratively. Stepwise selection combines both approaches, iteratively adding and removing variables to optimize model performance.
 
These methods were foundational but had limitations, particularly when applied to datasets with high dimensionality. For instance, @foster2004variable in their study on predicting bankruptcy highlighted the complexity of variable selection when dealing with large sets of predictors. They utilized a 20:80 train/validation split to ensure robust model evaluation and critiqued traditional criteria like the Akaike Information Criterion (AIC) for being too liberal, risking overfitting. Instead, they advocated for methods like the Bonferroni correction, which, though conservative, provided more reliable results by dynamically adjusting p-values and avoiding inflated values from high leverage points.
 
Univariate screening procedures marked another step in the evolution of feature selection. These methods involve assessing each predictor variable independently for its relationship with the target variable, selecting those that meet a certain statistical threshold. While straightforward and computationally inexpensive, univariate methods often fail to capture the complex interdependencies between variables. 
 
In the context of gene selection for cancer classification, @guyon2002gene demonstrated the limitations of univariate methods. Prior to their work, gene selection relied heavily on simple statistical measures such as correlations, which often resulted in large, redundant sets of genes. The advent of Recursive Feature Elimination (RFE) represented a significant improvement. RFE uses Support Vector Machines (SVMs) to iteratively eliminate the least important features, refining the feature set to a more relevant subset and thus enhancing model performance. There were multiple tests that demonstrated that SVMs outperforms the baseline methods. First, the best leave-one-out performance results in a 100% accuracy for SVMs and only 90% for the baseline method (combination of univariate classifiers). The second example was using the statistical test, (1-n), where n equals the number of features, we can state with 99.3% confidence that SVMs are better than the baseline method. Additionally, when comparing the SVM-selected genes and baseline-selected genes, the SVM genes are better with a 84.1% confidence based on test error rate and a 99.2% confidence based on the test rejection rate. Figure 1 shows the performance comparison between SVMs and the baseline method, success rate, acceptance rate, extremal margin, and median margin and also indicates that there is a significant difference between the gene selection. Table 1 summarizes the results of the comparison between SVM and the baseline method, and it also indicates that SVMs are significantly better based on the est error rate and that it achieves a higher accuracy in classifying the samples. This method highlighted the critical role of robust feature selection in bioinformatics, where the choice of features often influenced classification outcomes more than the specific classifier used.

**Figure 1. Reproduction of Figure 2 in @guyon2002gene.**

::: {#figure-1 .figure}
![Figure 1](Figure_1.jpeg){ width=100% }
*This figure shows a performance comparison between SVMs and the baseline method using Leukemia data. Classifiers were trained on gene subsets selected by SVMs and the baseline method using the training set of the Leukemia data. The number of genes is color-coded and indicated in the legend. Quality indicators are displayed radially: channels 1–4 show cross-validation results using the leave-one-out method; channels 5–8 show test set results. The indicators include success rate (suc), acceptance rate (acc), extremal margin (ext), and median margin (med). Coefficients are rescaled to have zero mean and variance 1 across all plots. For each classifier, a larger colored area indicates better performance. The figure reveals no significant difference in classifier performance on this dataset, but a significant difference in gene selections.*
:::


**Table 1. Reproduction of Table 6 in @guyon2002gene.**

::: {#table-1 .figure}
![Table 1](Table_1.jpeg){ width=100% }
*This table presents the performance of the top classifiers on test data consisting of 34 samples. The table includes combinations of SVM or Baseline genes with SVM or Baseline classifiers. For each combination, the table shows the number of selected genes, the number of errors at zero rejection, and the number of rejections at zero error. The number of genes represents the subset selected by the method that provides the best classification performance. Patient ID numbers corresponding to classification errors are indicated in brackets. Additionally, results without any gene selection are provided for comparison.*
:::
 
**Modern Methods**

Modern feature selection methods have become essential for data preprocessing, extracting the most important information from big data. By reducing the number of features, these methods mitigate the risk of overfitting and enhance the performance of machine learning models and pattern recognition systems [@venkatesh2019review].
 
Advanced feature selection methods include similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based approaches. Similarity-based approaches select features based on their similarity or dissimilarity, while information-theoretical-based approaches use concepts like entropy and mutual information to evaluate feature importance. Sparse-learning-based approaches focus on identifying a small, essential set of features by limiting the number of features selected, and statistical-based approaches utilize statistical tests and models to determine the significance of features for predicting the target variable [@li2017feature].
 
Early and modern approaches are now categorized into filter, wrapper, embedded, and hybrid approaches. The filter model selects features based on the general properties of the training data, independent of any learning algorithm, making it computationally efficient for large datasets [@das2001filters; @kohavi1997wrappers]. Within the filter model, feature selection algorithms can be further divided into feature weighting algorithms, which assign weights to each feature based on their relevance, and subset search algorithms, which explore candidate feature subsets based on specific evaluation measures [@kohavi1997wrappers; @liu2012feature]. The wrapper model, on the other hand, uses a specific learning algorithm to evaluate and determine which features to keep, often leading to better performance but at a higher computational cost [@langley1994selection]. Embedded methods perform feature selection during the model training process, integrating selection directly with learning. Hybrid methods combine the best aspects of filter and wrapper methods to achieve optimal performance with manageable computational complexity [@venkatesh2019review; @jovic2015review].
 
Feature selection can also be divided into supervised, semi-supervised, and unsupervised categories. Supervised learning involves labeled data, semi-supervised learning utilizes both labeled and unlabeled data, and unsupervised learning works with unlabeled data. @miao2016survey emphasized that the effectiveness of feature selection methods varies depending on the dataset and model used, necessitating tailored approaches to feature selection.
 
As feature selection methods evolved, addressing multicollinearity—a scenario where predictor variables are highly correlated—became a critical focus. Traditional methods often assumed independence among features, leading to inaccurate importance measures. @basu2022multicollinearity tackled this issue by introducing a framework to adjust for feature correlations when calculating Shapley values. Their method, termed Multicollinearity Corrected (MCC) Shapley values, provided more accurate measures of feature importance by accounting for existing correlations. Their experiments on datasets with moderate to high feature correlations demonstrated the superiority of MCC Shapley values over non-corrected ones, offering a more reliable interpretation of feature importance in the presence of multicollinearity.
 
A novel feature selection algorithm leveraging neural networks aims to enhance classification accuracy while reducing computational load by selecting only the most relevant features. This method utilizes feed-forward neural networks (FFNNs) and integrates seamlessly with other classifiers. The algorithm starts with a minimal feature set and iteratively includes additional features based on their contribution to classification accuracy, ensuring that only significant features are selected [@onnia2001feature].
 
Experiments on both artificial datasets, like the Monks problems, and real-world datasets, such as the University of Wisconsin Breast Cancer Dataset, the US Congressional Voting Records Dataset, and the Pima Indians Diabetes Dataset, demonstrated significant reductions in the number of features while improving classification accuracy. This approach emphasizes the importance of feature selection in enhancing classification accuracy and reducing computational load, making it a valuable tool for various classification tasks and data collection processes.
 
In more recent developments, multiple criteria decision-making (MCDM) methods have been applied to feature selection, particularly in the context of text classification with small datasets. @kou2020evaluation evaluated various feature selection methods using MCDM techniques across multiple text classification datasets. By employing methods like PROMETHEE, they ranked feature selection techniques based on a range of performance measures, including accuracy, stability, and efficiency. Their findings underscored the importance of comprehensive evaluation criteria in selecting the most appropriate feature selection method, tailored to specific dataset characteristics.
 
The trajectory of feature selection in machine learning illustrates a progression from simple, univariate methods to sophisticated, model-based approaches that address complex issues like multicollinearity and multi-criteria decision making. Early skepticism about "data dredging" has given way to an appreciation of the nuanced techniques necessary for robust predictive modeling. Feature selection methods have evolved to include similarity-based, information-theoretical-based, sparse-learning-based, statistical-based, filter, wrapper, embedded, hybrid, supervised, semi-supervised, and unsupervised approaches. Modern methods, such as neural networks, further enhance the accuracy and efficiency of feature selection, ensuring that only the most relevant features are utilized. As the field continues to advance, it is clear that feature selection will remain a cornerstone of machine learning, improving model performance and reducing computational complexity across a wide range of applications.


# Methods
Our exploration of feature selection will begin by comparing the performance of various feature selection methods on a variety of real-world datasets. We will look at four different aspects of performance: overall model performance on the holdout set, variance in performance between the training and holdout set, stability in the features selected, and parsimony of the final model. We explored an algorithm from each category of feature selection methods.

**Correlation - Based Feature Selection (CFS)**

A common filtering method uses the correlation coefficient to measure the relationship between each variable and the target, independently.

CFS's feature subset evaluation function is:

$\text{Ms} = \frac{k\bar{\text{rcf}}}{\sqrt k + k(k - 1)\bar{\text{rff}}}$

where Ms is the heuristic ”merit” of a feature subset S containing k features, $\bar{\text{rcf}}$ is the mean feature-class correlation and $\bar{\text{rff}}$ is the average feature - feature intercorrelation. The numerator of this equation reflects the extent to which a set of features is indicative of the class, while the denominator measures the level of redundancy among the features [@hall1999correlation].
 

**Recursive Feature Elimination (RFE)**

RFE is a common wrapper method that removes variables iteratively.

For an unknown input vector x, a linear classifier takes the form

$y(x)=$sign$(w⋅x−b)$

The recursive feature elimination method for support vector machines can be executed through these iterative steps[@guyon2002gene].

1. Train the SVM classifier
2. compute the ranking criterion for all features
3. remove the features with smallest ranking values

For LS-SVM, substituting the KKT conditions into the Lagrangian gives the objective function

$L = -\frac{1}{2} \sum \sum \alpha_i \alpha_j (K(x_i, x_j) + \frac{\sigma_{ij}}{C}) + \sum \alpha_i y_i$

Where ${\sigma_{ij}} = 1$ if $i=j$ and $0$ otherwise.

The ranking criterion can be obtained by:

$D(L^{-m}) = -\frac{1}{2} \sum \sum \alpha_i \alpha_j (K(x_i, x_j) - (K(x_i^{-m}, x_j^{-m})$

Where $x_i^{-m}$ , $x_j^{-m}$ are the vectors in which $m$-th feature has been removed.

**Least Absolute Shrinkage and Selection Operator (LASSO)**

LASSO is an embedded method that uses L1 Regularization to shrink coefficients to zero, effectively performing feature selection during parameter estimation. LASSO minimizes the sum of squared errors while imposing an upper limit on the sum of the absolute values of the model parameters.

The LASSO estimate is defined by the solution to the l1 optimization problem

minimize  $\frac{\| Y - X\beta \|_2^2}{n}$ subject to $X \sum_{j=1}^{k} \|\beta_j\|_1 < t$

where $t$ is the upper bound for the sum of the coefficients [@buhlmann2011statistics]

**CFS & RFE**

For the hybrid method, we will combine the filter and wrapper methods; first filtering using the correlation coefficient and then removing variables iteratively using RFE.

# Analysis and Results

## Data and Visualization

We evaluated the performance of the methods on several classification tasks. The datasets and associated tasks used were:

**Birds' Songs Numeric Dataset**

The Birds' Songs Numeric Dataset on Kaggle contains numeric representations of bird songs, aimed at facilitating audio classification tasks. The dataset includes multiple features extracted from the audio files, such as spectral properties and time-domain characteristics, enabling machine learning models to analyze and classify different bird species based on their songs [@fleanend2024birds]. This dataset was selected because it contains all numeric predictor variables.

```{r}
print("R code currently in separate notebook")
```

**Spambase Dataset**

The Spambase dataset from the UCI Machine Learning Repository contains 4601 instances of emails, with 57 features for classification tasks to determine whether an email is spam (1) or not (0). Features include word and character frequency counts, as well as measures of consecutive capital letters. It is commonly used for building and testing spam detection models [@misc_spambase_94]. This dataset was selected because it contains a mix of numeric and categorical predictor variables.

```{r}
print("R code currently in separate notebook")
```

**COVID-19 NLP Text Classification Dataset**

The COVID-19 NLP Text Classification dataset on Kaggle consists of tweets related to COVID-19, labeled for sentiment analysis. It includes features like tweet content, sentiment labels (positive, negative, neutral), and additional metadata. This dataset is designed for natural language processing tasks to analyze public sentiment during the pandemic [@datatattle2024covid]. An NLP task was chosen because the vector of feature space is sparse and of high dimension.


```{r}
# Load necessary libraries
library(dplyr)
library(quanteda)
library(caret)
library(e1071)
library(wordcloud)
library(ggplot2)
# Step #1 Data Ingesting into R 
# download datasets, if necessary

# download_kaggle_dataset("datatattle/covid-19-nlp-text-classification", "./data/covid")

set.seed(42)
# Importing Train Data
corona_train <- read.csv("./data/covid/Corona_NLP_train.csv")[, c("OriginalTweet", "Sentiment")]

# Importing Test Data
corona_test <- read.csv("./data/covid/Corona_NLP_test.csv")[, c("OriginalTweet", "Sentiment")]



# Recode Sentiment to factors
# Recode Sentiment to factors
corona_train <- corona_train %>%
  filter(Sentiment != "Neutral") %>%
  mutate(Sentiment = recode(Sentiment,
                            "Extremely Negative" = "Negative",
                            "Extremely Positive" = "Positive"),
         Sentiment = factor(Sentiment, levels = c("Negative", "Positive")))

corona_test <- corona_test %>%
  filter(Sentiment != "Neutral") %>%
  mutate(Sentiment = recode(Sentiment,
                            "Extremely Negative" = "Negative",
                            "Extremely Positive" = "Positive"),
         Sentiment = factor(Sentiment, levels = c("Negative", "Positive")))



# Preprocessing and tokenization using quanteda
preprocess_text <- function(text_column) {
  tokens <- tokens(text_column, 
                   what = "word", 
                   remove_punct = TRUE, 
                   remove_numbers = TRUE,
                   remove_symbols = TRUE) %>%
    tokens_tolower() %>%
    tokens_remove(stopwords("english")) %>%
    tokens_wordstem()
  
  # Create a Document-Feature Matrix (DFM)
  dfm <- dfm(tokens)
  
  return(dfm)
}


train_dfm <- dfm_trim(preprocess_text(corona_train$OriginalTweet), min_termfreq = 10)
test_dfm <- preprocess_text(corona_test$OriginalTweet)

# Convert DFM to data frame
train_data <- convert(train_dfm, to = "data.frame")
test_data <- convert(dfm_match(test_dfm, features = featnames(train_dfm)), to = "data.frame") 

# Plotting in graph to see the distribution and find outlier

ggplot(corona_train, aes(x = Sentiment)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Sentiment", y = "Count", title = "Distribution of Sentiment Labels")


# Calculate and visualize mean, standard deviation, and range of tokens per document
token_counts <- rowSums(train_dfm)


ggplot(data = data.frame(token_counts), aes(x = token_counts)) +
  geom_histogram(binwidth = 5, color = "black") +
  labs(title = "Histogram of Token Counts",
       x = "Token Counts",
       y = "Frequency") 


# Calculate and report sparsity
num_columns <- ncol(train_dfm)
num_nonzero <- sum(train_dfm@x != 0)
total_elements <- prod(dim(train_dfm))
sparsity <- ((total_elements - num_nonzero) / total_elements) * 100

cat("Number of columns in the training dataset:", num_columns, "\n")
cat("Average percent of values that are 0 in the training dataset:", sparsity, "%\n")

# Create a word frequency cloud
word_freq <- colSums(train_dfm)
wordcloud(names(word_freq), freq = word_freq, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
```



## Statistical Modeling

For each classification task, we held out 30% of the data at random. We evaluated performance on four metrics: f1 score on the holdout set, difference between f1 score on the training and holdout set, number of variables selected, and average change in selected variables. To examine variation in selected variables for the last performance metric, we generated datasets of equal size to the training data by randomly selecting records with replacement from the training data.

To establish a baseline for performance, we first trained a multinomial logistic regression model using all available variables. We then use the four selection methods, training a multinomial logistic regression model on the features selected with each method. 

We used 10-fold cross validation accuracy to select the variables in all methods requiring a validation dataset (RFE, LASSO).

```{r}
print("R code currently in separate notebook")
```


<table>
<caption>Table 2. Overall f1 score on holdout set</caption>
  <tr>
    <th>Method</th>
    <th colspan="3">Dataset</th>
  </tr>
  <tr>
    <th></th>
    <th>Birds' Songs</th>
    <th>Spambase</th>
    <th>COVID-19 </th>
  </tr>
  <tr>
    <td>Baseline</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>CFS</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>RFE</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>LASSO</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>CFS + RFE</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
</table>


<table>
<caption>Table 3. Difference between f1 score on the training and holdout set</caption>
  <tr>
    <th>Method</th>
    <th colspan="3">Dataset</th>
  </tr>
  <tr>
    <th></th>
    <th>Birds' Songs</th>
    <th>Spambase</th>
    <th>COVID-19 </th>
  </tr>
  <tr>
    <td>Baseline</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>CFS</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>RFE</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>LASSO</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>CFS + RFE</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
</table>

<table>
<caption>Table 4. Number of variables selected</caption>
  <tr>
    <th>Method</th>
    <th colspan="3">Dataset</th>
  </tr>
  <tr>
    <th></th>
    <th>Birds' Songs</th>
    <th>Spambase</th>
    <th>COVID-19 </th>
  </tr>
  <tr>
    <td>Baseline</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>CFS</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>RFE</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>LASSO</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>CFS + RFE</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
</table>

<table>
<caption>Table 5. Average change in selected variables</caption>
  <tr>
    <th>Method</th>
    <th colspan="3">Dataset</th>
  </tr>
  <tr>
    <th></th>
    <th>Birds' Songs</th>
    <th>Spambase</th>
    <th>COVID-19 </th>
  </tr>
  <tr>
    <td>Baseline</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>CFS</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>RFE</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>LASSO</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td>CFS + RFE</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
</table>


# Conclusion

# References



